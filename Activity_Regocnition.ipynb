{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xabz/Activity-recognition/blob/main/Activity_Regocnition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja_-OYBuaOTX"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import shutil\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "random.Random(12)\n",
        "random.seed(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpflMwa3moG7"
      },
      "source": [
        "###Wisdm Raw_sensor data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3FfroxqgIUq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip '/content/drive/MyDrive/Dataset/wisdm-dataset.zip' -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ3PuPaQnip6",
        "outputId": "badbec99-8244-494f-8e5a-0dadc0356b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51 51\n",
            "(45375, 128, 6) (45375, 18)\n",
            "(36300, 128, 6) (36300, 18) (9075, 128, 6) (9075, 18)\n"
          ]
        }
      ],
      "source": [
        "window_size = 128\n",
        "overlap_size = int(window_size * 0.5)\n",
        "\n",
        "segmented_data = np.zeros((45375, window_size, 6))\n",
        "labels = []\n",
        "\n",
        "accel_files = sorted(glob.glob('/content/wisdm-dataset/raw/phone/accel/*.txt'))\n",
        "gyro_files = sorted(glob.glob('/content/wisdm-dataset/raw/phone/gyro/*.txt'))\n",
        "\n",
        "accel_columns = ['Subject_id', 'Activity_Label_acc', 'Timestamp', 'Acc_x', 'Acc_y', 'Acc_z']\n",
        "gyro_columns = ['Subject_id', 'Activity_Label_gyro', 'Timestamp', 'Gyro_x', 'Gyro_y', 'Gyro_z']\n",
        "print(len(accel_files),len(gyro_files))\n",
        "\n",
        "initial=0\n",
        "\n",
        "for accel, gyro in zip(accel_files,gyro_files):\n",
        "    accel_df = pd.read_csv(accel,sep=',',names=accel_columns)\n",
        "    accel_df['Acc_z'] = accel_df['Acc_z'].str.replace(';','').apply(lambda x:float(x))\n",
        "    gyro_df = pd.read_csv(gyro,sep=',',names=gyro_columns)\n",
        "    gyro_df['Gyro_z'] = gyro_df['Gyro_z'].str.replace(';','').apply(lambda x:float(x))\n",
        "\n",
        "    merged_df = pd.merge(accel_df,gyro_df,on='Timestamp')\n",
        "    merged_df = merged_df.loc[:, ['Acc_x', 'Acc_y', 'Acc_z','Gyro_x', 'Gyro_y', 'Gyro_z','Activity_Label_acc']]\n",
        "\n",
        "\n",
        "    data = merged_df.values\n",
        "\n",
        "    num_windows = int(np.ceil((data.shape[0] - window_size) / overlap_size))\n",
        "\n",
        "    # Segment the data using a sliding window approach\n",
        "    for i in range(num_windows):\n",
        "        start = i * overlap_size\n",
        "        end = start + window_size\n",
        "        segmented_data[initial] = data[start:end,:-1]\n",
        "        labels.append(stats.mode(data[start:end,-1])[0][0])\n",
        "        initial+=1\n",
        "    \n",
        "\n",
        "labels = LabelEncoder().fit_transform(labels)\n",
        "labels = OneHotEncoder(sparse=False).fit_transform(labels.reshape(-1,1))\n",
        "\n",
        "print(segmented_data.shape,labels.shape)\n",
        "X_train = segmented_data[:36300,...]\n",
        "y_train = labels[:36300,...]\n",
        "x_test = segmented_data[36300:,...]\n",
        "y_test = labels[36300:,...]\n",
        "\n",
        "print(X_train.shape,y_train.shape,x_test.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsitEOZEnktK"
      },
      "source": [
        "### UCI_HAR raw_sensor data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHgT55oPbcPL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip '/content/drive/MyDrive/Dataset/UCI HAR Dataset.zip' -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuEXwikNb43t",
        "outputId": "b27f9560-db97-45ed-c457-c9bfa276e980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7352, 128, 9) (7352, 6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "files_path = sorted(glob.glob('/content/UCI HAR Dataset/train/Inertial Signals/*.txt'))\n",
        "label_path = '/content/UCI HAR Dataset/train/y_train.txt'\n",
        "\n",
        "X_train = np.zeros((7352,128,9))\n",
        "\n",
        "for index,file_name in enumerate(files_path):\n",
        "    column_name = file_name.split('/')[-1].split('.')[0]\n",
        "    df = pd.read_csv(file_name,sep='\\t', header=None,names=[column_name])\n",
        "    df[column_name] = df[column_name].apply(lambda x: [float(i) for i in x.split()])\n",
        "\n",
        "    for ind, row in df.iterrows():\n",
        "        X_train[ind,:,index] = row[0]\n",
        "        \n",
        "label_df = pd.read_csv(label_path,header=None,names=['label'])\n",
        "label_df['label'] = label_df['label'].apply(lambda x: int(x))\n",
        "y_train = np.asarray(label_df).squeeze()\n",
        "y_train = LabelEncoder().fit_transform(y_train)\n",
        "y_train = OneHotEncoder(sparse=False).fit_transform(y_train.reshape(-1,1))\n",
        "\n",
        "print(X_train.shape,y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtXBbxcPr7hV",
        "outputId": "776d6813-4bc0-40db-9f4f-d53b815f8cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2947, 128, 9) (2947, 6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "files_path = sorted(glob.glob('/content/UCI HAR Dataset/test/Inertial Signals/*.txt'))\n",
        "label_path = '/content/UCI HAR Dataset/test/y_test.txt'\n",
        "\n",
        "x_test = np.zeros((2947,128,9))\n",
        "\n",
        "for index,file_name in enumerate(files_path):\n",
        "    column_name = file_name.split('/')[-1].split('.')[0]\n",
        "    df = pd.read_csv(file_name,sep='\\t', header=None,names=[column_name])\n",
        "    df[column_name] = df[column_name].apply(lambda x: [float(i) for i in x.split()])\n",
        "\n",
        "    for ind, row in df.iterrows():\n",
        "        x_test[ind,:,index] = row[0]\n",
        "        \n",
        "label_df = pd.read_csv(label_path,header=None,names=['label'])\n",
        "label_df['label'] = label_df['label'].apply(lambda x: int(x))\n",
        "y_test = np.asarray(label_df).squeeze()\n",
        "y_test = LabelEncoder().fit_transform(y_test)\n",
        "y_test = OneHotEncoder(sparse=False).fit_transform(y_test.reshape(-1,1))\n",
        "print(x_test.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeW6unZui_xf"
      },
      "source": [
        "##DataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg_8XBY5tASs"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self,X,y,batch_size,input_shape,p=None):\n",
        "        self.X = X                    #input data\n",
        "        self.y = y                    #input labels\n",
        "        self.batch_size = batch_size\n",
        "        self.input_shape = input_shape\n",
        "        self.total       = len(X)        \n",
        "\n",
        "        self.indexes     = np.arange(self.total)\n",
        "        self.total_batch = self.total // self.batch_size \n",
        "        self.classes = len(np.unique(self.y))\n",
        "        # self.p = p\n",
        "        self.rand = random.Random(12)\n",
        "        random.seed(12)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(self.total // self.batch_size)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "            \n",
        "        return self.X[indexes], [self.y[indexes],self.y[indexes]]\n",
        "        # return self.X[indexes], self.y[indexes]\n",
        "        \n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ROnPZmFjD9-"
      },
      "source": [
        "##Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEy1yUlwjF8K"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "class MyLogger(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, n=None, validation_data=None,AE=None):\n",
        "        sel\n",
        "        f.n = n   # print loss & acc every n epochs\n",
        "        if validation_data != None:\n",
        "            self.x_val, self.y_val = validation_data\n",
        "        self.start_time = time.time()\n",
        "        self.start_epoch = 0\n",
        "        self.AE = AE\n",
        "\n",
        "            \n",
        "        \n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.start_epoch +=1\n",
        "        if self.start_epoch % self.n != 0 and self.start_epoch != 1: return\n",
        "\n",
        "        self.start_time = time.time()\n",
        "        \n",
        "        cm = self._evaluate(self.y_val)\n",
        "\n",
        "        ep_time = time.time() - self.start_time\n",
        "        print(f\"\\rEpoch {self.start_epoch}: Time:{ep_time:.1f}\",\n",
        "              end='\\n')\n",
        "        print(f\" {cm} \",end='\\n')\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def _evaluate(self,y_val):\n",
        "        outs = self.AE.predict(self.x_val)\n",
        "        cm = tf.math.confusion_matrix(np.argmax(y_val,axis=1), np.argmax(outs,axis=1)) \n",
        "      \n",
        "        return cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vSPhbEvrIAa"
      },
      "source": [
        "##ResNet type CNN and BI-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDKgfP53GJ8u"
      },
      "outputs": [],
      "source": [
        "def res_identity(x, filters): \n",
        "  #renet block where dimension doesnot change.\n",
        "  #The skip connection is just simple identity conncection\n",
        "  #we will have 3 blocks and then input will be added\n",
        "\n",
        "  x_skip = x # this will be used for addition with the residual block \n",
        "  f1, f2 = filters\n",
        "\n",
        "  #first block \n",
        "  #kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
        "  \n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  #second block # bottleneck (but size kept same with padding)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same', )(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # third block activation used after adding the input\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "  # x = Activation(activations.relu)(x)\n",
        "\n",
        "  # add the input \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def conv_skip(x, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  # when s = 2 then it is like downsizing the feature map\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "\n",
        "  # second block\n",
        "  # x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  # x = tf.keras.layers.BatchNormalization()(x)\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "\n",
        "  #third block\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.MaxPooling1D(pool_size=3,strides=2)(x)\n",
        "  x = tf.keras.layers.ZeroPadding1D(padding=(0,1))(x)\n",
        "\n",
        "  # shortcut \n",
        "  x_skip = tf.keras.layers.Conv1D(f1, kernel_size=1, strides=2, padding='valid')(x_skip)\n",
        "  x_skip = tf.keras.layers.BatchNormalization()(x_skip)\n",
        "\n",
        "  # add \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def maxpool_skip(x, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        " \n",
        "\n",
        "  x_skip = x # this will be used for addition with the residual block \n",
        "  f1, f2 = filters\n",
        "  \n",
        "  #first block \n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  #second block # bottleneck (but size kept same with padding)\n",
        "  \n",
        "  \n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "  # x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  # x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # third block activation used after adding the input\n",
        "  \n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same', )(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "  x = tf.keras.layers.MaxPooling1D(pool_size = 3,strides=2)(x)\n",
        "  x = tf.keras.layers.ZeroPadding1D(padding=(0,1))(x)\n",
        "\n",
        "  # shortcut \n",
        "  x_skip = tf.keras.layers.Conv1D(f1, kernel_size=1, strides=2, padding='valid')(x_skip)\n",
        "  x_skip = tf.keras.layers.BatchNormalization()(x_skip)\n",
        "\n",
        "  # add \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def buildAE(input_shape,classes,learning_rate):\n",
        "\n",
        "    input = tf.keras.Input(shape=input_shape)\n",
        "    # x = tf.keras.layers.ZeroPadding1D(padding=(2,2))(input)\n",
        "    x = tf.keras.layers.Conv1D(16, kernel_size=3, strides=1,padding='same',)(input)\n",
        "\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)(x)\n",
        "    \n",
        "\n",
        "\n",
        "    x = conv_skip(x ,filters=(16,16))\n",
        "    for i in range(3):\n",
        "        x = res_identity(x,filters=(16,16))\n",
        "    x = maxpool_skip(x,filters=(16,16))\n",
        "\n",
        "\n",
        "    # class_name = ['non-fall','pre-fall','fall']\n",
        "\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=16,activation='tanh',return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=32,activation='tanh'))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(units=64)(x)\n",
        "    x = tf.keras.layers.Dense(units=classes)(x)\n",
        "        # ens.append(res)\n",
        "\n",
        "    # x = tf.keras.layers.Concatenate()(ens)\n",
        "    output = tf.keras.layers.Activation('softmax')(x)\n",
        "    model = tf.keras.Model(input,output)\n",
        "\n",
        "    # model = tf.keras.Model(input,ens)\n",
        "\n",
        "    # loss_weights = {\n",
        "    # 'non-fall': class_weights[0],  # no weighting\n",
        "    # 'pre-fall': class_weights[1],  # double weight\n",
        "    # 'fall': class_weights[2]   # half weight\n",
        "    # }\n",
        "\n",
        "\n",
        "    \n",
        "    print('Params', model.count_params())\n",
        "    model.compile(loss = tf.keras.losses.CategoricalCrossentropy(),\n",
        "                  metrics=[tf.keras.metrics.CategoricalAccuracy(),],\n",
        "                   optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
        "\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "# model = buildAE((256,6),3,0.0005)\n",
        "# tf.keras.utils.plot_model(model,show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS2l0Ft_S8fI",
        "outputId": "559dddbf-de50-4beb-db42-7887d24f5eb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params 38854\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 9)]     0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 128, 16)      448         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 128, 16)     64          ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 128, 16)      0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 63, 16)       0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 63, 16)       784         ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 63, 16)      64          ['conv1d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 63, 16)       0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 63, 16)       784         ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 63, 16)      64          ['conv1d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 31, 16)      0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 32, 16)       272         ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " zero_padding1d (ZeroPadding1D)  (None, 32, 16)      0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32, 16)      64          ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 32, 16)       0           ['zero_padding1d[0][0]',         \n",
            "                                                                  'batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 32, 16)       0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 32, 16)       784         ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 32, 16)      64          ['conv1d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 32, 16)       0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 32, 16)       784         ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 16)      64          ['conv1d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 32, 16)       0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 32, 16)       784         ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 16)      64          ['conv1d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 32, 16)       0           ['batch_normalization_6[0][0]',  \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 16)       0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 32, 16)       784         ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 32, 16)      64          ['conv1d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 16)       0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 32, 16)       784         ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 32, 16)      64          ['conv1d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 32, 16)       0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 32, 16)       784         ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 32, 16)      64          ['conv1d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 32, 16)       0           ['batch_normalization_9[0][0]',  \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 32, 16)       0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 32, 16)       784         ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 32, 16)      64          ['conv1d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 32, 16)       0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 32, 16)       784         ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 32, 16)      64          ['conv1d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 32, 16)       0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 32, 16)       784         ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 32, 16)      64          ['conv1d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 32, 16)       0           ['batch_normalization_12[0][0]', \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 32, 16)       0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 32, 16)       784         ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 32, 16)      64          ['conv1d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 32, 16)       0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 32, 16)       784         ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 32, 16)      64          ['conv1d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 15, 16)      0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 16, 16)       272         ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " zero_padding1d_1 (ZeroPadding1  (None, 16, 16)      0           ['max_pooling1d_2[0][0]']        \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 16)      64          ['conv1d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 16, 16)       0           ['zero_padding1d_1[0][0]',       \n",
            "                                                                  'batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 16, 16)       784         ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 16, 16)      64          ['conv1d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16, 16)       0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 16, 16)       0           ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 16, 32)       4224        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 16, 32)      128         ['bidirectional[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 16, 32)       0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 64)          16640       ['dropout_1[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 64)          256         ['bidirectional_1[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 64)           4160        ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 6)            390         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 6)            0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 38,854\n",
            "Trainable params: 38,118\n",
            "Non-trainable params: 736\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/101\n",
            "114/114 [==============================] - 45s 58ms/step - loss: 1.2133 - categorical_accuracy: 0.5661 - val_loss: 1.6254 - val_categorical_accuracy: 0.4571 - lr: 0.0010\n",
            "Epoch 2/101\n",
            "114/114 [==============================] - 5s 42ms/step - loss: 0.3504 - categorical_accuracy: 0.8698 - val_loss: 0.8617 - val_categorical_accuracy: 0.6508 - lr: 0.0010\n",
            "Epoch 3/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.1862 - categorical_accuracy: 0.9249 - val_loss: 0.4080 - val_categorical_accuracy: 0.8568 - lr: 0.0010\n",
            "Epoch 4/101\n",
            "114/114 [==============================] - 5s 40ms/step - loss: 0.1635 - categorical_accuracy: 0.9337 - val_loss: 0.2554 - val_categorical_accuracy: 0.9121 - lr: 0.0010\n",
            "Epoch 5/101\n",
            "114/114 [==============================] - 5s 41ms/step - loss: 0.1503 - categorical_accuracy: 0.9390 - val_loss: 0.2362 - val_categorical_accuracy: 0.9172 - lr: 0.0010\n",
            "Epoch 6/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1466 - categorical_accuracy: 0.9364 - val_loss: 0.2219 - val_categorical_accuracy: 0.9196 - lr: 0.0010\n",
            "Epoch 7/101\n",
            "114/114 [==============================] - 5s 42ms/step - loss: 0.1305 - categorical_accuracy: 0.9449 - val_loss: 0.2051 - val_categorical_accuracy: 0.9311 - lr: 0.0010\n",
            "Epoch 8/101\n",
            "114/114 [==============================] - 4s 38ms/step - loss: 0.1299 - categorical_accuracy: 0.9476 - val_loss: 0.2206 - val_categorical_accuracy: 0.9209 - lr: 0.0010\n",
            "Epoch 9/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1311 - categorical_accuracy: 0.9465 - val_loss: 0.2214 - val_categorical_accuracy: 0.9165 - lr: 0.0010\n",
            "Epoch 10/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1356 - categorical_accuracy: 0.9450 - val_loss: 0.2550 - val_categorical_accuracy: 0.9240 - lr: 0.0010\n",
            "Epoch 11/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1309 - categorical_accuracy: 0.9463 - val_loss: 0.2358 - val_categorical_accuracy: 0.9162 - lr: 0.0010\n",
            "Epoch 12/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.1240 - categorical_accuracy: 0.9485 - val_loss: 0.2599 - val_categorical_accuracy: 0.9118 - lr: 0.0010\n",
            "Epoch 13/101\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.1288 - categorical_accuracy: 0.9454 - val_loss: 0.2884 - val_categorical_accuracy: 0.9155 - lr: 0.0010\n",
            "Epoch 14/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1264 - categorical_accuracy: 0.9471 - val_loss: 0.2449 - val_categorical_accuracy: 0.9253 - lr: 0.0010\n",
            "Epoch 15/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.1204 - categorical_accuracy: 0.9456 - val_loss: 0.2640 - val_categorical_accuracy: 0.9182 - lr: 0.0010\n",
            "Epoch 16/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.1218 - categorical_accuracy: 0.9492 - val_loss: 0.2379 - val_categorical_accuracy: 0.9226 - lr: 0.0010\n",
            "Epoch 17/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1221 - categorical_accuracy: 0.9471 - val_loss: 0.1862 - val_categorical_accuracy: 0.9281 - lr: 0.0010\n",
            "Epoch 18/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1176 - categorical_accuracy: 0.9493 - val_loss: 0.1510 - val_categorical_accuracy: 0.9410 - lr: 0.0010\n",
            "Epoch 19/101\n",
            "114/114 [==============================] - 5s 44ms/step - loss: 0.1111 - categorical_accuracy: 0.9527 - val_loss: 0.1976 - val_categorical_accuracy: 0.9172 - lr: 0.0010\n",
            "Epoch 20/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.1308 - categorical_accuracy: 0.9438 - val_loss: 0.2437 - val_categorical_accuracy: 0.9220 - lr: 0.0010\n",
            "Epoch 21/101\n",
            "114/114 [==============================] - 5s 43ms/step - loss: 0.1170 - categorical_accuracy: 0.9504 - val_loss: 0.1976 - val_categorical_accuracy: 0.9484 - lr: 0.0010\n",
            "Epoch 22/101\n",
            "114/114 [==============================] - 5s 40ms/step - loss: 0.1259 - categorical_accuracy: 0.9467 - val_loss: 0.2023 - val_categorical_accuracy: 0.9423 - lr: 0.0010\n",
            "Epoch 23/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.1088 - categorical_accuracy: 0.9537 - val_loss: 0.1801 - val_categorical_accuracy: 0.9372 - lr: 0.0010\n",
            "Epoch 24/101\n",
            "114/114 [==============================] - 5s 44ms/step - loss: 0.1259 - categorical_accuracy: 0.9472 - val_loss: 0.2635 - val_categorical_accuracy: 0.9308 - lr: 0.0010\n",
            "Epoch 25/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1120 - categorical_accuracy: 0.9533 - val_loss: 0.2210 - val_categorical_accuracy: 0.9315 - lr: 0.0010\n",
            "Epoch 26/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.1108 - categorical_accuracy: 0.9512 - val_loss: 0.2134 - val_categorical_accuracy: 0.9379 - lr: 0.0010\n",
            "Epoch 27/101\n",
            "114/114 [==============================] - 5s 44ms/step - loss: 0.1156 - categorical_accuracy: 0.9494 - val_loss: 0.1847 - val_categorical_accuracy: 0.9440 - lr: 0.0010\n",
            "Epoch 28/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1136 - categorical_accuracy: 0.9516 - val_loss: 0.2009 - val_categorical_accuracy: 0.9342 - lr: 0.0010\n",
            "Epoch 29/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.1183 - categorical_accuracy: 0.9497 - val_loss: 0.1922 - val_categorical_accuracy: 0.9270 - lr: 0.0010\n",
            "Epoch 30/101\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.1093 - categorical_accuracy: 0.9524 - val_loss: 0.2043 - val_categorical_accuracy: 0.9403 - lr: 0.0010\n",
            "Epoch 31/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1091 - categorical_accuracy: 0.9509 - val_loss: 0.2281 - val_categorical_accuracy: 0.9220 - lr: 9.9005e-04\n",
            "Epoch 32/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1164 - categorical_accuracy: 0.9482 - val_loss: 0.2978 - val_categorical_accuracy: 0.9250 - lr: 9.8020e-04\n",
            "Epoch 33/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1062 - categorical_accuracy: 0.9541 - val_loss: 0.2261 - val_categorical_accuracy: 0.9267 - lr: 9.7045e-04\n",
            "Epoch 34/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.1021 - categorical_accuracy: 0.9561 - val_loss: 0.2861 - val_categorical_accuracy: 0.9209 - lr: 9.6079e-04\n",
            "Epoch 35/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.1159 - categorical_accuracy: 0.9522 - val_loss: 0.2702 - val_categorical_accuracy: 0.9152 - lr: 9.5123e-04\n",
            "Epoch 36/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1098 - categorical_accuracy: 0.9533 - val_loss: 0.2461 - val_categorical_accuracy: 0.9277 - lr: 9.4176e-04\n",
            "Epoch 37/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1161 - categorical_accuracy: 0.9481 - val_loss: 0.2372 - val_categorical_accuracy: 0.9141 - lr: 9.3239e-04\n",
            "Epoch 38/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.1217 - categorical_accuracy: 0.9515 - val_loss: 0.2064 - val_categorical_accuracy: 0.9304 - lr: 9.2312e-04\n",
            "Epoch 39/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1096 - categorical_accuracy: 0.9524 - val_loss: 0.3149 - val_categorical_accuracy: 0.9125 - lr: 9.1393e-04\n",
            "Epoch 40/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.1119 - categorical_accuracy: 0.9508 - val_loss: 0.1853 - val_categorical_accuracy: 0.9321 - lr: 9.0484e-04\n",
            "Epoch 41/101\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.1033 - categorical_accuracy: 0.9534 - val_loss: 0.1914 - val_categorical_accuracy: 0.9359 - lr: 8.9583e-04\n",
            "Epoch 42/101\n",
            "114/114 [==============================] - 5s 42ms/step - loss: 0.1006 - categorical_accuracy: 0.9560 - val_loss: 0.2138 - val_categorical_accuracy: 0.9287 - lr: 8.8692e-04\n",
            "Epoch 43/101\n",
            "114/114 [==============================] - 5s 42ms/step - loss: 0.1007 - categorical_accuracy: 0.9567 - val_loss: 0.1923 - val_categorical_accuracy: 0.9172 - lr: 8.7810e-04\n",
            "Epoch 44/101\n",
            "114/114 [==============================] - 5s 40ms/step - loss: 0.1006 - categorical_accuracy: 0.9563 - val_loss: 0.1918 - val_categorical_accuracy: 0.9301 - lr: 8.6936e-04\n",
            "Epoch 45/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0999 - categorical_accuracy: 0.9568 - val_loss: 0.2560 - val_categorical_accuracy: 0.9352 - lr: 8.6071e-04\n",
            "Epoch 46/101\n",
            "114/114 [==============================] - 5s 41ms/step - loss: 0.0965 - categorical_accuracy: 0.9579 - val_loss: 0.2400 - val_categorical_accuracy: 0.9284 - lr: 8.5214e-04\n",
            "Epoch 47/101\n",
            "114/114 [==============================] - 5s 40ms/step - loss: 0.0968 - categorical_accuracy: 0.9575 - val_loss: 0.4009 - val_categorical_accuracy: 0.9192 - lr: 8.4366e-04\n",
            "Epoch 48/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0977 - categorical_accuracy: 0.9575 - val_loss: 0.2909 - val_categorical_accuracy: 0.9233 - lr: 8.3527e-04\n",
            "Epoch 49/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0934 - categorical_accuracy: 0.9592 - val_loss: 0.2448 - val_categorical_accuracy: 0.9352 - lr: 8.2696e-04\n",
            "Epoch 50/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.0937 - categorical_accuracy: 0.9605 - val_loss: 0.1747 - val_categorical_accuracy: 0.9335 - lr: 8.1873e-04\n",
            "Epoch 51/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0892 - categorical_accuracy: 0.9622 - val_loss: 0.2152 - val_categorical_accuracy: 0.9335 - lr: 8.1058e-04\n",
            "Epoch 52/101\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0919 - categorical_accuracy: 0.9623 - val_loss: 0.2283 - val_categorical_accuracy: 0.9291 - lr: 8.0252e-04\n",
            "Epoch 53/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0833 - categorical_accuracy: 0.9635 - val_loss: 0.2681 - val_categorical_accuracy: 0.9274 - lr: 7.9453e-04\n",
            "Epoch 54/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0906 - categorical_accuracy: 0.9597 - val_loss: 0.2139 - val_categorical_accuracy: 0.9311 - lr: 7.8663e-04\n",
            "Epoch 55/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0937 - categorical_accuracy: 0.9618 - val_loss: 0.1992 - val_categorical_accuracy: 0.9301 - lr: 7.7880e-04\n",
            "Epoch 56/101\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.0901 - categorical_accuracy: 0.9613 - val_loss: 0.2456 - val_categorical_accuracy: 0.9328 - lr: 7.7105e-04\n",
            "Epoch 57/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0834 - categorical_accuracy: 0.9635 - val_loss: 0.2405 - val_categorical_accuracy: 0.9287 - lr: 7.6338e-04\n",
            "Epoch 58/101\n",
            "114/114 [==============================] - 4s 39ms/step - loss: 0.0818 - categorical_accuracy: 0.9633 - val_loss: 0.3131 - val_categorical_accuracy: 0.9298 - lr: 7.5578e-04\n",
            "Epoch 59/101\n",
            "114/114 [==============================] - 5s 40ms/step - loss: 0.0828 - categorical_accuracy: 0.9655 - val_loss: 0.2758 - val_categorical_accuracy: 0.9128 - lr: 7.4826e-04\n",
            "Epoch 60/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0800 - categorical_accuracy: 0.9664 - val_loss: 0.2775 - val_categorical_accuracy: 0.9199 - lr: 7.4082e-04\n",
            "Epoch 61/101\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.0932 - categorical_accuracy: 0.9622 - val_loss: 0.2790 - val_categorical_accuracy: 0.9277 - lr: 7.3345e-04\n",
            "Epoch 62/101\n",
            "114/114 [==============================] - 4s 38ms/step - loss: 0.0775 - categorical_accuracy: 0.9668 - val_loss: 0.2504 - val_categorical_accuracy: 0.9325 - lr: 7.2615e-04\n",
            "Epoch 63/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0763 - categorical_accuracy: 0.9685 - val_loss: 0.2584 - val_categorical_accuracy: 0.9399 - lr: 7.1892e-04\n",
            "Epoch 64/101\n",
            "114/114 [==============================] - 5s 43ms/step - loss: 0.0857 - categorical_accuracy: 0.9618 - val_loss: 0.2139 - val_categorical_accuracy: 0.9369 - lr: 7.1177e-04\n",
            "Epoch 65/101\n",
            "114/114 [==============================] - 4s 38ms/step - loss: 0.0822 - categorical_accuracy: 0.9638 - val_loss: 0.2671 - val_categorical_accuracy: 0.9311 - lr: 7.0469e-04\n",
            "Epoch 66/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0823 - categorical_accuracy: 0.9638 - val_loss: 0.2074 - val_categorical_accuracy: 0.9325 - lr: 6.9768e-04\n",
            "Epoch 67/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0807 - categorical_accuracy: 0.9653 - val_loss: 0.2547 - val_categorical_accuracy: 0.9403 - lr: 6.9073e-04\n",
            "Epoch 68/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0761 - categorical_accuracy: 0.9678 - val_loss: 0.2562 - val_categorical_accuracy: 0.9338 - lr: 6.8386e-04\n",
            "Epoch 69/101\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0757 - categorical_accuracy: 0.9664 - val_loss: 0.3086 - val_categorical_accuracy: 0.9260 - lr: 6.7706e-04\n",
            "Epoch 70/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0809 - categorical_accuracy: 0.9652 - val_loss: 0.2769 - val_categorical_accuracy: 0.9304 - lr: 6.7032e-04\n",
            "Epoch 71/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0751 - categorical_accuracy: 0.9689 - val_loss: 0.2652 - val_categorical_accuracy: 0.9308 - lr: 6.6365e-04\n",
            "Epoch 72/101\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.0665 - categorical_accuracy: 0.9723 - val_loss: 0.2968 - val_categorical_accuracy: 0.9257 - lr: 6.5705e-04\n",
            "Epoch 73/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0754 - categorical_accuracy: 0.9672 - val_loss: 0.2137 - val_categorical_accuracy: 0.9501 - lr: 6.5051e-04\n",
            "Epoch 74/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0724 - categorical_accuracy: 0.9689 - val_loss: 0.2849 - val_categorical_accuracy: 0.9389 - lr: 6.4404e-04\n",
            "Epoch 75/101\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.0729 - categorical_accuracy: 0.9681 - val_loss: 0.2499 - val_categorical_accuracy: 0.9406 - lr: 6.3763e-04\n",
            "Epoch 76/101\n",
            "114/114 [==============================] - 4s 36ms/step - loss: 0.0716 - categorical_accuracy: 0.9693 - val_loss: 0.3453 - val_categorical_accuracy: 0.9274 - lr: 6.3128e-04\n",
            "Epoch 77/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.0689 - categorical_accuracy: 0.9712 - val_loss: 0.2607 - val_categorical_accuracy: 0.9274 - lr: 6.2500e-04\n",
            "Epoch 78/101\n",
            "114/114 [==============================] - 5s 48ms/step - loss: 0.0705 - categorical_accuracy: 0.9703 - val_loss: 0.3025 - val_categorical_accuracy: 0.9287 - lr: 6.1878e-04\n",
            "Epoch 79/101\n",
            "114/114 [==============================] - 4s 38ms/step - loss: 0.0692 - categorical_accuracy: 0.9729 - val_loss: 0.2547 - val_categorical_accuracy: 0.9257 - lr: 6.1263e-04\n",
            "Epoch 80/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.0663 - categorical_accuracy: 0.9731 - val_loss: 0.3193 - val_categorical_accuracy: 0.9338 - lr: 6.0653e-04\n",
            "Epoch 81/101\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0635 - categorical_accuracy: 0.9731 - val_loss: 0.3117 - val_categorical_accuracy: 0.9376 - lr: 6.0050e-04\n",
            "Epoch 82/101\n",
            "114/114 [==============================] - 4s 38ms/step - loss: 0.0692 - categorical_accuracy: 0.9711 - val_loss: 0.4120 - val_categorical_accuracy: 0.9267 - lr: 5.9452e-04\n",
            "Epoch 83/101\n",
            "114/114 [==============================] - 4s 38ms/step - loss: 0.0681 - categorical_accuracy: 0.9704 - val_loss: 0.2974 - val_categorical_accuracy: 0.9365 - lr: 5.8861e-04\n",
            "Epoch 84/101\n",
            "114/114 [==============================] - 5s 44ms/step - loss: 0.0695 - categorical_accuracy: 0.9712 - val_loss: 0.2566 - val_categorical_accuracy: 0.9345 - lr: 5.8275e-04\n",
            "Epoch 85/101\n",
            "114/114 [==============================] - 4s 38ms/step - loss: 0.0634 - categorical_accuracy: 0.9719 - val_loss: 0.2949 - val_categorical_accuracy: 0.9359 - lr: 5.7695e-04\n",
            "Epoch 86/101\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0614 - categorical_accuracy: 0.9749 - val_loss: 0.2532 - val_categorical_accuracy: 0.9437 - lr: 5.7121e-04\n",
            "Epoch 87/101\n",
            "114/114 [==============================] - 5s 40ms/step - loss: 0.0609 - categorical_accuracy: 0.9735 - val_loss: 0.2715 - val_categorical_accuracy: 0.9413 - lr: 5.6553e-04\n",
            "Epoch 88/101\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0626 - categorical_accuracy: 0.9737 - val_loss: 0.2570 - val_categorical_accuracy: 0.9430 - lr: 5.5990e-04\n",
            "Epoch 89/101\n",
            "114/114 [==============================] - 4s 39ms/step - loss: 0.0608 - categorical_accuracy: 0.9737 - val_loss: 0.2724 - val_categorical_accuracy: 0.9304 - lr: 5.5433e-04\n",
            "Epoch 90/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.0615 - categorical_accuracy: 0.9744 - val_loss: 0.2828 - val_categorical_accuracy: 0.9440 - lr: 5.4881e-04\n",
            "Epoch 91/101\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0699 - categorical_accuracy: 0.9718 - val_loss: 0.2483 - val_categorical_accuracy: 0.9444 - lr: 5.4335e-04\n",
            "Epoch 92/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.0616 - categorical_accuracy: 0.9742 - val_loss: 0.2708 - val_categorical_accuracy: 0.9257 - lr: 5.3794e-04\n",
            "Epoch 93/101\n",
            "114/114 [==============================] - 4s 38ms/step - loss: 0.0672 - categorical_accuracy: 0.9723 - val_loss: 0.4245 - val_categorical_accuracy: 0.9209 - lr: 5.3259e-04\n",
            "Epoch 94/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0578 - categorical_accuracy: 0.9745 - val_loss: 0.3379 - val_categorical_accuracy: 0.9379 - lr: 5.2729e-04\n",
            "Epoch 95/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.0583 - categorical_accuracy: 0.9740 - val_loss: 0.3601 - val_categorical_accuracy: 0.9362 - lr: 5.2205e-04\n",
            "Epoch 96/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.0615 - categorical_accuracy: 0.9738 - val_loss: 0.4018 - val_categorical_accuracy: 0.9270 - lr: 5.1685e-04\n",
            "Epoch 97/101\n",
            "114/114 [==============================] - 5s 42ms/step - loss: 0.0606 - categorical_accuracy: 0.9762 - val_loss: 0.3365 - val_categorical_accuracy: 0.9311 - lr: 5.1171e-04\n",
            "Epoch 98/101\n",
            "114/114 [==============================] - 4s 37ms/step - loss: 0.0587 - categorical_accuracy: 0.9742 - val_loss: 0.2923 - val_categorical_accuracy: 0.9420 - lr: 5.0662e-04\n",
            "Epoch 99/101\n",
            "114/114 [==============================] - 5s 41ms/step - loss: 0.0587 - categorical_accuracy: 0.9760 - val_loss: 0.2990 - val_categorical_accuracy: 0.9393 - lr: 5.0158e-04\n",
            "Epoch 100/101\n",
            "114/114 [==============================] - 5s 44ms/step - loss: 0.0537 - categorical_accuracy: 0.9782 - val_loss: 0.3526 - val_categorical_accuracy: 0.9393 - lr: 4.9659e-04\n",
            "Epoch 101/101\n",
            "114/114 [==============================] - 4s 38ms/step - loss: 0.0598 - categorical_accuracy: 0.9746 - val_loss: 0.3525 - val_categorical_accuracy: 0.9372 - lr: 4.9164e-04\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 30:\n",
        "    return lr\n",
        "  else:\n",
        "     return lr * tf.math.exp(-0.01)\n",
        "\n",
        "dg = DataGenerator(X_train,y_train,batch_size=batch_size,input_shape=X_train.shape[1:])\n",
        "model = buildAE(X_train.shape[1:],y_train.shape[-1],learning_rate)\n",
        "log = MyLogger(n=1, validation_data=(x_test,y_test), AE=model)\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "\n",
        "# history = model.fit(dg, epochs=101, verbose=1,callbacks = [lr_scheduler], validation_data=(x_test,y_test))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "uVLp9aofP5pJ",
        "outputId": "0b6a8a82-760d-4e29-f690-b56e97764c33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "93/93 [==============================] - 10s 11ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98       496\n",
            "           1       0.99      0.97      0.98       471\n",
            "           2       0.93      0.98      0.96       420\n",
            "           3       0.96      0.89      0.92       491\n",
            "           4       0.91      0.96      0.94       532\n",
            "           5       0.99      1.00      1.00       537\n",
            "\n",
            "    accuracy                           0.96      2947\n",
            "   macro avg       0.96      0.96      0.96      2947\n",
            "weighted avg       0.96      0.96      0.96      2947\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAHHCAYAAAAWM5p0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABe6UlEQVR4nO3dd1gUV9sG8HsXYelVpFjAimDvYi+oscSCxhpFYkkUNUo0hkTFjrHHbmIixmhsiebV2FCjxogNxYK9YgMEBKUtuDvfH3yu2YDK6s4OsPcv11yXe2b2zDMnlIfnnJmVCYIggIiIiEgkcqkDICIiouKNyQYRERGJiskGERERiYrJBhEREYmKyQYRERGJiskGERERiYrJBhEREYmKyQYRERGJiskGERERiYrJBpGIbty4gfbt28POzg4ymQw7duzQa/93796FTCZDeHi4Xvstylq1aoVWrVpJHQYR/QuTDSr2bt26hU8//RQVKlSAubk5bG1t0bRpU3z33XfIzMwU9dwBAQG4ePEiZs2ahfXr16N+/fqins+QBg8eDJlMBltb23zH8caNG5DJZJDJZJg/f77O/T969AhTp05FdHS0HqIlIimVkDoAIjH9+eef+Oijj6BQKDBo0CBUr14d2dnZOHbsGCZMmICYmBh8//33opw7MzMTkZGR+OabbzBq1ChRzuHh4YHMzEyYmpqK0v/blChRAhkZGdi5cyd69+6ttW/Dhg0wNzdHVlbWO/X96NEjTJs2DZ6enqhdu3aB37d///53Oh8RiYfJBhVbd+7cQd++feHh4YFDhw7Bzc1Nsy8oKAg3b97En3/+Kdr5nzx5AgCwt7cX7RwymQzm5uai9f82CoUCTZs2xa+//pon2di4cSM6d+6M3377zSCxZGRkwNLSEmZmZgY5HxEVHKdRqNiaO3cu0tLS8OOPP2olGi9VqlQJn3/+ueb1ixcvMGPGDFSsWBEKhQKenp74+uuvoVQqtd7n6emJLl264NixY2jYsCHMzc1RoUIF/Pzzz5pjpk6dCg8PDwDAhAkTIJPJ4OnpCSB3+uHlv/9t6tSpkMlkWm0RERFo1qwZ7O3tYW1tDS8vL3z99dea/a9bs3Ho0CE0b94cVlZWsLe3R7du3XDlypV8z3fz5k0MHjwY9vb2sLOzQ2BgIDIyMl4/sP/Rv39/7NmzBykpKZq206dP48aNG+jfv3+e45OTkzF+/HjUqFED1tbWsLW1RceOHXH+/HnNMYcPH0aDBg0AAIGBgZrpmJfX2apVK1SvXh1RUVFo0aIFLC0tNePy3zUbAQEBMDc3z3P9HTp0gIODAx49elTgayWid8Nkg4qtnTt3okKFCmjSpEmBjh86dCimTJmCunXrYtGiRWjZsiXCwsLQt2/fPMfevHkTvXr1Qrt27bBgwQI4ODhg8ODBiImJAQD4+/tj0aJFAIB+/fph/fr1WLx4sU7xx8TEoEuXLlAqlZg+fToWLFiArl274p9//nnj+w4cOIAOHTogISEBU6dORXBwMI4fP46mTZvi7t27eY7v3bs3nj9/jrCwMPTu3Rvh4eGYNm1ageP09/eHTCbD77//rmnbuHEjqlatirp16+Y5/vbt29ixYwe6dOmChQsXYsKECbh48SJatmyp+cXv7e2N6dOnAwCGDx+O9evXY/369WjRooWmn6SkJHTs2BG1a9fG4sWL0bp163zj++677+Ds7IyAgACoVCoAwOrVq7F//34sXboU7u7uBb5WInpHAlExlJqaKgAQunXrVqDjo6OjBQDC0KFDtdrHjx8vABAOHTqkafPw8BAACEePHtW0JSQkCAqFQvjiiy80bXfu3BEACPPmzdPqMyAgQPDw8MgTQ2hoqPDvb8lFixYJAIQnT568Nu6X51i7dq2mrXbt2kKpUqWEpKQkTdv58+cFuVwuDBo0KM/5PvnkE60+e/ToITg5Ob32nP++DisrK0EQBKFXr15C27ZtBUEQBJVKJbi6ugrTpk3LdwyysrIElUqV5zoUCoUwffp0Tdvp06fzXNtLLVu2FAAIq1atyndfy5Yttdr27dsnABBmzpwp3L59W7C2tha6d+/+1mskIv1gZYOKpWfPngEAbGxsCnT87t27AQDBwcFa7V988QUA5Fnb4ePjg+bNm2teOzs7w8vLC7dv337nmP/r5VqPP/74A2q1ukDvefz4MaKjozF48GA4Ojpq2mvWrIl27dpprvPfPvvsM63XzZs3R1JSkmYMC6J///44fPgw4uLicOjQIcTFxeU7hQLkrvOQy3N/9KhUKiQlJWmmiM6ePVvgcyoUCgQGBhbo2Pbt2+PTTz/F9OnT4e/vD3Nzc6xevbrA5yKi98Nkg4olW1tbAMDz588LdPy9e/cgl8tRqVIlrXZXV1fY29vj3r17Wu3lypXL04eDgwOePn36jhHn1adPHzRt2hRDhw6Fi4sL+vbtiy1btrwx8XgZp5eXV5593t7eSExMRHp6ulb7f6/FwcEBAHS6lk6dOsHGxgabN2/Ghg0b0KBBgzxj+ZJarcaiRYtQuXJlKBQKlCxZEs7Ozrhw4QJSU1MLfM7SpUvrtBh0/vz5cHR0RHR0NJYsWYJSpUoV+L1E9H6YbFCxZGtrC3d3d1y6dEmn9/13gebrmJiY5NsuCMI7n+PleoKXLCwscPToURw4cAADBw7EhQsX0KdPH7Rr1y7Pse/jfa7lJYVCAX9/f6xbtw7bt29/bVUDAGbPno3g4GC0aNECv/zyC/bt24eIiAhUq1atwBUcIHd8dHHu3DkkJCQAAC5evKjTe4no/TDZoGKrS5cuuHXrFiIjI996rIeHB9RqNW7cuKHVHh8fj5SUFM2dJfrg4OCgdefGS/+tngCAXC5H27ZtsXDhQly+fBmzZs3CoUOH8Ndff+Xb98s4r127lmff1atXUbJkSVhZWb3fBbxG//79ce7cOTx//jzfRbUvbdu2Da1bt8aPP/6Ivn37on379vDz88szJgVN/AoiPT0dgYGB8PHxwfDhwzF37lycPn1ab/0T0Zsx2aBi68svv4SVlRWGDh2K+Pj4PPtv3bqF7777DkDuNACAPHeMLFy4EADQuXNnvcVVsWJFpKam4sKFC5q2x48fY/v27VrHJScn53nvy4db/fd23Jfc3NxQu3ZtrFu3TuuX96VLl7B//37NdYqhdevWmDFjBpYtWwZXV9fXHmdiYpKnarJ161Y8fPhQq+1lUpRfYqariRMnIjY2FuvWrcPChQvh6emJgICA144jEekXH+pFxVbFihWxceNG9OnTB97e3lpPED1+/Di2bt2KwYMHAwBq1aqFgIAAfP/990hJSUHLli1x6tQprFu3Dt27d3/tbZXvom/fvpg4cSJ69OiBMWPGICMjAytXrkSVKlW0FkhOnz4dR48eRefOneHh4YGEhASsWLECZcqUQbNmzV7b/7x589CxY0f4+vpiyJAhyMzMxNKlS2FnZ4epU6fq7Tr+Sy6XY9KkSW89rkuXLpg+fToCAwPRpEkTXLx4ERs2bECFChW0jqtYsSLs7e2xatUq2NjYwMrKCo0aNUL58uV1iuvQoUNYsWIFQkNDNbfirl27Fq1atcLkyZMxd+5cnfojoncg8d0wRKK7fv26MGzYMMHT01MwMzMTbGxshKZNmwpLly4VsrKyNMfl5OQI06ZNE8qXLy+YmpoKZcuWFUJCQrSOEYTcW187d+6c5zz/veXydbe+CoIg7N+/X6hevbpgZmYmeHl5Cb/88kueW18PHjwodOvWTXB3dxfMzMwEd3d3oV+/fsL169fznOO/t4ceOHBAaNq0qWBhYSHY2toKH374oXD58mWtY16e77+31q5du1YAINy5c+e1YyoI2re+vs7rbn394osvBDc3N8HCwkJo2rSpEBkZme8tq3/88Yfg4+MjlChRQus6W7ZsKVSrVi3fc/67n2fPngkeHh5C3bp1hZycHK3jxo0bJ8jlciEyMvKN10BE708mCDqsAiMiIiLSEddsEBERkaiYbBAREZGomGwQERGRqJhsEBERkaiYbBAREZGomGwQERGRqJhsEBERkaiK5RNELT5cIXUIhcLT7SOlDqHQyHlR8A/4Ks5MS/DvC6L/MjfAb0KLOqP00k/muWV66cfQ+JOHiIiIRFUsKxtERESFisy4/7ZnskFERCQ2mUzqCCTFZIOIiEhsRl7ZMO6rJyIiItGxskFERCQ2TqMQERGRqDiNQkRERCQeVjaIiIjExmkUIiIiEhWnUYiIiIjEw8oGERGR2DiNQkRERKLiNAoRERGReFjZICIiEhunUYiIiEhURj6NwmSDiIhIbEZe2TDuVIuIiIhEx8oGERGR2DiNQkRERKIy8mTDuK+eiIiomJo6dSpkMpnWVrVqVc3+rKwsBAUFwcnJCdbW1ujZsyfi4+O1+oiNjUXnzp1haWmJUqVKYcKECXjx4oXOsbCyQUREJDa5NAtEq1WrhgMHDmhelyjx6tf+uHHj8Oeff2Lr1q2ws7PDqFGj4O/vj3/++QcAoFKp0LlzZ7i6uuL48eN4/PgxBg0aBFNTU8yePVunOJhsEBERiU2iaZQSJUrA1dU1T3tqaip+/PFHbNy4EW3atAEArF27Ft7e3jhx4gQaN26M/fv34/Llyzhw4ABcXFxQu3ZtzJgxAxMnTsTUqVNhZmZW4Dg4jUJERFRM3bhxA+7u7qhQoQIGDBiA2NhYAEBUVBRycnLg5+enObZq1aooV64cIiMjAQCRkZGoUaMGXFxcNMd06NABz549Q0xMjE5xMNl4R+N71UHmzpGYN7QpAKBcKRtk7hyZ7+bftKLmfa1qlsZfc/2RsHko7vw8GDMDGsNEovKamKLOnMbokZ/Br1Uz1KrmhUMHD7z9TUXc2ajTGDd6BD7wa4H6tbxx+FDea75z+xbGjRmJlk0boFmjuhjU/yPEPX4kQbSGt2njBnRs1wYN6tTAgL4f4eKFC1KHJAmOwytGNRYymV42pVKJZ8+eaW1KpTLfUzZq1Ajh4eHYu3cvVq5ciTt37qB58+Z4/vw54uLiYGZmBnt7e633uLi4IC4uDgAQFxenlWi83P9yny6YbLyDepVLYcgH1XDhTqKm7UFiGjwHrtXapm84hecZ2dgXdQ8AUMPTCTumdsH+s7FoPHYLBs7dj86NymPmYF+pLkU0mZkZ8PLyQsikUKlDMZjMzExU9vLCxJDJ+e5/cD8WQwcPgGf58li9Zh02bduBIcNHwMxMYeBIDW/vnt2YPzcMn44Mwqat2+HlVRUjPh2CpKQkqUMzKI7DK0Y3FjK5XrawsDDY2dlpbWFhYfmesmPHjvjoo49Qs2ZNdOjQAbt370ZKSgq2bNli4ItnsqEzK/MSWPuFH0YuPYyUtFfZpFotID4lU2vr2rg8fjt2C+lZuSt3ezWvhEt3kxC26QxuP36GY5ce4Zu1x/Fpp+qwtjCV6IrE0ax5S4z6fBza+rWTOhSDadqsBUaOGovWbfO/5uVLF6NJsxb4fNwEVPX2QZmy5dCyVRs4OjkZOFLDW79uLfx79Ub3Hj1RsVIlTAqdBnNzc+z4/TepQzMojsMrHIt3ExISgtTUVK0tJCSkQO+1t7dHlSpVcPPmTbi6uiI7OxspKSlax8THx2vWeLi6uua5O+Xl6/zWgbwJkw0dLf6sBfaeuYe/zj9443F1KjqjdkVnrIu4omlTmJogK1v7lqHMbBUsFCVQp6KzKPFS4aBWq/HP30fg4eGJUZ8NRbtWTREwoE++Uy3FTU52Nq5cjkFj3yaaNrlcjsaNm+DC+XMSRmZYHIdXjHIs9DSNolAoYGtrq7UpFAWrjqalpeHWrVtwc3NDvXr1YGpqioMHD2r2X7t2DbGxsfD1za22+/r64uLFi0hISNAcExERAVtbW/j4+Oh0+ZImG4mJiZg7dy569OgBX19f+Pr6okePHpg3bx6ePHkiZWj5+qh5JdSu6IzJ60689diA9t64EpuME1dfzWtFnLuPxlVd0btFJcjlMrg7WuHrvvUBAG6OlqLFTdJLTk5CRkYGwn9aA9+mzbBs1Rq0buOHCcFjEHXmlNThieppylOoVCo4/aeC4+TkhMTExNe8q/jhOLxilGOhp2kUXYwfPx5HjhzB3bt3cfz4cfTo0QMmJibo168f7OzsMGTIEAQHB+Ovv/5CVFQUAgMD4evri8aNGwMA2rdvDx8fHwwcOBDnz5/Hvn37MGnSJAQFBRU4wXlJsltfT58+jQ4dOsDS0hJ+fn6oUqUKgNwSzZIlSzBnzhzs27cP9evXf2M/SqUyz+IYQZUDmYl+pyXKlLTGvGHN0GXKTihzVG881tzMBH1aVMaczWe02g+eu4+v10ZiyciW+DHYD8ocFeZsPoNm1d2hVus1XCpkBLUAAGjZug0GDBwMAPCq6o3z58/ht62bUa9+QwmjIyLRSfBBbA8ePEC/fv2QlJQEZ2dnNGvWDCdOnICzc24lfdGiRZDL5ejZsyeUSiU6dOiAFStWaN5vYmKCXbt2YcSIEfD19YWVlRUCAgIwffp0nWORLNkYPXo0PvroI6xatQqy//xPEAQBn332GUaPHq25Bed1wsLCMG3aNK02k8qdYOrVWa/x1qnkDBcHS0Qu/kjTVsJEjmbV3PFZlxqw818N9f//QunRtCIsFSWw4dC1PP0s+eM8lvxxHm6OlniapoRHKVvMCPDFnfhneo2XChd7B3uYlCiB8hUqarWXL18B0dFnJYrKMBzsHWBiYpJn4V9SUhJKliwpUVSGx3F4hWNhGJs2bXrjfnNzcyxfvhzLly9/7TEeHh7YvXv3e8ci2TTK+fPnMW7cuDyJBgDIZDKMGzcO0dHRb+0nv8UyJSq113u8f51/gHpBm9BozBbNFnUjAZuOXEejMVs0iQYADG7njT9P3UXis6zX9vc4OQNZ2Sr0blkJ9588x7lbhW/aiPTH1NQM1apVx727d7TaY+/dhZubu0RRGYapmRm8farh5IlXfzio1WqcPBmJmrXqSBiZYXEcXjHKsZBgGqUwkayy4erqilOnTmk9p/3fTp06lef+3vwoFIo8c0f6nkIBgLTMHFyOTdZqS8/KQfKzLK32Cm62aFbNHd2n7cq3n3E9amP/2VioBQHdfCtgfM+6+Hjufq1kpTjISE/XPDwGAB4+eICrV67Azs4Obu7F85drRkY67v/7mh8+wLWrudfs6uaOgQGfIOTLL1C3Xn3Ub9AIx/85hr+PHsbqNeskjNowBgYEYvLXE1GtWnVUr1ETv6xfh8zMTHTv4S91aAbFcXjF6MZCgmmUwkSyZGP8+PEYPnw4oqKi0LZtW01iER8fj4MHD+KHH37A/PnzpQrvnQX4eeNhUhoOnLuf7/729crhy971oDA1wcU7ifho1h7sj4rN99iiLCbmEoYGDtK8nj839z7wrt16YMbsOVKFJarLMTH4bGiA5vWi+d8CALp07Y6pM8LQum07hEwKRfhP32P+t7Ph4Vke3y74DrXr1pMqZIP5oGMnPE1OxoplS5CY+AReVb2xYvUaOBlZyZzj8ArHwrjIBEGQ7E/qzZs3Y9GiRYiKioJKlbvo0sTEBPXq1UNwcDB69+79Tv1afLji7QcZgafbR0odQqGR84IrcAHAtETRLcMSicXcAH92W3T6Ti/9ZO7+XC/9GJqkH8TWp08f9OnTBzk5OZrbnUqWLAlT0+L1gCsiIjJynEaRnqmpKdzc3KQOg4iIiERQKJINIiKiYq0I30miD0w2iIiIxGbkyYZxXz0RERGJjpUNIiIisXGBKBEREYnKyKdRmGwQERGJzcgrG8adahEREZHoWNkgIiISG6dRiIiISFScRiEiIiISDysbREREIpMZeWWDyQYREZHIjD3Z4DQKERERiYqVDSIiIrEZd2GDyQYREZHYOI1CREREJCJWNoiIiERm7JUNJhtEREQiY7JBREREojL2ZINrNoiIiEhUrGwQERGJzbgLG0w2iIiIxMZpFCIiIiIRsbJBREQkMmOvbBTLZOPp9pFSh1AolPr4Z6lDKDQehn8sdQhEZMSMPdngNAoRERGJqlhWNoiIiAoTY69sMNkgIiISm3HnGpxGISIiInGxskFERCQyTqMQERGRqJhsEBERkaiMPdngmg0iIiISFSsbREREYjPuwgaTDSIiIrFxGoWIiIhIRKxsEBERiczYKxtMNoiIiERm7MkGp1GIiIhIVKxsEBERiczYKxtMNoiIiMRm3LkGp1GIiIhIXKxsEBERiYzTKERERCQqJhtEREQkKmNPNrhmg4iIiETFygYREZHYjLuwwWSDiIhIbJxGISIiIhIRkw0RbNq4AR3btUGDOjUwoO9HuHjhgtQhiWZc1+p4tmkQ5gyqr2n7c0p7PNs0SGtbNKRRnvf2b1kRx7/9EAk/D8Ct1R9hQWBDQ4aud2ejTmPc6BH4wK8F6tfyxuFDB7T2T50cgvq1vLW20SOGSRSt4RnT98WbcBxeMaaxkMlketmKKk6j6NnePbsxf24YJoVOQ40atbBh/TqM+HQI/ti1F05OTlKHp1d1Kzgh0K8yLt5LzrNv7cHrmLUlWvM6M1ultT+okzdGd6mGyRuicObmE1gqSqCcs7XYIYsqMzMTlb280LW7PyYEj8n3mCZNm2PK9Fma12ZmZoYKT1LG9H3xJhyHV4xtLIpyoqAPrGzo2fp1a+Hfqze69+iJipUqYVLoNJibm2PH779JHZpeWSlKYM3o5hjz/QmkpGfn2Z+pfIGE1CzN9jwzR7PP3soMk/vUwacrjmHrP3dwJz4NMbEp2BP1wJCXoHdNm7XAyFFj0bptu9ceY2pmhpIlnTWbra2dASOUjrF8X7wNx+EVjoVxYbKhRznZ2bhyOQaNfZto2uRyORo3boIL589JGJn+LfikEfade4DDlx7nu793swq4831vnJj3IUL71oGFmYlmX+sabpDLZHB3sMTpBV1xZXlPhH/eAqWdLA0VvmSizpxCu1ZN4d+1I8JmTkVKylOpQxKdMX1fvAnH4RVjHIvCMI0yZ84cyGQyjB07VtOWlZWFoKAgODk5wdraGj179kR8fLzW+2JjY9G5c2dYWlqiVKlSmDBhAl68eKHTuQt1snH//n188sknUodRYE9TnkKlUuUpATo5OSExMVGiqPSvp68napV3xNRfz+a7f+s/dzBs2TF0nrEfC3dcQt/mFfDDqGaa/Z6lbCCXA190r4Gv1p3BoEVH4GCtwB9ft4OpSaH+knwvvk2aYdrMOVj5w1qMGfsFzkadwZiRn0KlUr39zUWYsXxfvA3H4RWjHAuZnrZ3dPr0aaxevRo1a9bUah83bhx27tyJrVu34siRI3j06BH8/f01+1UqFTp37ozs7GwcP34c69atQ3h4OKZMmaLT+Qv1T/bk5GSsW7fujccolUo8e/ZMa1MqlQaK0PiUdrLEtwENMHTZ31DmqPM9JvzgDRy88AiX76dgyz938OmKf9C1oQfKu+SuyZDLAbMSJvhy3SkcvPAIp28m4pMlR1HRzQYtqrka8nIMqkPHzmjZqg0qVa6CVm38sGjpSlyOuYioM6ekDo2IirG0tDQMGDAAP/zwAxwcHDTtqamp+PHHH7Fw4UK0adMG9erVw9q1a3H8+HGcOHECALB//35cvnwZv/zyC2rXro2OHTtixowZWL58ObKz806hv46kC0T/97//vXH/7du339pHWFgYpk2bptX2zeRQTJoy9X1CeycO9g4wMTFBUlKSVntSUhJKlixp8HjEULu8E0rZW+DvsC6athImcjSt6oLhHaqi5McboBYErfecuZn7l0oFF1vciU9D3NNMAMDVB6maY5KeK5H0TIkyJa0McBWFQ5kyZWHv4ID7sbFo2MhX6nBEYwzfFwXBcXjFGMdCXwtElUplnj+oFQoFFArFa98TFBSEzp07w8/PDzNnztS0R0VFIScnB35+fpq2qlWroly5coiMjETjxo0RGRmJGjVqwMXFRXNMhw4dMGLECMTExKBOnToFilvSZKN79+6QyWQQ/vPL6d/e9j8oJCQEwcHBWm2CyesHXUymZmbw9qmGkyci0aZt7v88tVqNkycj0bffx5LEpG9HLj1Go/HaSeLKEU1w/VEqFv0RkyfRAIAaHrmZdFxKBgDg5PUEAEBld1s8Ss5tc7Ayg5OtAvcT08QMv1CJj49DakoKSjo7Sx2KqIzh+6IgOA6vGONY6CvZyO8P7NDQUEydOjXf4zdt2oSzZ8/i9OnTefbFxcXBzMwM9vb2Wu0uLi6Ii4vTHPPvROPl/pf7CkrSZMPNzQ0rVqxAt27d8t0fHR2NevXqvbGP/DK6LN3WrejVwIBATP56IqpVq47qNWril/XrkJmZie49/N/+5iIgLesFrjxI0WpLV75A8nMlrjxIQXkXa3zUtDz2n3uI5DQlqpVzwJxBDXDschxiYnPfd/Pxc+w6HYtvAxpgzA8n8DwjB1P71cH1h89wNKbgX7yFTUZGOu7HxmpeP3z4ANeuXoGdnR1s7ezww6oVaOPXDk5OznjwIBZLFs1H2bLl4Nuk2Rt6LR6K+/dFQXEcXjG2sdDXna/5/YH9uqrG/fv38fnnnyMiIgLm5ub6CeAdSZps1KtXD1FRUa9NNt5W9SiMPujYCU+Tk7Fi2RIkJj6BV1VvrFi9Bk7FtDT4X9kv1GhV3Q0jO/rAUlECD5PS8cfJe5i3/aLWcZ+u+Adhg+pj65dtIAjAsStx8J9zAC9URev/979djonBZ0MDNK8Xzf8WANCla3d89U0obly/hl3/24Hnz5/DuZQzGvs2xWdBY4ziWRvG/n3xEsfhFY7Fu3nblMm/RUVFISEhAXXr1tW0qVQqHD16FMuWLcO+ffuQnZ2NlJQUrepGfHw8XF1z18+5urri1CntdWUv71Z5eUxByAQJf5v//fffSE9PxwcffJDv/vT0dJw5cwYtW7bUqV8pKxuFSamPf5Y6hELjYXjxLM3qyrREoV4TTiQJcwP82V15wl699HNjXv6/L/Pz/Plz3Lt3T6stMDAQVatWxcSJE1G2bFk4Ozvj119/Rc+ePQEA165dQ9WqVTVrNvbs2YMuXbrg8ePHKFWqFADg+++/x4QJE5CQkFDgxEfSykbz5s3fuN/KykrnRIOIiKiwkeIBojY2NqhevbpWm5WVFZycnDTtQ4YMQXBwMBwdHWFra4vRo0fD19cXjRs3BgC0b98ePj4+GDhwIObOnYu4uDhMmjQJQUFBBU40AD6unIiIyGgtWrQIcrkcPXv2hFKpRIcOHbBixQrNfhMTE+zatQsjRoyAr68vrKysEBAQgOnTp+t0HkmnUcTCaZRcnEZ5hdMouTiNQpSXIaZRvCbu00s/177toJd+DI2VDSIiIpEZ+eewFe4niBIREVHRx8oGERGRyORy4y5tMNkgIiISGadRiIiIiETEygYREZHI9PXZKEUVkw0iIiKRGXmuwWSDiIhIbMZe2eCaDSIiIhIVKxtEREQiM/bKBpMNIiIikRl5rsFpFCIiIhIXKxtEREQi4zQKERERicrIcw1OoxAREZG4WNkgIiISGadRiIiISFRGnmtwGoWIiIjExcoGERGRyDiNQkRERKIy8lyDyQYREZHYjL2ywTUbREREJKpiWdkQBKkjKBwSfhkkdQiFRpVx/5M6hELh+qKuUodAZJSMvLBRPJMNIiKiwoTTKEREREQiYmWDiIhIZEZe2GCyQUREJDZOoxARERGJiJUNIiIikRl5YYPJBhERkdg4jUJEREQkIlY2iIiIRGbslQ0mG0RERCIz8lyDyQYREZHYjL2ywTUbREREJCpWNoiIiERm5IUNJhtERERi4zQKERERkYhY2SAiIhKZkRc2mGwQERGJTW7k2QanUYiIiEhUrGwQERGJzMgLG0w2iIiIxGbsd6Mw2SAiIhKZ3LhzDa7ZICIiInGxskFERCQyTqMQERGRqIw81+A0iph+WvM9alf3wtw5s6QOxaB+/GE1+vfuCd8GddCquS/Gjh6Ju3duSx2WqEa2q4TYpV0R6l9N09a/iQc2j2mCmLkdEbu0K2wt8ub2Pw5viMhpfri+sDPOzGyPxQPrwMVWYcjQDWbTxg3o2K4NGtSpgQF9P8LFCxekDsngos6cxuiRn8GvVTPUquaFQwcPSB2SpPg1YTz0kmykpKToo5ti5dLFC9i2dROqVPGSOhSDO3P6FPr0G4D1v27B6h/W4sWLF/hs2BBkZGRIHZooapazR/+mHrj8MFWr3cLMBEeuJGB5xI3Xvvf4jUSMXBuF1jMO4dMfT6NcSSusHNJA7JANbu+e3Zg/NwyfjgzCpq3b4eVVFSM+HYKkpCSpQzOozMwMeHl5IWRSqNShSM7YviZkevqvqNI52fj222+xefNmzevevXvDyckJpUuXxvnz5/UaXFGVkZGOr7+agClTZ8LG1k7qcAxu5fc/olsPf1SqVBleVati+qw5ePz4Ea5cjpE6NL2zNDPBkoC6+OrX80jNyNHa9+Ph21gRcRNn7zx97ft//Os2zt19iodPMxF15ylWRtxAXU8HlChmS9fXr1sL/1690b1HT1SsVAmTQqfB3NwcO37/TerQDKpZ85YY9fk4tPVrJ3UokjO2rwm5TD9bUaVzsrFq1SqULVsWABAREYGIiAjs2bMHHTt2xIQJE/QeYFE0e+Z0NG/REo19m0gdSqGQ9vw5AMDWrvglXjN718ShmHgcu5b43n3ZWZqie4MyiLqTjBdqQQ/RFQ452dm4cjlG6/tBLpejceMmuHD+nISRkVT4NWF8dF4gGhcXp0k2du3ahd69e6N9+/bw9PREo0aN9B5gUbN395+4euUyNmzaJnUohYJarcbcb2ejdp26qFy5itTh6NWHdd1RvawdPpx39L36CenqjYAW5WGpKIGoO8kIXHVSTxEWDk9TnkKlUsHJyUmr3cnJCXeK+Voeyp8xfk0Y+90oOlc2HBwccP/+fQDA3r174efnBwAQBAEqlUrnADIzM3Hs2DFcvnw5z76srCz8/PPPb3y/UqnEs2fPtDalUqlzHPoQ9/gx5s6Zhdlz5kGhKJ6L/HQ1e+Y03LpxA3PnL5I6FL1yszfH1J41MGbdWShfqN+rr1UHb6Hjt0cwYFkk1GoBiwbV1VOURFRYyGT62YoqnZMNf39/9O/fH+3atUNSUhI6duwIADh37hwqVaqkU1/Xr1+Ht7c3WrRogRo1aqBly5Z4/PixZn9qaioCAwPf2EdYWBjs7Oy0tnnfhul6WXpx+XIMkpOT0K+3P+rV8kG9Wj6IOnMKv25Yj3q1fN4pGSvKZs+cjqNHDuOHtevg4uoqdTh6VaOcPZxtFdj9ZQvcXtwFtxd3gW/lkghsWQG3F3fRaW71aXo27jxJx9/XniAoPAptq7mgrqeDeMEbmIO9A0xMTPIs/EtKSkLJkiUlioqkxK8J46PzNMqiRYvg6emJ+/fvY+7cubC2tgYAPH78GCNHjtSpr4kTJ6J69eo4c+YMUlJSMHbsWDRt2hSHDx9GuXLlCtRHSEgIgoODtdrUcmmqCo0aN8a27Tu12qZMCkH58hUQOGQYTExMJInL0ARBQNisGTh0MAI/hq9HmTJlpQ5J7/659gR+s//SalswoDZuxadhxYGbeNclFy8/htqsRPG5K93UzAzePtVw8kQk2rTNrYSq1WqcPBmJvv0+ljg6koIxfk0Y+0fM65xsmJqaYvz48Xnax40bp/PJjx8/jgMHDqBkyZIoWbIkdu7ciZEjR6J58+b466+/YGVl9dY+FApFnimLzJzXHCwyKytrVPrPugQLC0vY2dvnaS/OZs+Yhj27d2Hx0hWwsrRC4pMnAABrGxuYm5tLHJ1+pCtVuP74uVZbRrYKT9OzNe3ONgo42yrg6Zz7dVzV3RZpWS/w8GkmUjNyUNvDHrU87HH6VjJSM3Lg4WyF8Z2r4u6TdJy9+/o7WIqigQGBmPz1RFSrVh3Va9TEL+vXITMzE917+EsdmkFlpKcjNjZW8/rhgwe4euUK7Ozs4ObuLmFkhmdsXxNGnmsULNn43//+V+AOu3btWuBjMzMzUaLEqxBkMhlWrlyJUaNGoWXLlti4cWOB+6LCY8vmXwEAQwYP1GqfPjMM3YrpD5L8fNzME+M6vXrOyraxzQAAwb+cw7aT95GZrcIHtdwQ3KkqLMxMkPAsC0cuP8GSfdeR/Z7rQAqbDzp2wtPkZKxYtgSJiU/gVdUbK1avgZORlcxjYi5haOAgzev5c3OnfLt264EZs+dIFZYkjO1rwtgXiMoEQXhrwVcuL1hJVyaT6bQuoWHDhhg9ejQGDhyYZ9+oUaOwYcMGPHv2TOe1DlJVNgobI//a1lJlXMET5uLs+qKC/zFAZCzMDfDBHb3WntVLP9sCi+YC8gJlEWq1ukCbrklBjx498Ouvv+a7b9myZejXrx8KkAsREREValLcjbJy5UrUrFkTtra2sLW1ha+vL/bs2aPZn5WVhaCgIDg5OcHa2ho9e/ZEfHy8Vh+xsbHo3LkzLC0tUapUKUyYMAEvXrzQ+frfaxVaVlbW+7wdISEh2L1792v3r1ixAmp18SonExGR8ZHLZHrZdFGmTBnMmTMHUVFROHPmDNq0aYNu3bohJib3ac7jxo3Dzp07sXXrVhw5cgSPHj2Cv/+rqW6VSoXOnTsjOzsbx48fx7p16xAeHo4pU6bofP0Fmkb5N5VKhdmzZ2PVqlWIj4/H9evXUaFCBUyePBmenp4YMmSIzkHoG6dRcnEa5RVOo+TiNApRXoaYRumzTj9PRt0cUOe93u/o6Ih58+ahV69ecHZ2xsaNG9GrVy8AwNWrV+Ht7Y3IyEg0btwYe/bsQZcuXfDo0SO4uLgAyH2K+MSJE/HkyROYmZkV+Lw6VzZmzZqF8PBwzJ07V+tE1atXx5o1a3TtjoiIqNiT6Wl71wdZqlQqbNq0Cenp6fD19UVUVBRycnI0D+YEgKpVq6JcuXKIjIwEAERGRqJGjRqaRAMAOnTogGfPnmmqIwWlc7Lx888/4/vvv8eAAQO0nhtRq1YtXL16VdfuiIiIij2ZTKaXLb8HWYaFvf5BlhcvXoS1tTUUCgU+++wzbN++HT4+PoiLi4OZmRns7e21jndxcUFcXByA3I8n+Xei8XL/y3260Ll49PDhw3yfFKpWq5GTw/kLIiIiseT3IMs3fTyGl5cXoqOjkZqaim3btiEgIABHjhwRO8w8dE42fHx88Pfff8PDw0Orfdu2bahT5/3mkoiIiIojfX08fH4PsnwTMzMzTYGgXr16OH36NL777jv06dMH2dnZSElJ0apuxMfHw/X/P17C1dUVp06d0urv5d0qrjp+BIXOycaUKVMQEBCAhw8fQq1W4/fff8e1a9fw888/Y9euXbp2R0REVOwVlod6qdVqKJVK1KtXD6ampjh48CB69uwJALh27RpiY2Ph6+sLAPD19cWsWbOQkJCAUqVKAQAiIiJga2sLHx8fnc6rc7LRrVs37Ny5E9OnT4eVlRWmTJmCunXrYufOnWjXrp2u3REREZEIQkJC0LFjR5QrVw7Pnz/Hxo0bcfjwYezbtw92dnYYMmQIgoOD4ejoCFtbW4wePRq+vr5o3LgxAKB9+/bw8fHBwIEDMXfuXMTFxWHSpEkICgrS+ZPN3+mGn+bNmyMiIuJd3kpERGR0pChsJCQkYNCgQXj8+DHs7OxQs2ZN7Nu3T1MYWLRoEeRyOXr27AmlUokOHTpgxYoVmvebmJhg165dGDFiBHx9fWFlZYWAgABMnz5d51h0fs7GS2fOnMGVK1cA5K7jqFev3rt0Iwo+ZyNXIanaFQp8zkYuPmeDKC9DPGdj0MYLeunn5/419dKPoek8xA8ePEC/fv3wzz//aBaVpKSkoEmTJti0aRPKlCmj7xiJiIiKNH0tEC2qdH7OxtChQ5GTk4MrV64gOTkZycnJuHLlCtRqNYYOHSpGjERERFSE6VzZOHLkCI4fPw4vr1cfne3l5YWlS5eiefPmeg2OiIioOCgsd6NIRedko2zZsvk+vEulUsHd3V0vQRERERUnxp1qvMM0yrx58zB69GicOXNG03bmzBl8/vnnmD9/vl6DIyIioqKvQJUNBwcHrRJQeno6GjVqhBIlct/+4sULlChRAp988gm6d+8uSqBERERFla4fD1/cFCjZWLx4schhEBERFV9GnmsULNkICAgQOw4iIiIqpt7rUSZZWVnIzs7WarO1tX2vgIiIiIobY78bRecFounp6Rg1ahRKlSoFKysrODg4aG1ERESkTSbTz1ZU6ZxsfPnllzh06BBWrlwJhUKBNWvWYNq0aXB3d8fPP/8sRoxERERUhOk8jbJz5078/PPPaNWqFQIDA9G8eXNUqlQJHh4e2LBhAwYMGCBGnEREREWWsd+NonNlIzk5GRUqVACQuz4jOTkZANCsWTMcPXpUv9EREREVA5xG0VGFChVw584dAEDVqlWxZcsWALkVj5cfzEZERESvyGQyvWxFlc7JRmBgIM6fPw8A+Oqrr7B8+XKYm5tj3LhxmDBhgt4DJCIioqJNJgiC8D4d3Lt3D1FRUahUqRJq1qypr7jeS9YLqSMgKpzKDtssdQiFwvlFPaQOoVCwtzKVOoRCwdJU/IrB6O1X9NLP0h7eeunH0N7rORsA4OHhAQ8PD33EQkREVCwV5SkQfShQsrFkyZICdzhmzJh3DoaIiIiKnwIlG4sWLSpQZzKZjMkGERHRf8iNu7BRsGTj5d0nREREpDtjTzZ0vhuFiIiISBfvvUCUiIiI3owLRImIiEhUnEYhIiIiEhErG0RERCIz8lmUd6ts/P333/j444/h6+uLhw8fAgDWr1+PY8eO6TU4IiKi4kAuk+llK6p0TjZ+++03dOjQARYWFjh37hyUSiUAIDU1FbNnz9Z7gEREREWdXE9bUaVz7DNnzsSqVavwww8/wNT01XP1mzZtirNnz+o1OCIiIir6dF6zce3aNbRo0SJPu52dHVJSUvQRExERUbFShGdA9ELnyoarqytu3ryZp/3YsWOoUKGCXoIiIiIqTrhmQ0fDhg3D559/jpMnT0Imk+HRo0fYsGEDxo8fjxEjRogRIxERERVhOk+jfPXVV1Cr1Wjbti0yMjLQokULKBQKjB8/HqNHjxYjRiIioiKtCBcl9ELnZEMmk+Gbb77BhAkTcPPmTaSlpcHHxwfW1tZixEdERFTkGfsTRN/5oV5mZmbw8fHRZyxERERUDOmcbLRu3fqNHyhz6NCh9wqIiIiouCnKizv1Qedko3bt2lqvc3JyEB0djUuXLiEgIEBfcRERERUbRp5r6J5sLFq0KN/2qVOnIi0t7b0DIiIiouJFb08//fjjj/HTTz/pqzsiIqJiQy7Tz1ZU6e1TXyMjI2Fubq6v7oiIiIoNGYpwpqAHOicb/v7+Wq8FQcDjx49x5swZTJ48WW+BERERFRdFuSqhDzonG3Z2dlqv5XI5vLy8MH36dLRv315vgRVlmzZuwLq1PyIx8QmqeFXFV19PRo2aNaUOy+A4DrmMaRzGdKqKyR/Vwur91zHp13MAgPkB9dHCxwWu9uZIV77A6ZtJmL7lPG7GPQcA9G3qiaVDG+Xbn/eYHUh8rjRY/GLauG4NfljxHXr2+RijgicCAB4+uI9VS+bj4vlzyMnORgPfphjzRQgcnUpKHK24Vi1fitUrl2u1eZYvj+0790gUEYlNp2RDpVIhMDAQNWrUgIODg1gxFWl79+zG/LlhmBQ6DTVq1MKG9esw4tMh+GPXXjg5OUkdnsFwHHIZ0zjULu+IQa0q4lJsilb7+bvJ+C3yHh4kpcPBWoEJ3aph6/iWqDfhT6gFATtO3cehi3Fa71k6tCEUpibFJtG4evkSdm7fhgqVqmjaMjMz8OWY4ahY2QsLl68BAPy0ehm+GT8ay3/cALm8KH+g+NtVrFQZq9a8WudnYqK3Wf1CydgrGzp9NZuYmKB9+/b8dNc3WL9uLfx79Ub3Hj1RsVIlTAqdBnNzc+z4/TepQzMojkMuYxkHK0UJrBreGMHhZ5Caka21b/2R24i8/gT3kzJw4d5ThP1+EWWcrFCupCUAICtHhYRnWZpNJQho5l0KG47eluJS9C4zIwOzpnyF8V+HwsbWVtN+6Xw04h4/wsTJM1GhUhVUqFQFX4XOwrUrMTh35qSEERuGiYkJSpZ01mzF/Q9YmUyml62o0jl1rl69Om7fLh4/BPQtJzsbVy7HoLFvE02bXC5H48ZNcOH8OQkjMyyOQy5jGodvB9ZFxPlHOHo5/o3HWZqZoF+z8ribkIaHyZn5HtO7iScys1XYeeaBGKEa3OJ5s9C4aXPUa+ir1Z6Tkw3IZDA1M9O0mZkpIJPLcbGYfX3kJzb2Htq1bo4uH/jh64nj8fjxI6lDIhHpnGzMnDkT48ePx65du/D48WM8e/ZMa9PVlStXsHbtWly9ehUAcPXqVYwYMQKffPJJkXsa6dOUp1CpVHnK405OTkhMTJQoKsPjOOQylnHo3rAsang4YOa2C689JrB1Jdxd6Y97q3uhbU03fDT/MHJU6nyPHdC8PH47EYusHJVYIRvMof17cOPaZQwbOTbPPp/qNWFhboHvly1CVlYmMjMzsGrJfKhVKiQlPjF8sAZUvWYtTJ8ZhuWr1uDryaF4+OABPhn0MdLTi++zmnjrawFNnz4dX3zxBTp16gQA6Nq1q1ZJRxAEyGQyqFQF/wGxd+9edOvWDdbW1sjIyMD27dsxaNAg1KpVC2q1Gu3bt8f+/fvRpk2b1/ahVCqhVGrP6womCigUigLHQUTvxt3RArP618VH8w9D+SL/5AEAtp24hyOX4+BiZ4GRH3hhzcgm6DzrYJ731K/oBK/Sdhj5Q9GfRkiIj8OyhXMwb+n3MMvn55G9gyNCZy/A4rkz8PuWDZDJ5WjbriMqe3kX+/UazZq30Py7ipcXatSohU7t22D/3r3o0bOXhJGJpwjPgOhFgZONadOm4bPPPsNff/2lt5NPnz4dEyZMwMyZM7Fp0yb0798fI0aMwKxZswAAISEhmDNnzhuTjbCwMEybNk2r7ZvJoZg0Zare4iwoB3sHmJiYICkpSas9KSkJJUsW79Xl/8ZxyGUM41DLwxGl7MxxcOqrO9FKmMjhW8UZQ9pWQulh26AWBDzPzMHzzBzcjk/DmVtJuLG8BzrVK4PtJ2O1+vu4RQVcvPcUF+49NfSl6N31qzF4+jQZwwP6aNrUKhUunIvC9m2/Yv/fUWjQuAk2/L4HqSlPYWJiAmsbW/h3bAU39zISRm54Nra2KOfhifux96QOhURS4GRDEAQAQMuWLfV28piYGPz8888AgN69e2PgwIHo1etVVjtgwACsXbv2jX2EhIQgODhYO1YTaaoapmZm8PaphpMnItGmrR8AQK1W4+TJSPTt97EkMUmB45DLGMbh6JV4NJ+0V6ttyZCGuPH4GZbuvgr1///c+DeZDJABUJTQ/uvdSlEC3RqUxczfXj8dU5TUrd8YP238Xavt2xmTUc6jPPoN+gQmJiaadjv73MWRZ8+cRMrTZDRp0cqQoUouIyMdD+7fR+cPu0odimj4QWw6EGMl7Ms+5XI5zM3NtZ7jYWNjg9TU1De+X6HIO2WS9ULvYRbYwIBATP56IqpVq47qNWril/XrkJmZie49/N/+5mKE45CruI9DetYLXH2o/T2aoXyBp2nZuPowFR7OVujesBz+uhSHpOdKuDtaYEwnb2TlqHDgwmOt93VvWBYmJjJsPV48/rq1tLJC+YqVtdrMLSxga2evad+zczs8PCvAzsERly9GY9nCb9Gr30CU8ygvRcgGs3Det2jRqjXc3d2RkJCAVcuXQW4ixwedukgdmmiK8noLfdAp2ahSpcpbE47k5OQC9+fp6YkbN26gYsWKAHIfeV6uXDnN/tjYWLi5uekSouQ+6NgJT5OTsWLZEiQmPoFXVW+sWL0GTsWkbF5QHIdcxj4OWTkqNK5SEsPbVYG9lSmePFMi8toTdJp1MM8zNPq3qIA/ox7iWWaORNEa3v3Yu/hhxXd4/iwVrm6lMSBwGD7qN0jqsEQXHx+PkC+/QGpKChwcHVG7Tj38vGEzHB0dpQ6NRCIThHzqnPmQy+VYvHhxnieI/pcuHzO/atUqlC1bFp07d853/9dff42EhASsWbOmwH0C0lY2iAqzssM2Sx1CoXB+UQ+pQygU7K1MpQ6hULA0Fb/ssPSfO3rpZ3TToln10qmy0bdvX5QqVUpvJ//ss8/euH/27Nl6OxcREZFU5PwgtoIpyk8uIyIikpKx/wot8M3cBZxtISIiItJS4MqGWv36B/YQERHR6/FuFCIiIhKVsT9no3g/E5eIiIgkx8oGERGRyIy8sMHKBhERkdjkMpleNl2EhYWhQYMGsLGxQalSpdC9e3dcu3ZN65isrCwEBQXByckJ1tbW6NmzJ+Lj47WOiY2NRefOnWFpaYlSpUphwoQJePFCtwdaMdkgIiIqho4cOYKgoCCcOHECERERyMnJQfv27ZGenq45Zty4cdi5cye2bt2KI0eO4NGjR/D3f/VxCiqVCp07d0Z2djaOHz+OdevWITw8HFOmTNEplgI/QbQo4RNEifLHJ4jm4hNEc/EJorkM8QTRn07Hvv2gAvikQbm3H/QaT548QalSpXDkyBG0aNECqampcHZ2xsaNGzUfgnr16lV4e3sjMjISjRs3xp49e9ClSxc8evQILi4uAHKf/j1x4kQ8efIEZmZmBTo3KxtEREQik+tpUyqVePbsmdamVCr/e7p8vfxg05efQRMVFYWcnBz4+flpjqlatSrKlSuHyMhIALmfWVajRg1NogEAHTp0wLNnzxATE6PT9RMREVEREBYWBjs7O60tLCzsre9Tq9UYO3YsmjZtiurVqwMA4uLiYGZmBnt7e61jXVxcEBcXpznm34nGy/0v9xUU70YhIiISmb4+8iMkJATBwcFabQqF4q3vCwoKwqVLl3Ds2DG9xKErJhtEREQi09eqEIVCUaDk4t9GjRqFXbt24ejRoyhTpoym3dXVFdnZ2UhJSdGqbsTHx8PV1VVzzKlTp7T6e3m3ystjCoLTKERERCKT4tZXQRAwatQobN++HYcOHUL58tofT1+vXj2Ympri4MGDmrZr164hNjYWvr6+AABfX19cvHgRCQkJmmMiIiJga2sLHx+fAsfCygYREVExFBQUhI0bN+KPP/6AjY2NZo2FnZ0dLCwsYGdnhyFDhiA4OBiOjo6wtbXF6NGj4evri8aNGwMA2rdvDx8fHwwcOBBz585FXFwcJk2ahKCgIJ0qLEw2iIiIRCbFA0RXrlwJAGjVqpVW+9q1azF48GAAwKJFiyCXy9GzZ08olUp06NABK1as0BxrYmKCXbt2YcSIEfD19YWVlRUCAgIwffp0nWLhczaIjAifs5GLz9nIxeds5DLEczY2nn2gl3761y3z9oMKIa7ZICIiIlFxGoWIiEhk+rr1tahiskFERCQyY59GMPbrJyIiIpGxskFERCQyTqMQERGRqIw71eA0ChEREYmMlQ0iIiKRcRqFiIzGzRUfSR1CoeDaLlTqEAqFp4dnSB2C0TD2aQQmG0RERCIz9sqGsSdbREREJDJWNoiIiERm3HUNJhtERESiM/JZFE6jEBERkbhY2SAiIhKZ3MgnUphsEBERiYzTKEREREQiYmWDiIhIZDJOoxAREZGYOI1CREREJCJWNoiIiETGu1GIiIhIVMY+jcJkg4iISGTGnmxwzQYRERGJipUNIiIikfHWVyIiIhKV3LhzDU6jEBERkbhY2SAiIhIZp1GIiIhIVLwbhYiIiEhErGwQERGJjNMoREREJCrejUJEREQkIiYbIti0cQM6tmuDBnVqYEDfj3DxwgWpQ5IExyGXsY3D2ajTCB4zAp3atUDD2t44fOiA1v6kpERMmxyCTu1aoHnjOhgzchhi792VJlg9+uaT1sg8NkNri94wRrP/k671sW/pJ4jf9w0yj82AnbV5nj62zhmA6799gacHp+D2ji/x46SecHOyMeRlGJQxfW/I9PRfUcVkQ8/27tmN+XPD8OnIIGzauh1eXlUx4tMhSEpKkjo0g+I45DLGccjKzETlKl6YEDI5zz5BEDBh3Cg8fHgf8xctxy+bfoebmztGffYJMjMzJIhWv2Jux8Oz67eare3INZp9lgpTRJy8gXnrj772/UfP3sbHUzajVv/v0H/Sr6hQ2hEbZ/Y1ROgGZ2zfGzKZfraiqtAlG4IgSB3Ce1m/bi38e/VG9x49UbFSJUwKnQZzc3Ps+P03qUMzKI5DLmMchybNWmDEqLFo3aZdnn2xsXdx6cJ5TPw6FD7Va8DDszwmfhMKZZYS+/b8KUG0+vVCpUZ8cppmS0p9lUAt2xqJ+b/8jZMx91/7/qVbInEq5gFi41Nx4tJ9zP/lbzSsVgYlTArdj+r3ZmzfGzI9bUVVofsKVigUuHLlitRhvJOc7GxcuRyDxr5NNG1yuRyNGzfBhfPnJIzMsDgOuTgOeeVk5wDI/T5/SS6Xw9TMDOfPnZUqLL2pVMYJt3dMwOUt47B2Si+UdbF7574cbCzQt31NnLh0Hy9Uaj1GKT1+bxgfye5GCQ4OzrddpVJhzpw5cHJyAgAsXLjwjf0olUoolUqtNsFEofXDzFCepjyFSqXSxP6Sk5MT7ty5bfB4pMJxyMVxyMvTszxc3dywfMkihEyeCgsLC2z8ZR0S4uOQmPhE6vDey+nLDzB89u+4HpsIVycbfBPYGgeWD0W9gUuRlpld4H5mjmiPz/wbwcrCDCcvxcL/y19EjFoaxvi9IS/KcyB6IFllY/Hixfjrr79w7tw5rU0QBFy5cgXnzp1DdHT0W/sJCwuDnZ2d1jbv2zDxL4CIdFbC1BTfLliK2Ht34deiMVo0rouo06fQpGlzyOWFrtCqk/0nbuD3v2Jw6VY8Dpy6ie4T1sPO2hw921TXqZ9FG4+h8Scr0HlsOFRqAWsm9RQpYjIkY59GkayyMXv2bHz//fdYsGAB2rRpo2k3NTVFeHg4fHx8CtRPSEhIniqJYGL4qgYAONg7wMTEJM8Cp6SkJJQsWVKSmKTAccjFccift081bNiyHWnPnyMnJwcOjo4I/LgPvH2qSR2aXqWmZeHm/URULOP09oP/JSk1A0mpGbh5PwnX7j3Bze0T0Kha2Teu9Shq+L1hfCT7U+Krr77C5s2bMWLECIwfPx45OTnv1I9CoYCtra3WJsUUCgCYmpnB26caTp6I1LSp1WqcPBmJmrXqSBKTFDgOuTgOb2ZtYwMHR0fE3ruLK5cvoUWrtlKHpFdWFmYoX9oRcUnP37kP+f8/CcrMzERfYRUKRvm9YeSlDUmfINqgQQNERUUhKCgI9evXx4YNGyAr4vNaAwMCMfnriahWrTqq16iJX9avQ2ZmJrr38Jc6NIPiOOQyxnHIyEjHg9hYzetHDx/g+tUrsLWzg6ubOw7s3wsHB0e4urnh5o3rWDh3Nlq2bovGTZpKGPX7CwvqgD//uYbYuBS4l7TBpCFtoFIJ2HIg99kRLo7WcHG0RsXSuZWO6hVc8DxDifvxqXj6PBMNfMqgXtXSOH7hHlKeZ6J8aUeEDm2LWw+ScPJS8alqvGRs3xtF+RkZ+iD548qtra2xbt06bNq0CX5+flCpVFKH9F4+6NgJT5OTsWLZEiQmPoFXVW+sWL0GTkZWGuQ45DLGcbgSE4MRwwI0rxcv+BYA0PnD7gidEYakxCdYvOBbJCcloaRzSXTq0g1Dho+QKly9Ke1sh5+nfgRHW0skpqTj+IVYtPx0NRJTcm9/Hdq9ASZ98mrK+MCKoQCAYbN+xy97ziEjKwfdWvpg0pA2sDI3RVxSGvafvIFvp2xGdk7R/rmYH2P83jBmMqEQPdjiwYMHiIqKgp+fH6ysrN65n6wXegyKqBhR5hSvWyjflWu7UKlDKBSeHp4hdQiFgrkB/uw+dTtVL/00rPDut1NLSfLKxr+VKVMGZcqUkToMIiIivTLuSZRC+FAvIiIiKl4KVWWDiIioWDLy0gaTDSIiIpHxbhQiIiISVRF/qsN745oNIiIiEhUrG0RERCIz8sIGkw0iIiLRGXm2wWkUIiIiEhUrG0RERCLj3ShEREQkKt6NQkRERCQiVjaIiIhEZuSFDSYbREREojPybIPTKERERMXU0aNH8eGHH8Ld3R0ymQw7duzQ2i8IAqZMmQI3NzdYWFjAz88PN27c0DomOTkZAwYMgK2tLezt7TFkyBCkpaXpFAeTDSIiIpHJ9PSfrtLT01GrVi0sX7483/1z587FkiVLsGrVKpw8eRJWVlbo0KEDsrKyNMcMGDAAMTExiIiIwK5du3D06FEMHz5ct+sXBEHQOfpCLuuF1BEQFU7KHLXUIRQKru1CpQ6hUHh6eIbUIRQK5gZYUHDxgW6VgNepUcb6nd8rk8mwfft2dO/eHUBuVcPd3R1ffPEFxo8fDwBITU2Fi4sLwsPD0bdvX1y5cgU+Pj44ffo06tevDwDYu3cvOnXqhAcPHsDd3b1A52Zlg4iISGQyPW1KpRLPnj3T2pRK5TvFdOfOHcTFxcHPz0/TZmdnh0aNGiEyMhIAEBkZCXt7e02iAQB+fn6Qy+U4efJkgc/FZIOIiKiICAsLg52dndYWFhb2Tn3FxcUBAFxcXLTaXVxcNPvi4uJQqlQprf0lSpSAo6Oj5piC4N0oREREYtPT3SghISEIDg7WalMoFPrpXERMNoiIiESmr8eVKxQKvSUXrq6uAID4+Hi4ublp2uPj41G7dm3NMQkJCVrve/HiBZKTkzXvLwhOoxARERmh8uXLw9XVFQcPHtS0PXv2DCdPnoSvry8AwNfXFykpKYiKitIcc+jQIajVajRq1KjA52Jlg4iISGRSfTZKWloabt68qXl9584dREdHw9HREeXKlcPYsWMxc+ZMVK5cGeXLl8fkyZPh7u6uuWPF29sbH3zwAYYNG4ZVq1YhJycHo0aNQt++fQt8JwrAZIOIiEh0Uj1A9MyZM2jdurXm9cv1HgEBAQgPD8eXX36J9PR0DB8+HCkpKWjWrBn27t0Lc3NzzXs2bNiAUaNGoW3btpDL5ejZsyeWLFmiUxx8zgaREeFzNnLxORu5+JyNXIZ4zsaVR+l66cfb3Uov/Rgakw0iIiPl0GCU1CEUCpnnlol+jiuP9ZRsuBXNZIPTKERERCLT190oRRXvRiEiIiJRsbJBREQkMqnuRiksmGwQERGJzMhzDSYbREREojPybINrNoiIiEhUrGwQERGJzNjvRmGyQUREJDJjXyDKaRQiIiISFSsbREREIjPywgaTDSIiItEZebbBaRQiIiISFSsbREREIuPdKERERCQq3o1CREREJCJWNoiIiERm5IUNJhtERESiM/Jsg8kGERGRyIx9gSjXbBAREZGoWNkgIiISmbHfjcJkg4iISGRGnmtwGoWIiIjExcoGERGRyDiNQkRERCIz7myD0ygi2LRxAzq2a4MGdWpgQN+PcPHCBalDkgTHIRfHIRfHIVdxH4dvPu2EzHPLtLbo3ydp9i/9pi9i/heK5MiFiD0Uhi2LhqOKp4tm/8cfNsrz/pebs4O1FJdEesBkQ8/27tmN+XPD8OnIIGzauh1eXlUx4tMhSEpKkjo0g+I45OI45OI45DKWcYi5+QiefiGare0nizT7zl25j+FTf0Ft/5noOnI5ZDIZdq0Iglye+5f/tv1ntd7r6ReC/f9cxtEzN/DkaZpUl/TeZDL9bEUVkw09W79uLfx79Ub3Hj1RsVIlTAqdBnNzc+z4/TepQzMojkMujkMujkMuYxmHFyo14pOea7aklHTNvp9+/wf/nL2F2MfJiL76ANOW70RZN0d4uDsBALKUOVrvVakFtGpYBeE7jkt1OXoh09NWVDHZ0KOc7GxcuRyDxr5NNG1yuRyNGzfBhfPnJIzMsDgOuTgOuTgOuYxpHCqVc8bt/bNweedUrJ0VgLKuDvkeZ2luhkFdG+POg0Q8iHua7zEDujRERlY2th+IFjFiEhuTDT16mvIUKpUKTk5OWu1OTk5ITEyUKCrD4zjk4jjk4jjkMpZxOH3pLoZP+QVdg5ZjzOzN8CzthAM/jYO1pUJzzPCPmuPJPwuQFLkQ7Zv6oPOIZch5ocq3v4Duvti85wyylDmGugRRGPs0SqG6GyU9PR1btmzBzZs34ebmhn79+uX5xvwvpVIJpVKp1SaYKKBQKF7zDiIiEsv+fy5r/n3pxiOcvngX13ZPR8/2dbFuRyQAYNOe0zh48ipcS9pi7CA//PLtJ2gTuBDK7BdafTWqWR7eFdwwZNLPBr0GMfCzUSTk4+OD5ORkAMD9+/dRvXp1jBs3DhEREQgNDYWPjw/u3Lnzxj7CwsJgZ2entc37NswQ4efhYO8AExOTPIu9kpKSULJkSUlikgLHIRfHIRfHIZexjkNqWiZuxiagYllnTduztCzcin2Cf87eQv/xa+BV3gXd2tTK897BPXwRffU+zl25b8iQxWHkizYkTTauXr2KFy9yM9mQkBC4u7vj3r17OHXqFO7du4eaNWvim2++eWMfISEhSE1N1domTAwxRPh5mJqZwdunGk6eiNS0qdVqnDwZiZq16kgSkxQ4Drk4Drk4DrmMdRysLMxQvkxJxCWm5rtfJpNBBhnMTEvkeV/Pdq+qIVS0FZpplMjISKxatQp2dnYAAGtra0ybNg19+/Z94/sUirxTJlkvXnOwAQwMCMTkryeiWrXqqF6jJn5Zvw6ZmZno3sNfuqAkwHHIxXHIxXHIZQzjEDauB/48ehGxj5LhXsoOkz7rDJVajS17o+BZ2gm9OtTDwcgrSHyahtIu9vgisD0ylTnYdyxGq59eHeqhhIkcv/55WqIr0a8iXJTQC8mTDdn/r3jJysqCm5ub1r7SpUvjyZMnUoT1zj7o2AlPk5OxYtkSJCY+gVdVb6xYvQZOxbhMmh+OQy6OQy6OQy5jGIfSLvb4OSwQjnaWSHyahuPRt9Fy0AIkPk2DaQkTNK1TEaP6t4KDrSUSkp7j2NmbaD14QZ5naAzu7os/Dp1HalqmRFeiX0V5cac+yARBEKQ6uVwuR/Xq1VGiRAncuHED4eHh6Nmzp2b/0aNH0b9/fzx48ECnfqWsbBARFRUODUZJHUKhkHlumejnSHiun7tpStmY6qUfQ5O0shEaGqr12tpa+1G0O3fuRPPmzQ0ZEhERkd4Z+90oklY2xMLKBhHR27GykcsQlY0nafr5xeRsLfnqh3fCh3oRERGRqIpmikRERFSEGPckCpMNIiIi0Rn73SicRiEiIiJRsbJBREQkMmO/G4XJBhERkcg4jUJEREQkIiYbREREJCpOoxAREYnM2KdRmGwQERGJzNgXiHIahYiIiETFygYREZHIOI1CREREojLyXIPTKERERCQuVjaIiIjEZuSlDSYbREREIuPdKEREREQiYmWDiIhIZLwbhYiIiERl5LkGp1GIiIhEJ9PT9g6WL18OT09PmJubo1GjRjh16tR7Xcq7YLJBRERUTG3evBnBwcEIDQ3F2bNnUatWLXTo0AEJCQkGjYPJBhERkchkevpPVwsXLsSwYcMQGBgIHx8frFq1CpaWlvjpp59EuMrXY7JBREQkMplMP5susrOzERUVBT8/P02bXC6Hn58fIiMj9XyFb8YFokREREWEUqmEUqnUalMoFFAoFHmOTUxMhEqlgouLi1a7i4sLrl69KmqceQikd1lZWUJoaKiQlZUldSiS4ji8wrHIxXHIxXHIxXHQXWhoqABAawsNDc332IcPHwoAhOPHj2u1T5gwQWjYsKEBon1FJgiCYNj0pvh79uwZ7OzskJqaCltbW6nDkQzH4RWORS6OQy6OQy6Og+50qWxkZ2fD0tIS27ZtQ/fu3TXtAQEBSElJwR9//CF2uBpcs0FERFREKBQK2Nraam35JRoAYGZmhnr16uHgwYOaNrVajYMHD8LX19dQIQPgmg0iIqJiKzg4GAEBAahfvz4aNmyIxYsXIz09HYGBgQaNg8kGERFRMdWnTx88efIEU6ZMQVxcHGrXro29e/fmWTQqNiYbIlAoFAgNDX1tactYcBxe4Vjk4jjk4jjk4jgYxqhRozBq1ChJY+ACUSIiIhIVF4gSERGRqJhsEBERkaiYbBAREZGomGwQERGRqJhsiGD58uXw9PSEubk5GjVqhFOnTkkdksEdPXoUH374Idzd3SGTybBjxw6pQzK4sLAwNGjQADY2NihVqhS6d++Oa9euSR2Wwa1cuRI1a9bUPIDI19cXe/bskTosyc2ZMwcymQxjx46VOhSDmzp1KmQymdZWtWpVqcMiETHZ0LPNmzcjODgYoaGhOHv2LGrVqoUOHTogISFB6tAMKj09HbVq1cLy5culDkUyR44cQVBQEE6cOIGIiAjk5OSgffv2SE9Plzo0gypTpgzmzJmDqKgonDlzBm3atEG3bt0QExMjdWiSOX36NFavXo2aNWtKHYpkqlWrhsePH2u2Y8eOSR0SiYi3vupZo0aN0KBBAyxbtgxA7qNhy5Yti9GjR+Orr76SODppyGQybN++XevZ/MboyZMnKFWqFI4cOYIWLVpIHY6kHB0dMW/ePAwZMkTqUAwuLS0NdevWxYoVKzBz5kzUrl0bixcvljosg5o6dSp27NiB6OhoqUMhA2FlQ4+ys7MRFRUFPz8/TZtcLoefnx8iIyMljIwKg9TUVAC5v2iNlUqlwqZNm5Cenm7wz2YoLIKCgtC5c2etnxPG6MaNG3B3d0eFChUwYMAAxMbGSh0SiYhPENWjxMREqFSqPI+BdXFxwdWrVyWKigoDtVqNsWPHomnTpqhevbrU4RjcxYsX4evri6ysLFhbW2P79u3w8fGROiyD27RpE86ePYvTp09LHYqkGjVqhPDwcHh5eeHx48eYNm0amjdvjkuXLsHGxkbq8EgETDaIDCAoKAiXLl0y2nlpLy8vREdHIzU1Fdu2bUNAQACOHDliVAnH/fv38fnnnyMiIgLm5uZShyOpjh07av5ds2ZNNGrUCB4eHtiyZYtRTq0ZAyYbelSyZEmYmJggPj5eqz0+Ph6urq4SRUVSGzVqFHbt2oWjR4+iTJkyUocjCTMzM1SqVAkAUK9ePZw+fRrfffcdVq9eLXFkhhMVFYWEhATUrVtX06ZSqXD06FEsW7YMSqUSJiYmEkYoHXt7e1SpUgU3b96UOhQSCdds6JGZmRnq1auHgwcPatrUajUOHjxotPPTxkwQBIwaNQrbt2/HoUOHUL58ealDKjTUajWUSqXUYRhU27ZtcfHiRURHR2u2+vXrY8CAAYiOjjbaRAPIXTR769YtuLm5SR0KiYSVDT0LDg5GQEAA6tevj4YNG2Lx4sVIT09HYGCg1KEZVFpamtZfKXfu3EF0dDQcHR1Rrlw5CSMznKCgIGzcuBF//PEHbGxsEBcXBwCws7ODhYWFxNEZTkhICDp27Ihy5crh+fPn2LhxIw4fPox9+/ZJHZpB2djY5FmvY2VlBScnJ6NbxzN+/Hh8+OGH8PDwwKNHjxAaGgoTExP069dP6tBIJEw29KxPnz548uQJpkyZgri4ONSuXRt79+7Ns2i0uDtz5gxat26teR0cHAwACAgIQHh4uERRGdbKlSsBAK1atdJqX7t2LQYPHmz4gCSSkJCAQYMG4fHjx7Czs0PNmjWxb98+tGvXTurQSCIPHjxAv379kJSUBGdnZzRr1gwnTpyAs7Oz1KGRSPicDSIiIhIV12wQERGRqJhsEBERkaiYbBAREZGomGwQERGRqJhsEBERkaiYbBAREZGomGwQERGRqJhsEElo8ODB6N69u+Z1q1atMHbsWIPHcfjwYchkMqSkpLz2GJlMhh07dhS4z6lTp6J27drvFdfdu3chk8kQHR39Xv0QkbSYbBD9x+DBgyGTySCTyTQfIDZ9+nS8ePFC9HP//vvvmDFjRoGOLUiCQERUGPBx5UT5+OCDD7B27VoolUrs3r0bQUFBMDU1RUhISJ5js7OzYWZmppfzOjo66qUfIqLChJUNonwoFAq4urrCw8MDI0aMgJ+fH/73v/8BeDX1MWvWLLi7u8PLywsAcP/+ffTu3Rv29vZwdHREt27dcPfuXU2fKpUKwcHBsLe3h5OTE7788kv899MC/juNolQqMXHiRJQtWxYKhQKVKlXCjz/+iLt372o+e8bBwQEymUzzeStqtRphYWEoX748LCwsUKtWLWzbtk3rPLt370aVKlVgYWGB1q1ba8VZUBMnTkSVKlVgaWmJChUqYPLkycjJyclz3OrVq1G2bFlYWlqid+/eSE1N1dq/Zs0aeHt7w9zcHFWrVsWKFStee86nT59iwIABcHZ2hoWFBSpXroy1a9fqHDsRGRYrG0QFYGFhgaSkJM3rgwcPwtbWFhEREQCAnJwcdOjQAb6+vvj7779RokQJzJw5Ex988AEuXLgAMzMzLFiwAOHh4fjpp5/g7e2NBQsWYPv27WjTps1rzzto0CBERkZiyZIlqFWrFu7cuYPExESULVsWv/32G3r27Ilr167B1tZW80myYWFh+OWXX7Bq1SpUrlwZR48exccffwxnZ2e0bNkS9+/fh7+/P4KCgjB8+HCcOXMGX3zxhc5jYmNjg/DwcLi7u+PixYsYNmwYbGxs8OWXX2qOuXnzJrZs2YKdO3fi2bNnGDJkCEaOHIkNGzYAADZs2IApU6Zg2bJlqFOnDs6dO4dhw4bBysoKAQEBec45efJkXL58GXv27EHJkiVx8+ZNZGZm6hw7ERmYQERaAgIChG7dugmCIAhqtVqIiIgQFAqFMH78eM1+FxcXQalUat6zfv16wcvLS1Cr1Zo2pVIpWFhYCPv27RMEQRDc3NyEuXPnavbn5OQIZcqU0ZxLEAShZcuWwueffy4IgiBcu3ZNACBERETkG+dff/0lABCePn2qacvKyhIsLS2F48ePax07ZMgQoV+/foIgCEJISIjg4+OjtX/ixIl5+vovAML27dtfu3/evHlCvXr1NK9DQ0MFExMT4cGDB5q2PXv2CHK5XHj8+LEgCIJQsWJFYePGjVr9zJgxQ/D19RUEQRDu3LkjABDOnTsnCIIgfPjhh0JgYOBrYyCiwomVDaJ87Nq1C9bW1sjJyYFarUb//v0xdepUzf4aNWpordM4f/48bt68CRsbG61+srKycOvWLaSmpuLx48do1KiRZl+JEiVQv379PFMpL0VHR8PExAQtW7YscNw3b95ERkZGno9vz87ORp06dQAAV65c0YoDAHx9fQt8jpc2b96MJUuW4NatW0hLS8OLFy9ga2urdUy5cuVQunRprfOo1Wpcu3YNNjY2uHXrFoYMGYJhw4Zpjnnx4gXs7OzyPeeIESPQs2dPnD17Fu3bt0f37t3RpEkTnWMnIsNiskGUj9atW2PlypUwMzODu7s7SpTQ/laxsrLSep2WloZ69epppgf+zdnZ+Z1ieDktoou0tDQAwJ9//qn1Sx7IXYeiL5GRkRgwYACmTZuGDh06wM7ODps2bcKCBQt0jvWHH37Ik/yYmJjk+56OHTvi3r172L17NyIiItC2bVsEBQVh/vz5734xRCQ6JhtE+bCyskKlSpUKfHzdunWxefNmlCpVKs9f9y+5ubnh5MmTaNGiBYDcv+CjoqJQt27dfI+vUaMG1Go1jhw5Aj8/vzz7X1ZWVCqVps3HxwcKhQKxsbGvrYh4e3trFru+dOLEibdf5L8cP34cHh4e+OabbzRt9+7dy3NcbGwsHj16BHd3d8155HI5vLy84OLiAnd3d9y+fRsDBgwo8LmdnZ0REBCAgIAANG/eHBMmTGCyQVTI8W4UIj0YMGAASpYsiW7duuHvv//GnTt3cPjwYYwZMwYPHjwAAHz++eeYM2cOduzYgatXr2LkyJFvfEaGp6cnAgIC8Mknn2DHjh2aPrds2QIA8PDwgEwmw65du/DkyROkpaXBxsYG48ePx7hx47Bu3TrcunULZ8+exdKlS7Fu3ToAwGeffYYbN25gwoQJuHbtGjZu3Ijw8HCdrrdy5cqIjY3Fpk2bcOvWLSxZsgTbt2/Pc5y5uTkCAgJw/vx5/P333xgzZgx69+4NV1dXAMC0adMQFhaGJUuW4Pr167h48SLWrl2LhQsX5nveKVOm4I8//sDNmzcRExODXbt2wdvbW6fYicjwmGwQ6YGlpSWOHj2KcuXKwd/fH97e3hgyZAiysrI0lY4vvvgCAwcOREBAAHx9fWFjY4MePXq8sd+VK1eiV69eGDlyJKpWrYphw4YhPT0dAFC6dGlMmzYNX331FVxcXDBq1CgAwIwZMzB58mSEhYXB29sbH3zwAf7880+UL18eQO46it9++w07duxArVq1sGrVKsyePVun6+3atSvGjRuHUaNGoXbt2jh+/DgmT56c57hKlSrB398fnTp1Qvv27VGzZk2tW1uHDh2KNWvWYO3atahRowZatmyJ8PBwTaz/ZWZmhpCQENSsWRMtWrSAiYkJNm3apFPsRGR4MuF1q9OIiIiI9ICVDSIiIhIVkw0iIiISFZMNIiIiEhWTDSIiIhIVkw0iIiISFZMNIiIiEhWTDSIiIhIVkw0iIiISFZMNIiIiEhWTDSIiIhIVkw0iIiISFZMNIiIiEtX/AUcCidPWGftlAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "model = tf.keras.models.load_model('/content/resnet.h5')\n",
        "outs = model.predict(x_test)\n",
        "\n",
        "class_names= np.unique(np.argmax(y_test,axis=1))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(np.argmax(y_test,axis=1), np.argmax(outs,axis=1))\n",
        "\n",
        "print(classification_report(np.argmax(y_test,axis=1), np.argmax(outs,axis=1)))\n",
        "\n",
        "\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "# set the axis labels and title of the plot\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "plt.savefig('confusion_matrix.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "aFk7JhV1xvKa",
        "outputId": "f9ecdaa8-9ce1-43a8-942f-39abf8fed0c5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['categorical_accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_categorical_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPD-L19PXiur"
      },
      "source": [
        "## resnet like multi-filters module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8RHed-d8nGY"
      },
      "outputs": [],
      "source": [
        "def res_identity(x, filters): \n",
        "  #renet block where dimension doesnot change.\n",
        "  #The skip connection is just simple identity conncection\n",
        "  #we will have 3 blocks and then input will be added\n",
        "\n",
        "  x_skip = x # this will be used for addition with the residual block \n",
        "  f1, f2 = filters\n",
        "\n",
        "  #first block \n",
        "  #kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=1, strides=1, padding='same')(x)\n",
        "  \n",
        "\n",
        "  #second block # bottleneck (but size kept same with padding)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.Conv1D(f2, kernel_size=3, strides=1, padding='same', )(x)\n",
        "  \n",
        "\n",
        "  # third block activation used after adding the input\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=1, strides=1, padding='same')(x)\n",
        "\n",
        "  # add the input \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def conv_skip(x, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=2, padding='valid')(x)\n",
        "  # when s = 2 then it is like downsizing the feature map\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  # x = tf.keras.layers.ZeroPadding1D(padding=(1,1))(x)\n",
        "  \n",
        "\n",
        "  # second block\n",
        "  x = tf.keras.layers.Conv1D(f2, kernel_size=1, strides=1, padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  #third block\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  # x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "\n",
        "  # shortcut \n",
        "  x_skip = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=2, padding='valid')(x_skip)\n",
        "  # x_skip = tf.keras.layers.BatchNormalization()(x_skip)\n",
        "\n",
        "  # add \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def multi_fiter_conv(x, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x1 = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "  x1 = tf.keras.layers.Activation(tf.keras.activations.relu)(x1)\n",
        "  \n",
        "\n",
        "  # second block\n",
        "  x2 = tf.keras.layers.Conv1D(f1, kernel_size=5, strides=1, padding='same')(x)\n",
        "  x2 = tf.keras.layers.BatchNormalization()(x2)\n",
        "  x2 = tf.keras.layers.Activation(tf.keras.activations.relu)(x2)\n",
        "\n",
        "  #third block\n",
        "  x3 = tf.keras.layers.Conv1D(f1, kernel_size=7, strides=1, padding='same')(x)\n",
        "  x3 = tf.keras.layers.BatchNormalization()(x3)\n",
        "  x3 = tf.keras.layers.Activation(tf.keras.activations.relu)(x3)\n",
        "\n",
        "\n",
        "  # # forth block\n",
        "  # x4 = tf.keras.layers.Conv1D(f1, kernel_size=9, strides=1, padding='same')(x)\n",
        "  # x4 = tf.keras.layers.BatchNormalization()(x4)\n",
        "  # x4 = tf.keras.layers.Activation(tf.keras.activations.relu)(x4)\n",
        "\n",
        "  # concatenate \n",
        "  x = tf.keras.layers.Concatenate()([x1,x2,x3])\n",
        "  x = tf.keras.layers.Conv1D(f1,kernel_size=3,strides=1,padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  #add\n",
        "  x = tf.keras.layers.Add()([x,x_skip])\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "\n",
        "def buildAE(input_shape,classes,learning_rate):\n",
        "\n",
        "    input = tf.keras.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.ZeroPadding1D(padding=(1,1))(input)\n",
        "    x = tf.keras.layers.Conv1D(16, kernel_size=3, strides=1,padding='valid',)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)(x)\n",
        "\n",
        "    x = multi_fiter_conv(x,filters=(16,16))\n",
        "\n",
        "    x = conv_skip(x ,filters=(16,32))\n",
        "    # print(x.shape)\n",
        "    x = res_identity(x,filters=(16,32))\n",
        "    x = multi_fiter_conv(x,filters=(16,16))\n",
        "    x = conv_skip(x ,filters=(16,32))\n",
        "    x = res_identity(x,filters=(16,32))\n",
        "    x = res_identity(x,filters=(16,32))\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "    # x = maxpool_skip(x,filters=(16,16))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=16,activation='tanh',return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=32,activation='tanh'))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(units=64)(x)\n",
        "    x = tf.keras.layers.Dense(units=classes)(x)\n",
        "    output = tf.keras.layers.Activation('softmax')(x)\n",
        "\n",
        "\n",
        "    model = tf.keras.Model(input,output)\n",
        "\n",
        "\n",
        "    \n",
        "    print('Params', model.count_params())\n",
        "    model.compile(loss = tf.keras.losses.CategoricalCrossentropy(),\n",
        "                  metrics=[tf.keras.metrics.CategoricalAccuracy(),],\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
        "\n",
        "\n",
        "    # model.summary()\n",
        "    \n",
        "    return model\n",
        "model = buildAE((128,9),6,0.0005)\n",
        "# tf.keras.utils.plot_model(model,show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgWsPkvRh-JW",
        "outputId": "b8049137-9a7f-4893-95ff-9d3c74436f59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7352, 128, 9) (7352, 6)\n",
            "Params 54886\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 128, 9)]     0           []                               \n",
            "                                                                                                  \n",
            " zero_padding1d_3 (ZeroPadding1  (None, 130, 9)      0           ['input_3[0][0]']                \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv1d_43 (Conv1D)             (None, 128, 16)      448         ['zero_padding1d_3[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 128, 16)     64          ['conv1d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 128, 16)      0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 63, 16)      0           ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_44 (Conv1D)             (None, 63, 16)       784         ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_45 (Conv1D)             (None, 63, 16)       1296        ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_46 (Conv1D)             (None, 63, 16)       1808        ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 63, 16)      64          ['conv1d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 63, 16)      64          ['conv1d_45[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 63, 16)      64          ['conv1d_46[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 63, 16)       0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 63, 16)       0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 63, 16)       0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 63, 48)       0           ['activation_43[0][0]',          \n",
            "                                                                  'activation_44[0][0]',          \n",
            "                                                                  'activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_47 (Conv1D)             (None, 63, 16)       2320        ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 63, 16)      64          ['conv1d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 63, 16)       0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 63, 16)       0           ['activation_46[0][0]',          \n",
            "                                                                  'max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_48 (Conv1D)             (None, 31, 16)       784         ['add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 31, 16)      64          ['conv1d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 31, 16)       0           ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_49 (Conv1D)             (None, 31, 32)       544         ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 31, 32)      128         ['conv1d_49[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 31, 32)       0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_50 (Conv1D)             (None, 31, 16)       1552        ['activation_48[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_51 (Conv1D)             (None, 31, 16)       784         ['add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 31, 16)       0           ['conv1d_50[0][0]',              \n",
            "                                                                  'conv1d_51[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 31, 16)      64          ['add_13[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 31, 16)       0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_52 (Conv1D)             (None, 31, 16)       272         ['activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 31, 16)      64          ['conv1d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 31, 16)       0           ['batch_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_53 (Conv1D)             (None, 31, 32)       1568        ['activation_50[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 31, 32)      128         ['conv1d_53[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 31, 32)       0           ['batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_54 (Conv1D)             (None, 31, 16)       528         ['activation_51[0][0]']          \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 31, 16)       0           ['conv1d_54[0][0]',              \n",
            "                                                                  'add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, 31, 16)       0           ['add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_55 (Conv1D)             (None, 31, 16)       784         ['activation_52[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_56 (Conv1D)             (None, 31, 16)       1296        ['activation_52[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_57 (Conv1D)             (None, 31, 16)       1808        ['activation_52[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 31, 16)      64          ['conv1d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, 31, 16)      64          ['conv1d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, 31, 16)      64          ['conv1d_57[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_53 (Activation)     (None, 31, 16)       0           ['batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " activation_54 (Activation)     (None, 31, 16)       0           ['batch_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " activation_55 (Activation)     (None, 31, 16)       0           ['batch_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 31, 48)       0           ['activation_53[0][0]',          \n",
            "                                                                  'activation_54[0][0]',          \n",
            "                                                                  'activation_55[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_58 (Conv1D)             (None, 31, 16)       2320        ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, 31, 16)      64          ['conv1d_58[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_56 (Activation)     (None, 31, 16)       0           ['batch_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 31, 16)       0           ['activation_56[0][0]',          \n",
            "                                                                  'activation_52[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_59 (Conv1D)             (None, 15, 16)       784         ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, 15, 16)      64          ['conv1d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_57 (Activation)     (None, 15, 16)       0           ['batch_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_60 (Conv1D)             (None, 15, 32)       544         ['activation_57[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_59 (BatchN  (None, 15, 32)      128         ['conv1d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_58 (Activation)     (None, 15, 32)       0           ['batch_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_61 (Conv1D)             (None, 15, 16)       1552        ['activation_58[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_62 (Conv1D)             (None, 15, 16)       784         ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 15, 16)       0           ['conv1d_61[0][0]',              \n",
            "                                                                  'conv1d_62[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_60 (BatchN  (None, 15, 16)      64          ['add_16[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_59 (Activation)     (None, 15, 16)       0           ['batch_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_63 (Conv1D)             (None, 15, 16)       272         ['activation_59[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_61 (BatchN  (None, 15, 16)      64          ['conv1d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_60 (Activation)     (None, 15, 16)       0           ['batch_normalization_61[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_64 (Conv1D)             (None, 15, 32)       1568        ['activation_60[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_62 (BatchN  (None, 15, 32)      128         ['conv1d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_61 (Activation)     (None, 15, 32)       0           ['batch_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_65 (Conv1D)             (None, 15, 16)       528         ['activation_61[0][0]']          \n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 15, 16)       0           ['conv1d_65[0][0]',              \n",
            "                                                                  'add_16[0][0]']                 \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, 15, 16)       0           ['add_17[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_63 (BatchN  (None, 15, 16)      64          ['activation_62[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, 15, 16)       0           ['batch_normalization_63[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_66 (Conv1D)             (None, 15, 16)       272         ['activation_63[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_64 (BatchN  (None, 15, 16)      64          ['conv1d_66[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_64 (Activation)     (None, 15, 16)       0           ['batch_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_67 (Conv1D)             (None, 15, 32)       1568        ['activation_64[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_65 (BatchN  (None, 15, 32)      128         ['conv1d_67[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_65 (Activation)     (None, 15, 32)       0           ['batch_normalization_65[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_68 (Conv1D)             (None, 15, 16)       528         ['activation_65[0][0]']          \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 15, 16)       0           ['conv1d_68[0][0]',              \n",
            "                                                                  'activation_62[0][0]']          \n",
            "                                                                                                  \n",
            " activation_66 (Activation)     (None, 15, 16)       0           ['add_18[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, 15, 16)      64          ['activation_66[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_67 (Activation)     (None, 15, 16)       0           ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 15, 16)       0           ['activation_67[0][0]']          \n",
            "                                                                                                  \n",
            " bidirectional_4 (Bidirectional  (None, 15, 32)      4224        ['dropout_4[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, 15, 32)      128         ['bidirectional_4[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 15, 32)       0           ['batch_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " bidirectional_5 (Bidirectional  (None, 64)          16640       ['dropout_5[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, 64)          256         ['bidirectional_5[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 64)           4160        ['batch_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 6)            390         ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " activation_68 (Activation)     (None, 6)            0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 54,886\n",
            "Trainable params: 53,798\n",
            "Non-trainable params: 1,088\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/101\n",
            "114/114 [==============================] - 39s 78ms/step - loss: 1.4199 - categorical_accuracy: 0.4768 - val_loss: 1.6646 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
            "Epoch 2/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.5257 - categorical_accuracy: 0.7832 - val_loss: 0.6247 - val_categorical_accuracy: 0.7346 - lr: 0.0010\n",
            "Epoch 3/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.2693 - categorical_accuracy: 0.8961 - val_loss: 0.3145 - val_categorical_accuracy: 0.8935 - lr: 0.0010\n",
            "Epoch 4/101\n",
            "114/114 [==============================] - 5s 44ms/step - loss: 0.1789 - categorical_accuracy: 0.9306 - val_loss: 0.3129 - val_categorical_accuracy: 0.9026 - lr: 0.0010\n",
            "Epoch 5/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.1673 - categorical_accuracy: 0.9335 - val_loss: 0.3800 - val_categorical_accuracy: 0.9026 - lr: 0.0010\n",
            "Epoch 6/101\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.1429 - categorical_accuracy: 0.9401 - val_loss: 0.4054 - val_categorical_accuracy: 0.8989 - lr: 0.0010\n",
            "Epoch 7/101\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.1418 - categorical_accuracy: 0.9385 - val_loss: 0.4268 - val_categorical_accuracy: 0.8992 - lr: 0.0010\n",
            "Epoch 8/101\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.1226 - categorical_accuracy: 0.9459 - val_loss: 0.3922 - val_categorical_accuracy: 0.8921 - lr: 0.0010\n",
            "Epoch 9/101\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.1271 - categorical_accuracy: 0.9452 - val_loss: 0.3495 - val_categorical_accuracy: 0.9057 - lr: 0.0010\n",
            "Epoch 10/101\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.1323 - categorical_accuracy: 0.9435 - val_loss: 0.4200 - val_categorical_accuracy: 0.9033 - lr: 0.0010\n",
            "Epoch 11/101\n",
            "114/114 [==============================] - 5s 44ms/step - loss: 0.1329 - categorical_accuracy: 0.9409 - val_loss: 0.3428 - val_categorical_accuracy: 0.9087 - lr: 0.0010\n",
            "Epoch 12/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.1293 - categorical_accuracy: 0.9474 - val_loss: 0.3355 - val_categorical_accuracy: 0.8951 - lr: 0.0010\n",
            "Epoch 13/101\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.1215 - categorical_accuracy: 0.9489 - val_loss: 0.3006 - val_categorical_accuracy: 0.9118 - lr: 0.0010\n",
            "Epoch 14/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.1181 - categorical_accuracy: 0.9479 - val_loss: 0.3434 - val_categorical_accuracy: 0.9111 - lr: 0.0010\n",
            "Epoch 15/101\n",
            "114/114 [==============================] - 5s 44ms/step - loss: 0.1183 - categorical_accuracy: 0.9519 - val_loss: 0.3497 - val_categorical_accuracy: 0.9084 - lr: 0.0010\n",
            "Epoch 16/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1075 - categorical_accuracy: 0.9542 - val_loss: 0.4074 - val_categorical_accuracy: 0.9057 - lr: 0.0010\n",
            "Epoch 17/101\n",
            "114/114 [==============================] - 5s 48ms/step - loss: 0.1158 - categorical_accuracy: 0.9493 - val_loss: 0.3804 - val_categorical_accuracy: 0.9155 - lr: 0.0010\n",
            "Epoch 18/101\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.1202 - categorical_accuracy: 0.9481 - val_loss: 0.4484 - val_categorical_accuracy: 0.8931 - lr: 0.0010\n",
            "Epoch 19/101\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.1072 - categorical_accuracy: 0.9541 - val_loss: 0.7439 - val_categorical_accuracy: 0.8531 - lr: 0.0010\n",
            "Epoch 20/101\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.1131 - categorical_accuracy: 0.9544 - val_loss: 0.4574 - val_categorical_accuracy: 0.8928 - lr: 0.0010\n",
            "Epoch 21/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.1117 - categorical_accuracy: 0.9546 - val_loss: 0.4132 - val_categorical_accuracy: 0.8907 - lr: 0.0010\n",
            "Epoch 22/101\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.1097 - categorical_accuracy: 0.9505 - val_loss: 0.4142 - val_categorical_accuracy: 0.9084 - lr: 0.0010\n",
            "Epoch 23/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.1093 - categorical_accuracy: 0.9535 - val_loss: 0.4378 - val_categorical_accuracy: 0.8992 - lr: 0.0010\n",
            "Epoch 24/101\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.1091 - categorical_accuracy: 0.9526 - val_loss: 0.4166 - val_categorical_accuracy: 0.9121 - lr: 0.0010\n",
            "Epoch 25/101\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.1073 - categorical_accuracy: 0.9538 - val_loss: 0.3105 - val_categorical_accuracy: 0.8989 - lr: 0.0010\n",
            "Epoch 26/101\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.1025 - categorical_accuracy: 0.9513 - val_loss: 0.3280 - val_categorical_accuracy: 0.9145 - lr: 0.0010\n",
            "Epoch 27/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0968 - categorical_accuracy: 0.9582 - val_loss: 0.4561 - val_categorical_accuracy: 0.9036 - lr: 0.0010\n",
            "Epoch 28/101\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.1120 - categorical_accuracy: 0.9538 - val_loss: 0.2569 - val_categorical_accuracy: 0.9196 - lr: 0.0010\n",
            "Epoch 29/101\n",
            "114/114 [==============================] - 5s 44ms/step - loss: 0.1031 - categorical_accuracy: 0.9567 - val_loss: 0.2610 - val_categorical_accuracy: 0.9257 - lr: 0.0010\n",
            "Epoch 30/101\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 0.1107 - categorical_accuracy: 0.9542 - val_loss: 0.4883 - val_categorical_accuracy: 0.8914 - lr: 0.0010\n",
            "Epoch 31/101\n",
            "114/114 [==============================] - 5s 44ms/step - loss: 0.0998 - categorical_accuracy: 0.9534 - val_loss: 0.3087 - val_categorical_accuracy: 0.9301 - lr: 9.9005e-04\n",
            "Epoch 32/101\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.1011 - categorical_accuracy: 0.9552 - val_loss: 0.2534 - val_categorical_accuracy: 0.9311 - lr: 9.8020e-04\n",
            "Epoch 33/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.1004 - categorical_accuracy: 0.9559 - val_loss: 0.3047 - val_categorical_accuracy: 0.9230 - lr: 9.7045e-04\n",
            "Epoch 34/101\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 0.0968 - categorical_accuracy: 0.9589 - val_loss: 0.2928 - val_categorical_accuracy: 0.9233 - lr: 9.6079e-04\n",
            "Epoch 35/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0973 - categorical_accuracy: 0.9598 - val_loss: 0.3156 - val_categorical_accuracy: 0.9165 - lr: 9.5123e-04\n",
            "Epoch 36/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0997 - categorical_accuracy: 0.9612 - val_loss: 0.3890 - val_categorical_accuracy: 0.9192 - lr: 9.4176e-04\n",
            "Epoch 37/101\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0999 - categorical_accuracy: 0.9574 - val_loss: 0.3040 - val_categorical_accuracy: 0.9189 - lr: 9.3239e-04\n",
            "Epoch 38/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0975 - categorical_accuracy: 0.9608 - val_loss: 0.3162 - val_categorical_accuracy: 0.9220 - lr: 9.2312e-04\n",
            "Epoch 39/101\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0983 - categorical_accuracy: 0.9623 - val_loss: 0.2777 - val_categorical_accuracy: 0.9169 - lr: 9.1393e-04\n",
            "Epoch 40/101\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.0980 - categorical_accuracy: 0.9593 - val_loss: 0.3144 - val_categorical_accuracy: 0.9213 - lr: 9.0484e-04\n",
            "Epoch 41/101\n",
            "114/114 [==============================] - 5s 44ms/step - loss: 0.1054 - categorical_accuracy: 0.9581 - val_loss: 0.2789 - val_categorical_accuracy: 0.9355 - lr: 8.9583e-04\n",
            "Epoch 42/101\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0937 - categorical_accuracy: 0.9619 - val_loss: 0.2215 - val_categorical_accuracy: 0.9399 - lr: 8.8692e-04\n",
            "Epoch 43/101\n",
            "114/114 [==============================] - 12s 104ms/step - loss: 0.1005 - categorical_accuracy: 0.9590 - val_loss: 0.2944 - val_categorical_accuracy: 0.9260 - lr: 8.7810e-04\n",
            "Epoch 44/101\n",
            "114/114 [==============================] - 5s 48ms/step - loss: 0.1013 - categorical_accuracy: 0.9608 - val_loss: 0.3359 - val_categorical_accuracy: 0.9284 - lr: 8.6936e-04\n",
            "Epoch 45/101\n",
            "114/114 [==============================] - 5s 44ms/step - loss: 0.0918 - categorical_accuracy: 0.9601 - val_loss: 0.3119 - val_categorical_accuracy: 0.9328 - lr: 8.6071e-04\n",
            "Epoch 46/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0889 - categorical_accuracy: 0.9640 - val_loss: 0.3017 - val_categorical_accuracy: 0.9250 - lr: 8.5214e-04\n",
            "Epoch 47/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0864 - categorical_accuracy: 0.9640 - val_loss: 0.4604 - val_categorical_accuracy: 0.9145 - lr: 8.4366e-04\n",
            "Epoch 48/101\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 0.0943 - categorical_accuracy: 0.9570 - val_loss: 0.4928 - val_categorical_accuracy: 0.9226 - lr: 8.3527e-04\n",
            "Epoch 49/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0878 - categorical_accuracy: 0.9611 - val_loss: 0.2892 - val_categorical_accuracy: 0.9199 - lr: 8.2696e-04\n",
            "Epoch 50/101\n",
            "114/114 [==============================] - 7s 64ms/step - loss: 0.0908 - categorical_accuracy: 0.9615 - val_loss: 0.4157 - val_categorical_accuracy: 0.9287 - lr: 8.1873e-04\n",
            "Epoch 51/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0789 - categorical_accuracy: 0.9682 - val_loss: 0.3735 - val_categorical_accuracy: 0.9277 - lr: 8.1058e-04\n",
            "Epoch 52/101\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0770 - categorical_accuracy: 0.9688 - val_loss: 0.4024 - val_categorical_accuracy: 0.9097 - lr: 8.0252e-04\n",
            "Epoch 53/101\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0826 - categorical_accuracy: 0.9661 - val_loss: 0.4052 - val_categorical_accuracy: 0.9145 - lr: 7.9453e-04\n",
            "Epoch 54/101\n",
            "114/114 [==============================] - 8s 70ms/step - loss: 0.0843 - categorical_accuracy: 0.9635 - val_loss: 0.2780 - val_categorical_accuracy: 0.9308 - lr: 7.8663e-04\n",
            "Epoch 55/101\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.0883 - categorical_accuracy: 0.9644 - val_loss: 0.3377 - val_categorical_accuracy: 0.9308 - lr: 7.7880e-04\n",
            "Epoch 56/101\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0791 - categorical_accuracy: 0.9675 - val_loss: 0.3853 - val_categorical_accuracy: 0.9284 - lr: 7.7105e-04\n",
            "Epoch 57/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0818 - categorical_accuracy: 0.9678 - val_loss: 0.3820 - val_categorical_accuracy: 0.9321 - lr: 7.6338e-04\n",
            "Epoch 58/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0745 - categorical_accuracy: 0.9678 - val_loss: 0.2239 - val_categorical_accuracy: 0.9406 - lr: 7.5578e-04\n",
            "Epoch 59/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0777 - categorical_accuracy: 0.9703 - val_loss: 0.2266 - val_categorical_accuracy: 0.9362 - lr: 7.4826e-04\n",
            "Epoch 60/101\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0789 - categorical_accuracy: 0.9698 - val_loss: 0.2755 - val_categorical_accuracy: 0.9399 - lr: 7.4082e-04\n",
            "Epoch 61/101\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.0829 - categorical_accuracy: 0.9652 - val_loss: 0.2782 - val_categorical_accuracy: 0.9338 - lr: 7.3345e-04\n",
            "Epoch 62/101\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0683 - categorical_accuracy: 0.9712 - val_loss: 0.2609 - val_categorical_accuracy: 0.9467 - lr: 7.2615e-04\n",
            "Epoch 63/101\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.0765 - categorical_accuracy: 0.9692 - val_loss: 0.3176 - val_categorical_accuracy: 0.9304 - lr: 7.1892e-04\n",
            "Epoch 64/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0737 - categorical_accuracy: 0.9707 - val_loss: 0.2478 - val_categorical_accuracy: 0.9437 - lr: 7.1177e-04\n",
            "Epoch 65/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0724 - categorical_accuracy: 0.9705 - val_loss: 0.1860 - val_categorical_accuracy: 0.9481 - lr: 7.0469e-04\n",
            "Epoch 66/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0696 - categorical_accuracy: 0.9698 - val_loss: 0.3195 - val_categorical_accuracy: 0.9365 - lr: 6.9768e-04\n",
            "Epoch 67/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0678 - categorical_accuracy: 0.9726 - val_loss: 0.2594 - val_categorical_accuracy: 0.9427 - lr: 6.9073e-04\n",
            "Epoch 68/101\n",
            "114/114 [==============================] - 6s 48ms/step - loss: 0.0636 - categorical_accuracy: 0.9759 - val_loss: 0.2608 - val_categorical_accuracy: 0.9335 - lr: 6.8386e-04\n",
            "Epoch 69/101\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.0615 - categorical_accuracy: 0.9752 - val_loss: 0.2183 - val_categorical_accuracy: 0.9437 - lr: 6.7706e-04\n",
            "Epoch 70/101\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0687 - categorical_accuracy: 0.9712 - val_loss: 0.3295 - val_categorical_accuracy: 0.9321 - lr: 6.7032e-04\n",
            "Epoch 71/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0624 - categorical_accuracy: 0.9738 - val_loss: 0.2227 - val_categorical_accuracy: 0.9389 - lr: 6.6365e-04\n",
            "Epoch 72/101\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.0679 - categorical_accuracy: 0.9731 - val_loss: 0.2132 - val_categorical_accuracy: 0.9420 - lr: 6.5705e-04\n",
            "Epoch 73/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0629 - categorical_accuracy: 0.9762 - val_loss: 0.1989 - val_categorical_accuracy: 0.9447 - lr: 6.5051e-04\n",
            "Epoch 74/101\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0594 - categorical_accuracy: 0.9770 - val_loss: 0.2119 - val_categorical_accuracy: 0.9508 - lr: 6.4404e-04\n",
            "Epoch 75/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0628 - categorical_accuracy: 0.9755 - val_loss: 0.2417 - val_categorical_accuracy: 0.9389 - lr: 6.3763e-04\n",
            "Epoch 76/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0596 - categorical_accuracy: 0.9760 - val_loss: 0.2395 - val_categorical_accuracy: 0.9474 - lr: 6.3128e-04\n",
            "Epoch 77/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0541 - categorical_accuracy: 0.9788 - val_loss: 0.2499 - val_categorical_accuracy: 0.9444 - lr: 6.2500e-04\n",
            "Epoch 78/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0552 - categorical_accuracy: 0.9790 - val_loss: 0.3150 - val_categorical_accuracy: 0.9369 - lr: 6.1878e-04\n",
            "Epoch 79/101\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0592 - categorical_accuracy: 0.9782 - val_loss: 0.3999 - val_categorical_accuracy: 0.9325 - lr: 6.1263e-04\n",
            "Epoch 80/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0541 - categorical_accuracy: 0.9789 - val_loss: 0.2768 - val_categorical_accuracy: 0.9389 - lr: 6.0653e-04\n",
            "Epoch 81/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0602 - categorical_accuracy: 0.9779 - val_loss: 0.3248 - val_categorical_accuracy: 0.9457 - lr: 6.0050e-04\n",
            "Epoch 82/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0619 - categorical_accuracy: 0.9778 - val_loss: 0.3307 - val_categorical_accuracy: 0.9264 - lr: 5.9452e-04\n",
            "Epoch 83/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0604 - categorical_accuracy: 0.9753 - val_loss: 0.1933 - val_categorical_accuracy: 0.9447 - lr: 5.8861e-04\n",
            "Epoch 84/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0559 - categorical_accuracy: 0.9796 - val_loss: 0.1597 - val_categorical_accuracy: 0.9532 - lr: 5.8275e-04\n",
            "Epoch 85/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0509 - categorical_accuracy: 0.9794 - val_loss: 0.2019 - val_categorical_accuracy: 0.9430 - lr: 5.7695e-04\n",
            "Epoch 86/101\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0548 - categorical_accuracy: 0.9775 - val_loss: 0.2506 - val_categorical_accuracy: 0.9345 - lr: 5.7121e-04\n",
            "Epoch 87/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0544 - categorical_accuracy: 0.9788 - val_loss: 0.2489 - val_categorical_accuracy: 0.9427 - lr: 5.6553e-04\n",
            "Epoch 88/101\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.0512 - categorical_accuracy: 0.9770 - val_loss: 0.2961 - val_categorical_accuracy: 0.9294 - lr: 5.5990e-04\n",
            "Epoch 89/101\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0523 - categorical_accuracy: 0.9803 - val_loss: 0.2530 - val_categorical_accuracy: 0.9416 - lr: 5.5433e-04\n",
            "Epoch 90/101\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0606 - categorical_accuracy: 0.9772 - val_loss: 0.3204 - val_categorical_accuracy: 0.9294 - lr: 5.4881e-04\n",
            "Epoch 91/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0530 - categorical_accuracy: 0.9794 - val_loss: 0.2980 - val_categorical_accuracy: 0.9430 - lr: 5.4335e-04\n",
            "Epoch 92/101\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0577 - categorical_accuracy: 0.9796 - val_loss: 0.2370 - val_categorical_accuracy: 0.9460 - lr: 5.3794e-04\n",
            "Epoch 93/101\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0503 - categorical_accuracy: 0.9801 - val_loss: 0.1712 - val_categorical_accuracy: 0.9522 - lr: 5.3259e-04\n",
            "Epoch 94/101\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0481 - categorical_accuracy: 0.9815 - val_loss: 0.2586 - val_categorical_accuracy: 0.9355 - lr: 5.2729e-04\n",
            "Epoch 95/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0522 - categorical_accuracy: 0.9797 - val_loss: 0.2594 - val_categorical_accuracy: 0.9423 - lr: 5.2205e-04\n",
            "Epoch 96/101\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.0470 - categorical_accuracy: 0.9818 - val_loss: 0.2564 - val_categorical_accuracy: 0.9406 - lr: 5.1685e-04\n",
            "Epoch 97/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0513 - categorical_accuracy: 0.9790 - val_loss: 0.3645 - val_categorical_accuracy: 0.9427 - lr: 5.1171e-04\n",
            "Epoch 98/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0473 - categorical_accuracy: 0.9818 - val_loss: 0.3023 - val_categorical_accuracy: 0.9406 - lr: 5.0662e-04\n",
            "Epoch 99/101\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.0464 - categorical_accuracy: 0.9829 - val_loss: 0.2249 - val_categorical_accuracy: 0.9488 - lr: 5.0158e-04\n",
            "Epoch 100/101\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.0562 - categorical_accuracy: 0.9807 - val_loss: 0.2615 - val_categorical_accuracy: 0.9413 - lr: 4.9659e-04\n",
            "Epoch 101/101\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0452 - categorical_accuracy: 0.9836 - val_loss: 0.2769 - val_categorical_accuracy: 0.9345 - lr: 4.9164e-04\n"
          ]
        }
      ],
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "num_classes = y_train.shape[-1]\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "print(X_train.shape,y_train.shape)\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 30:\n",
        "    return lr\n",
        "  else:\n",
        "     return lr * tf.math.exp(-0.01)\n",
        "\n",
        "dg = DataGenerator(X_train,y_train,batch_size=batch_size,input_shape=X_train.shape[1:])\n",
        "model = buildAE(X_train.shape[1:],y_train.shape[-1],learning_rate)\n",
        "log = MyLogger(n=1, validation_data=(x_test,y_test), AE=model)\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "history = model.fit(dg, epochs=101, verbose=1,callbacks = [lr_scheduler], validation_data=(x_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYkOJBg2e3Xj"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['categorical_accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_categorical_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "pSzRP-r5SyQP",
        "outputId": "9fa2ef37-7d37-4b76-e093-1551c886dd9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "93/93 [==============================] - 3s 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97       496\n",
            "           1       0.97      0.98      0.97       471\n",
            "           2       0.98      1.00      0.99       420\n",
            "           3       0.93      0.87      0.90       491\n",
            "           4       0.89      0.97      0.93       532\n",
            "           5       1.00      1.00      1.00       537\n",
            "\n",
            "    accuracy                           0.96      2947\n",
            "   macro avg       0.96      0.96      0.96      2947\n",
            "weighted avg       0.96      0.96      0.96      2947\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "model = tf.keras.models.load_model('/content/resnet_inception_v2_my_model.h5')\n",
        "outs = model.predict(x_test)\n",
        "\n",
        "class_names= np.unique(np.argmax(y_test,axis=1))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(np.argmax(y_test,axis=1), np.argmax(outs,axis=1))\n",
        "\n",
        "print(classification_report(np.argmax(y_test,axis=1), np.argmax(outs,axis=1)))\n",
        "\n",
        "\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "# set the axis labels and title of the plot\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "plt.savefig('confusion_matrix.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyMq8nVByHGb"
      },
      "source": [
        "## new rcjtecture with 96.76"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dhZ0HwUvqqK",
        "outputId": "412a7e4b-4a1b-4ab0-bae4-712db87f505f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params 186156\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 9)]     0           []                               \n",
            "                                                                                                  \n",
            " zero_padding1d (ZeroPadding1D)  (None, 130, 9)      0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 128, 16)      448         ['zero_padding1d[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 128, 16)     64          ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 128, 16)      0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 63, 16)       0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 63, 16)       784         ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 63, 16)       1296        ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 63, 16)       1808        ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 63, 16)      64          ['conv1d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 63, 16)      64          ['conv1d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 63, 16)      64          ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 63, 16)       0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 63, 16)       0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 63, 16)       0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 63, 48)       0           ['activation_1[0][0]',           \n",
            "                                                                  'activation_2[0][0]',           \n",
            "                                                                  'activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 63, 16)       2320        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 63, 16)      64          ['conv1d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 63, 16)       0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 63, 16)       0           ['activation_4[0][0]',           \n",
            "                                                                  'max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 31, 16)       784         ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 31, 16)      64          ['conv1d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 31, 16)       0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 31, 16)       784         ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 31, 16)      64          ['conv1d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 31, 16)       0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 31, 32)       1568        ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 31, 32)       1568        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 31, 32)       0           ['conv1d_7[0][0]',               \n",
            "                                                                  'conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 31, 32)      128         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 31, 32)       0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 31, 16)       1552        ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 31, 16)      64          ['conv1d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 31, 16)       0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 31, 16)       784         ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 31, 16)      64          ['conv1d_10[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 31, 16)       0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 31, 32)       1568        ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 31, 32)       0           ['conv1d_11[0][0]',              \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 31, 32)       3104        ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 31, 32)       5152        ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 31, 32)       7200        ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 31, 32)      128         ['conv1d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 31, 32)      128         ['conv1d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 31, 32)      128         ['conv1d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 31, 32)       0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 31, 32)       0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 31, 32)       0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 31, 96)       0           ['activation_10[0][0]',          \n",
            "                                                                  'activation_11[0][0]',          \n",
            "                                                                  'activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 31, 32)       9248        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 31, 32)      128         ['conv1d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 31, 32)       0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 31, 32)       0           ['activation_13[0][0]',          \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 15, 32)       3104        ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 15, 32)      128         ['conv1d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 15, 32)       0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 15, 32)       3104        ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 15, 32)      128         ['conv1d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 15, 32)       0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 15, 64)       6208        ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 15, 64)       6208        ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 15, 64)       0           ['conv1d_18[0][0]',              \n",
            "                                                                  'conv1d_19[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 15, 64)      256         ['add_4[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 15, 64)       0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)             (None, 15, 32)       6176        ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 15, 32)      128         ['conv1d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 15, 32)       0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_21 (Conv1D)             (None, 15, 32)       3104        ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 15, 32)      128         ['conv1d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 15, 32)       0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_22 (Conv1D)             (None, 15, 64)       6208        ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 15, 64)       0           ['conv1d_22[0][0]',              \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 15, 64)      256         ['add_5[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 15, 64)       0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_23 (Conv1D)             (None, 15, 32)       6176        ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 15, 32)      128         ['conv1d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 15, 32)       0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_24 (Conv1D)             (None, 15, 32)       3104        ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 15, 32)      128         ['conv1d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 15, 32)       0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_25 (Conv1D)             (None, 15, 64)       6208        ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 15, 64)       0           ['conv1d_25[0][0]',              \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 15, 64)      256         ['add_6[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 15, 64)       0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 15, 64)       0           ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 15, 32)       10368       ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 15, 32)      128         ['bidirectional[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 15, 32)       0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 64)          16640       ['dropout_1[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 64)          256         ['bidirectional_1[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 960)          0           ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           4160        ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 64)           61504       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 6)            390         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 6)            390         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 6)            0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 6)            0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 186,156\n",
            "Trainable params: 184,588\n",
            "Non-trainable params: 1,568\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def res_identity(x, filters): \n",
        "  #renet block where dimension doesnot change.\n",
        "  #The skip connection is just simple identity conncection\n",
        "  #we will have 3 blocks and then input will be added\n",
        "\n",
        "  x_skip = x # this will be used for addition with the residual block \n",
        "  f1, f2 = filters\n",
        "\n",
        "  #first block \n",
        "  #kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  \n",
        "\n",
        "  #second block # bottleneck (but size kept same with padding)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same', )(x)\n",
        "  \n",
        "\n",
        "  # third block activation used after adding the input\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.Conv1D(f2, kernel_size=3, strides=1, padding='same')(x)\n",
        "\n",
        "  # add the input \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def conv_skip(x, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=2, padding='valid')(x)\n",
        "  # when s = 2 then it is like downsizing the feature map\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  # x = tf.keras.layers.ZeroPadding1D(padding=(1,1))(x)\n",
        "  \n",
        "\n",
        "  # second block\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  #third block\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "  x = tf.keras.layers.Conv1D(f2, kernel_size=3, strides=1, padding='same')(x)\n",
        "  # x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "\n",
        "  # shortcut \n",
        "  x_skip = tf.keras.layers.Conv1D(f2, kernel_size=3, strides=2, padding='valid')(x_skip)\n",
        "  # x_skip = tf.keras.layers.MaxPool1D(pool_size=3,strides=2)(x_skip)\n",
        "  # x_skip = tf.keras.layers.BatchNormalization()(x_skip)\n",
        "\n",
        "  # add \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def multi_fiter_conv(x, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x1 = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "  x1 = tf.keras.layers.Activation(tf.keras.activations.relu)(x1)\n",
        "  \n",
        "\n",
        "  # second block\n",
        "  x2 = tf.keras.layers.Conv1D(f1, kernel_size=5, strides=1, padding='same')(x)\n",
        "  x2 = tf.keras.layers.BatchNormalization()(x2)\n",
        "  x2 = tf.keras.layers.Activation(tf.keras.activations.relu)(x2)\n",
        "\n",
        "  #third block\n",
        "  x3 = tf.keras.layers.Conv1D(f1, kernel_size=7, strides=1, padding='same')(x)\n",
        "  x3 = tf.keras.layers.BatchNormalization()(x3)\n",
        "  x3 = tf.keras.layers.Activation(tf.keras.activations.relu)(x3)\n",
        "\n",
        "\n",
        "  # # forth block\n",
        "  # x4 = tf.keras.layers.Conv1D(f1, kernel_size=9, strides=1, padding='same')(x)\n",
        "  # x4 = tf.keras.layers.BatchNormalization()(x4)\n",
        "  # x4 = tf.keras.layers.Activation(tf.keras.activations.relu)(x4)\n",
        "\n",
        "  # concatenate \n",
        "  x = tf.keras.layers.Concatenate()([x1,x2,x3])\n",
        "  x = tf.keras.layers.Conv1D(f2,kernel_size=3,strides=1,padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  #add\n",
        "  x = tf.keras.layers.Add()([x,x_skip])\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def buildAE(input_shape,classes,learning_rate):\n",
        "\n",
        "    input = tf.keras.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.ZeroPadding1D(padding=(1,1))(input)\n",
        "    x = tf.keras.layers.Conv1D(16, kernel_size=3, strides=1,padding='valid',)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)(x)\n",
        "\n",
        "    x = multi_fiter_conv(x,filters=(16,16))\n",
        "\n",
        "    x = conv_skip(x ,filters=(16,32))\n",
        "    # print(x.shape)\n",
        "    x = res_identity(x,filters=(16,32))\n",
        "    x = multi_fiter_conv(x,filters=(32,32))\n",
        "    x = conv_skip(x ,filters=(32,64))\n",
        "    x = res_identity(x,filters=(32,64))\n",
        "    x = res_identity(x,filters=(32,64))\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "    # x = maxpool_skip(x,filters=(16,16))\n",
        "\n",
        "    output_opt = tf.keras.layers.Flatten()(x)\n",
        "    output_opt = tf.keras.layers.Dense(units=64)(output_opt)\n",
        "    output_opt = tf.keras.layers.Dense(units=classes)(output_opt)\n",
        "    output_opt = tf.keras.layers.Activation('softmax')(output_opt)\n",
        "\n",
        "\n",
        "\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=16,activation='tanh',return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=32,activation='tanh'))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(units=64)(x)\n",
        "    x = tf.keras.layers.Dense(units=classes)(x)\n",
        "    output = tf.keras.layers.Activation('softmax')(x)\n",
        "\n",
        "\n",
        "    # model = tf.keras.Model(input,output)\n",
        "\n",
        "    pred_model = tf.keras.Model(input,output)\n",
        "    model = tf.keras.Model(input,[output,output_opt])\n",
        "\n",
        "\n",
        "    \n",
        "    print('Params', model.count_params())\n",
        "    model.compile(loss = [tf.keras.losses.CategoricalCrossentropy(),\n",
        "                          tf.keras.losses.CategoricalCrossentropy()],\n",
        "                  metrics=[tf.keras.metrics.CategoricalAccuracy(),],\n",
        "                   optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
        "\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    return model, pred_model\n",
        "model = buildAE((128,9),6,0.0005)\n",
        "# tf.keras.utils.plot_model(model,show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6iBK_dMzeBL",
        "outputId": "7a0187f9-28fa-4f09-8175-41da4cb3627b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7352, 128, 9) (7352, 6)\n",
            "Params 186156\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 128, 9)]     0           []                               \n",
            "                                                                                                  \n",
            " zero_padding1d_1 (ZeroPadding1  (None, 130, 9)      0           ['input_2[0][0]']                \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv1d_26 (Conv1D)             (None, 128, 16)      448         ['zero_padding1d_1[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 128, 16)     64          ['conv1d_26[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 128, 16)      0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 63, 16)      0           ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_27 (Conv1D)             (None, 63, 16)       784         ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_28 (Conv1D)             (None, 63, 16)       1296        ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_29 (Conv1D)             (None, 63, 16)       1808        ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 63, 16)      64          ['conv1d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 63, 16)      64          ['conv1d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 63, 16)      64          ['conv1d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 63, 16)       0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 63, 16)       0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 63, 16)       0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 63, 48)       0           ['activation_26[0][0]',          \n",
            "                                                                  'activation_27[0][0]',          \n",
            "                                                                  'activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_30 (Conv1D)             (None, 63, 16)       2320        ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 63, 16)      64          ['conv1d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 63, 16)       0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 63, 16)       0           ['activation_29[0][0]',          \n",
            "                                                                  'max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_31 (Conv1D)             (None, 31, 16)       784         ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 31, 16)      64          ['conv1d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 31, 16)       0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_32 (Conv1D)             (None, 31, 16)       784         ['activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 31, 16)      64          ['conv1d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 31, 16)       0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_33 (Conv1D)             (None, 31, 32)       1568        ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_34 (Conv1D)             (None, 31, 32)       1568        ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 31, 32)       0           ['conv1d_33[0][0]',              \n",
            "                                                                  'conv1d_34[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 31, 32)      128         ['add_8[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 31, 32)       0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_35 (Conv1D)             (None, 31, 16)       1552        ['activation_32[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 31, 16)      64          ['conv1d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 31, 16)       0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_36 (Conv1D)             (None, 31, 16)       784         ['activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 31, 16)      64          ['conv1d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 31, 16)       0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_37 (Conv1D)             (None, 31, 32)       1568        ['activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 31, 32)       0           ['conv1d_37[0][0]',              \n",
            "                                                                  'add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_38 (Conv1D)             (None, 31, 32)       3104        ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_39 (Conv1D)             (None, 31, 32)       5152        ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_40 (Conv1D)             (None, 31, 32)       7200        ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 31, 32)      128         ['conv1d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 31, 32)      128         ['conv1d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 31, 32)      128         ['conv1d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 31, 32)       0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 31, 32)       0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 31, 32)       0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 31, 96)       0           ['activation_35[0][0]',          \n",
            "                                                                  'activation_36[0][0]',          \n",
            "                                                                  'activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_41 (Conv1D)             (None, 31, 32)       9248        ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 31, 32)      128         ['conv1d_41[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 31, 32)       0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 31, 32)       0           ['activation_38[0][0]',          \n",
            "                                                                  'add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_42 (Conv1D)             (None, 15, 32)       3104        ['add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 15, 32)      128         ['conv1d_42[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 15, 32)       0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_43 (Conv1D)             (None, 15, 32)       3104        ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 15, 32)      128         ['conv1d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 15, 32)       0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_44 (Conv1D)             (None, 15, 64)       6208        ['activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_45 (Conv1D)             (None, 15, 64)       6208        ['add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 15, 64)       0           ['conv1d_44[0][0]',              \n",
            "                                                                  'conv1d_45[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 15, 64)      256         ['add_11[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 15, 64)       0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_46 (Conv1D)             (None, 15, 32)       6176        ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 15, 32)      128         ['conv1d_46[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 15, 32)       0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_47 (Conv1D)             (None, 15, 32)       3104        ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 15, 32)      128         ['conv1d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 15, 32)       0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_48 (Conv1D)             (None, 15, 64)       6208        ['activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 15, 64)       0           ['conv1d_48[0][0]',              \n",
            "                                                                  'add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 15, 64)      256         ['add_12[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 15, 64)       0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_49 (Conv1D)             (None, 15, 32)       6176        ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 15, 32)      128         ['conv1d_49[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 15, 32)       0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_50 (Conv1D)             (None, 15, 32)       3104        ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 15, 32)      128         ['conv1d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 15, 32)       0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_51 (Conv1D)             (None, 15, 64)       6208        ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 15, 64)       0           ['conv1d_51[0][0]',              \n",
            "                                                                  'add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 15, 64)      256         ['add_13[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 15, 64)       0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 15, 64)       0           ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirectional  (None, 15, 32)      10368       ['dropout_2[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 15, 32)      128         ['bidirectional_2[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 15, 32)       0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " bidirectional_3 (Bidirectional  (None, 64)          16640       ['dropout_3[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 64)          256         ['bidirectional_3[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 960)          0           ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 64)           4160        ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 64)           61504       ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 6)            390         ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 6)            390         ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 6)            0           ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 6)            0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 186,156\n",
            "Trainable params: 184,588\n",
            "Non-trainable params: 1,568\n",
            "__________________________________________________________________________________________________\n",
            "Params 186156\n",
            "Epoch 1/251\n",
            "114/114 [==============================] - 65s 246ms/step - loss: 2.1988 - activation_49_loss: 1.1573 - activation_48_loss: 1.0416 - activation_49_categorical_accuracy: 0.5783 - activation_48_categorical_accuracy: 0.6732 - val_loss: 5.4122 - val_activation_49_loss: 1.6885 - val_activation_48_loss: 3.7237 - val_activation_49_categorical_accuracy: 0.2324 - val_activation_48_categorical_accuracy: 0.1778 - lr: 0.0010\n",
            "Epoch 2/251\n",
            "114/114 [==============================] - 28s 247ms/step - loss: 0.4566 - activation_49_loss: 0.2531 - activation_48_loss: 0.2035 - activation_49_categorical_accuracy: 0.9075 - activation_48_categorical_accuracy: 0.9224 - val_loss: 0.9519 - val_activation_49_loss: 0.4305 - val_activation_48_loss: 0.5214 - val_activation_49_categorical_accuracy: 0.8649 - val_activation_48_categorical_accuracy: 0.7794 - lr: 0.0010\n",
            "Epoch 3/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 0.2619 - activation_49_loss: 0.1418 - activation_48_loss: 0.1201 - activation_49_categorical_accuracy: 0.9433 - activation_48_categorical_accuracy: 0.9513 - val_loss: 0.4326 - val_activation_49_loss: 0.2106 - val_activation_48_loss: 0.2220 - val_activation_49_categorical_accuracy: 0.9328 - val_activation_48_categorical_accuracy: 0.9277 - lr: 0.0010\n",
            "Epoch 4/251\n",
            "114/114 [==============================] - 20s 177ms/step - loss: 0.2452 - activation_49_loss: 0.1283 - activation_48_loss: 0.1169 - activation_49_categorical_accuracy: 0.9478 - activation_48_categorical_accuracy: 0.9511 - val_loss: 0.4548 - val_activation_49_loss: 0.2508 - val_activation_48_loss: 0.2040 - val_activation_49_categorical_accuracy: 0.9128 - val_activation_48_categorical_accuracy: 0.9352 - lr: 0.0010\n",
            "Epoch 5/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.2274 - activation_49_loss: 0.1194 - activation_48_loss: 0.1080 - activation_49_categorical_accuracy: 0.9522 - activation_48_categorical_accuracy: 0.9533 - val_loss: 0.4970 - val_activation_49_loss: 0.2690 - val_activation_48_loss: 0.2280 - val_activation_49_categorical_accuracy: 0.9284 - val_activation_48_categorical_accuracy: 0.9206 - lr: 0.0010\n",
            "Epoch 6/251\n",
            "114/114 [==============================] - 18s 161ms/step - loss: 0.2356 - activation_49_loss: 0.1217 - activation_48_loss: 0.1139 - activation_49_categorical_accuracy: 0.9456 - activation_48_categorical_accuracy: 0.9482 - val_loss: 0.4754 - val_activation_49_loss: 0.2485 - val_activation_48_loss: 0.2269 - val_activation_49_categorical_accuracy: 0.9264 - val_activation_48_categorical_accuracy: 0.9243 - lr: 0.0010\n",
            "Epoch 7/251\n",
            "114/114 [==============================] - 19s 168ms/step - loss: 0.2496 - activation_49_loss: 0.1317 - activation_48_loss: 0.1179 - activation_49_categorical_accuracy: 0.9471 - activation_48_categorical_accuracy: 0.9502 - val_loss: 0.4278 - val_activation_49_loss: 0.2261 - val_activation_48_loss: 0.2018 - val_activation_49_categorical_accuracy: 0.9145 - val_activation_48_categorical_accuracy: 0.9186 - lr: 0.0010\n",
            "Epoch 8/251\n",
            "114/114 [==============================] - 20s 177ms/step - loss: 0.2351 - activation_49_loss: 0.1219 - activation_48_loss: 0.1132 - activation_49_categorical_accuracy: 0.9442 - activation_48_categorical_accuracy: 0.9479 - val_loss: 0.5880 - val_activation_49_loss: 0.3038 - val_activation_48_loss: 0.2842 - val_activation_49_categorical_accuracy: 0.9141 - val_activation_48_categorical_accuracy: 0.9213 - lr: 0.0010\n",
            "Epoch 9/251\n",
            "114/114 [==============================] - 20s 177ms/step - loss: 0.2159 - activation_49_loss: 0.1090 - activation_48_loss: 0.1068 - activation_49_categorical_accuracy: 0.9537 - activation_48_categorical_accuracy: 0.9544 - val_loss: 0.4615 - val_activation_49_loss: 0.2253 - val_activation_48_loss: 0.2361 - val_activation_49_categorical_accuracy: 0.9158 - val_activation_48_categorical_accuracy: 0.9189 - lr: 0.0010\n",
            "Epoch 10/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 0.2117 - activation_49_loss: 0.1095 - activation_48_loss: 0.1022 - activation_49_categorical_accuracy: 0.9501 - activation_48_categorical_accuracy: 0.9550 - val_loss: 0.7312 - val_activation_49_loss: 0.3837 - val_activation_48_loss: 0.3475 - val_activation_49_categorical_accuracy: 0.9060 - val_activation_48_categorical_accuracy: 0.9094 - lr: 0.0010\n",
            "Epoch 11/251\n",
            "114/114 [==============================] - 19s 163ms/step - loss: 0.2306 - activation_49_loss: 0.1206 - activation_48_loss: 0.1100 - activation_49_categorical_accuracy: 0.9493 - activation_48_categorical_accuracy: 0.9515 - val_loss: 0.4685 - val_activation_49_loss: 0.2361 - val_activation_48_loss: 0.2324 - val_activation_49_categorical_accuracy: 0.9321 - val_activation_48_categorical_accuracy: 0.9311 - lr: 0.0010\n",
            "Epoch 12/251\n",
            "114/114 [==============================] - 21s 188ms/step - loss: 0.2105 - activation_49_loss: 0.1106 - activation_48_loss: 0.0999 - activation_49_categorical_accuracy: 0.9502 - activation_48_categorical_accuracy: 0.9538 - val_loss: 0.5600 - val_activation_49_loss: 0.2980 - val_activation_48_loss: 0.2620 - val_activation_49_categorical_accuracy: 0.9192 - val_activation_48_categorical_accuracy: 0.9196 - lr: 0.0010\n",
            "Epoch 13/251\n",
            "114/114 [==============================] - 21s 187ms/step - loss: 0.2134 - activation_49_loss: 0.1111 - activation_48_loss: 0.1022 - activation_49_categorical_accuracy: 0.9498 - activation_48_categorical_accuracy: 0.9522 - val_loss: 0.5545 - val_activation_49_loss: 0.2757 - val_activation_48_loss: 0.2788 - val_activation_49_categorical_accuracy: 0.9284 - val_activation_48_categorical_accuracy: 0.9220 - lr: 0.0010\n",
            "Epoch 14/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 0.2168 - activation_49_loss: 0.1152 - activation_48_loss: 0.1017 - activation_49_categorical_accuracy: 0.9474 - activation_48_categorical_accuracy: 0.9535 - val_loss: 0.5100 - val_activation_49_loss: 0.2613 - val_activation_48_loss: 0.2487 - val_activation_49_categorical_accuracy: 0.9253 - val_activation_48_categorical_accuracy: 0.9230 - lr: 0.0010\n",
            "Epoch 15/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 0.2073 - activation_49_loss: 0.1085 - activation_48_loss: 0.0988 - activation_49_categorical_accuracy: 0.9504 - activation_48_categorical_accuracy: 0.9542 - val_loss: 0.9198 - val_activation_49_loss: 0.3970 - val_activation_48_loss: 0.5228 - val_activation_49_categorical_accuracy: 0.9118 - val_activation_48_categorical_accuracy: 0.9040 - lr: 0.0010\n",
            "Epoch 16/251\n",
            "114/114 [==============================] - 20s 174ms/step - loss: 0.2295 - activation_49_loss: 0.1155 - activation_48_loss: 0.1140 - activation_49_categorical_accuracy: 0.9494 - activation_48_categorical_accuracy: 0.9541 - val_loss: 0.4766 - val_activation_49_loss: 0.2408 - val_activation_48_loss: 0.2358 - val_activation_49_categorical_accuracy: 0.9270 - val_activation_48_categorical_accuracy: 0.9372 - lr: 0.0010\n",
            "Epoch 17/251\n",
            "114/114 [==============================] - 18s 160ms/step - loss: 0.1932 - activation_49_loss: 0.1002 - activation_48_loss: 0.0930 - activation_49_categorical_accuracy: 0.9553 - activation_48_categorical_accuracy: 0.9585 - val_loss: 0.4422 - val_activation_49_loss: 0.2468 - val_activation_48_loss: 0.1954 - val_activation_49_categorical_accuracy: 0.9257 - val_activation_48_categorical_accuracy: 0.9274 - lr: 0.0010\n",
            "Epoch 18/251\n",
            "114/114 [==============================] - 20s 177ms/step - loss: 0.2052 - activation_49_loss: 0.1076 - activation_48_loss: 0.0976 - activation_49_categorical_accuracy: 0.9537 - activation_48_categorical_accuracy: 0.9568 - val_loss: 0.4653 - val_activation_49_loss: 0.2379 - val_activation_48_loss: 0.2273 - val_activation_49_categorical_accuracy: 0.9260 - val_activation_48_categorical_accuracy: 0.9206 - lr: 0.0010\n",
            "Epoch 19/251\n",
            "114/114 [==============================] - 22s 192ms/step - loss: 0.1797 - activation_49_loss: 0.0935 - activation_48_loss: 0.0861 - activation_49_categorical_accuracy: 0.9564 - activation_48_categorical_accuracy: 0.9618 - val_loss: 0.5954 - val_activation_49_loss: 0.3160 - val_activation_48_loss: 0.2794 - val_activation_49_categorical_accuracy: 0.9237 - val_activation_48_categorical_accuracy: 0.9199 - lr: 0.0010\n",
            "Epoch 20/251\n",
            "114/114 [==============================] - 19s 165ms/step - loss: 0.1983 - activation_49_loss: 0.1028 - activation_48_loss: 0.0955 - activation_49_categorical_accuracy: 0.9541 - activation_48_categorical_accuracy: 0.9608 - val_loss: 0.5189 - val_activation_49_loss: 0.2547 - val_activation_48_loss: 0.2643 - val_activation_49_categorical_accuracy: 0.9196 - val_activation_48_categorical_accuracy: 0.9206 - lr: 0.0010\n",
            "Epoch 21/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.1910 - activation_49_loss: 0.0997 - activation_48_loss: 0.0913 - activation_49_categorical_accuracy: 0.9586 - activation_48_categorical_accuracy: 0.9596 - val_loss: 0.5070 - val_activation_49_loss: 0.2577 - val_activation_48_loss: 0.2493 - val_activation_49_categorical_accuracy: 0.9369 - val_activation_48_categorical_accuracy: 0.9338 - lr: 0.0010\n",
            "Epoch 22/251\n",
            "114/114 [==============================] - 19s 163ms/step - loss: 0.1870 - activation_49_loss: 0.0979 - activation_48_loss: 0.0891 - activation_49_categorical_accuracy: 0.9581 - activation_48_categorical_accuracy: 0.9638 - val_loss: 0.5385 - val_activation_49_loss: 0.2859 - val_activation_48_loss: 0.2526 - val_activation_49_categorical_accuracy: 0.9301 - val_activation_48_categorical_accuracy: 0.9287 - lr: 0.0010\n",
            "Epoch 23/251\n",
            "114/114 [==============================] - 20s 173ms/step - loss: 0.1863 - activation_49_loss: 0.0970 - activation_48_loss: 0.0893 - activation_49_categorical_accuracy: 0.9582 - activation_48_categorical_accuracy: 0.9638 - val_loss: 0.4927 - val_activation_49_loss: 0.2504 - val_activation_48_loss: 0.2422 - val_activation_49_categorical_accuracy: 0.9325 - val_activation_48_categorical_accuracy: 0.9257 - lr: 0.0010\n",
            "Epoch 24/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 0.1999 - activation_49_loss: 0.1037 - activation_48_loss: 0.0962 - activation_49_categorical_accuracy: 0.9579 - activation_48_categorical_accuracy: 0.9586 - val_loss: 0.6140 - val_activation_49_loss: 0.2926 - val_activation_48_loss: 0.3215 - val_activation_49_categorical_accuracy: 0.9240 - val_activation_48_categorical_accuracy: 0.9148 - lr: 0.0010\n",
            "Epoch 25/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 0.1918 - activation_49_loss: 0.1033 - activation_48_loss: 0.0884 - activation_49_categorical_accuracy: 0.9523 - activation_48_categorical_accuracy: 0.9604 - val_loss: 0.7044 - val_activation_49_loss: 0.3526 - val_activation_48_loss: 0.3518 - val_activation_49_categorical_accuracy: 0.9023 - val_activation_48_categorical_accuracy: 0.9237 - lr: 0.0010\n",
            "Epoch 26/251\n",
            "114/114 [==============================] - 20s 179ms/step - loss: 0.1933 - activation_49_loss: 0.1028 - activation_48_loss: 0.0905 - activation_49_categorical_accuracy: 0.9564 - activation_48_categorical_accuracy: 0.9613 - val_loss: 0.5243 - val_activation_49_loss: 0.2573 - val_activation_48_loss: 0.2670 - val_activation_49_categorical_accuracy: 0.9199 - val_activation_48_categorical_accuracy: 0.9179 - lr: 0.0010\n",
            "Epoch 27/251\n",
            "114/114 [==============================] - 18s 161ms/step - loss: 0.1830 - activation_49_loss: 0.0958 - activation_48_loss: 0.0872 - activation_49_categorical_accuracy: 0.9608 - activation_48_categorical_accuracy: 0.9641 - val_loss: 0.5681 - val_activation_49_loss: 0.2968 - val_activation_48_loss: 0.2713 - val_activation_49_categorical_accuracy: 0.9284 - val_activation_48_categorical_accuracy: 0.9270 - lr: 0.0010\n",
            "Epoch 28/251\n",
            "114/114 [==============================] - 19s 163ms/step - loss: 0.1717 - activation_49_loss: 0.0894 - activation_48_loss: 0.0822 - activation_49_categorical_accuracy: 0.9590 - activation_48_categorical_accuracy: 0.9622 - val_loss: 0.5865 - val_activation_49_loss: 0.3035 - val_activation_48_loss: 0.2829 - val_activation_49_categorical_accuracy: 0.9250 - val_activation_48_categorical_accuracy: 0.9264 - lr: 0.0010\n",
            "Epoch 29/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 0.1889 - activation_49_loss: 0.0966 - activation_48_loss: 0.0924 - activation_49_categorical_accuracy: 0.9593 - activation_48_categorical_accuracy: 0.9620 - val_loss: 0.5494 - val_activation_49_loss: 0.2763 - val_activation_48_loss: 0.2731 - val_activation_49_categorical_accuracy: 0.9172 - val_activation_48_categorical_accuracy: 0.9192 - lr: 0.0010\n",
            "Epoch 30/251\n",
            "114/114 [==============================] - 18s 157ms/step - loss: 0.1858 - activation_49_loss: 0.0974 - activation_48_loss: 0.0884 - activation_49_categorical_accuracy: 0.9574 - activation_48_categorical_accuracy: 0.9613 - val_loss: 0.5482 - val_activation_49_loss: 0.2721 - val_activation_48_loss: 0.2760 - val_activation_49_categorical_accuracy: 0.9335 - val_activation_48_categorical_accuracy: 0.9284 - lr: 0.0010\n",
            "Epoch 31/251\n",
            "114/114 [==============================] - 18s 161ms/step - loss: 0.1704 - activation_49_loss: 0.0890 - activation_48_loss: 0.0813 - activation_49_categorical_accuracy: 0.9597 - activation_48_categorical_accuracy: 0.9635 - val_loss: 0.6854 - val_activation_49_loss: 0.3532 - val_activation_48_loss: 0.3323 - val_activation_49_categorical_accuracy: 0.9186 - val_activation_48_categorical_accuracy: 0.9152 - lr: 9.9005e-04\n",
            "Epoch 32/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.1728 - activation_49_loss: 0.0912 - activation_48_loss: 0.0815 - activation_49_categorical_accuracy: 0.9612 - activation_48_categorical_accuracy: 0.9637 - val_loss: 0.5908 - val_activation_49_loss: 0.3064 - val_activation_48_loss: 0.2844 - val_activation_49_categorical_accuracy: 0.9315 - val_activation_48_categorical_accuracy: 0.9308 - lr: 9.8020e-04\n",
            "Epoch 33/251\n",
            "114/114 [==============================] - 21s 182ms/step - loss: 0.1730 - activation_49_loss: 0.0920 - activation_48_loss: 0.0810 - activation_49_categorical_accuracy: 0.9587 - activation_48_categorical_accuracy: 0.9656 - val_loss: 0.6309 - val_activation_49_loss: 0.3203 - val_activation_48_loss: 0.3106 - val_activation_49_categorical_accuracy: 0.9233 - val_activation_48_categorical_accuracy: 0.9264 - lr: 9.7045e-04\n",
            "Epoch 34/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.1593 - activation_49_loss: 0.0842 - activation_48_loss: 0.0751 - activation_49_categorical_accuracy: 0.9616 - activation_48_categorical_accuracy: 0.9668 - val_loss: 0.7038 - val_activation_49_loss: 0.3410 - val_activation_48_loss: 0.3628 - val_activation_49_categorical_accuracy: 0.9315 - val_activation_48_categorical_accuracy: 0.9199 - lr: 9.6079e-04\n",
            "Epoch 35/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.1658 - activation_49_loss: 0.0870 - activation_48_loss: 0.0788 - activation_49_categorical_accuracy: 0.9604 - activation_48_categorical_accuracy: 0.9641 - val_loss: 0.6192 - val_activation_49_loss: 0.3250 - val_activation_48_loss: 0.2942 - val_activation_49_categorical_accuracy: 0.9257 - val_activation_48_categorical_accuracy: 0.9220 - lr: 9.5123e-04\n",
            "Epoch 36/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.1677 - activation_49_loss: 0.0882 - activation_48_loss: 0.0795 - activation_49_categorical_accuracy: 0.9645 - activation_48_categorical_accuracy: 0.9682 - val_loss: 0.6542 - val_activation_49_loss: 0.3223 - val_activation_48_loss: 0.3319 - val_activation_49_categorical_accuracy: 0.9152 - val_activation_48_categorical_accuracy: 0.9121 - lr: 9.4176e-04\n",
            "Epoch 37/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.1780 - activation_49_loss: 0.0949 - activation_48_loss: 0.0831 - activation_49_categorical_accuracy: 0.9616 - activation_48_categorical_accuracy: 0.9670 - val_loss: 0.5926 - val_activation_49_loss: 0.3102 - val_activation_48_loss: 0.2825 - val_activation_49_categorical_accuracy: 0.9192 - val_activation_48_categorical_accuracy: 0.9226 - lr: 9.3239e-04\n",
            "Epoch 38/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.1544 - activation_49_loss: 0.0813 - activation_48_loss: 0.0731 - activation_49_categorical_accuracy: 0.9667 - activation_48_categorical_accuracy: 0.9686 - val_loss: 0.7749 - val_activation_49_loss: 0.4015 - val_activation_48_loss: 0.3734 - val_activation_49_categorical_accuracy: 0.9186 - val_activation_48_categorical_accuracy: 0.9196 - lr: 9.2312e-04\n",
            "Epoch 39/251\n",
            "114/114 [==============================] - 19s 164ms/step - loss: 0.1466 - activation_49_loss: 0.0780 - activation_48_loss: 0.0686 - activation_49_categorical_accuracy: 0.9686 - activation_48_categorical_accuracy: 0.9707 - val_loss: 0.6141 - val_activation_49_loss: 0.2737 - val_activation_48_loss: 0.3404 - val_activation_49_categorical_accuracy: 0.9345 - val_activation_48_categorical_accuracy: 0.9328 - lr: 9.1393e-04\n",
            "Epoch 40/251\n",
            "114/114 [==============================] - 19s 170ms/step - loss: 0.1664 - activation_49_loss: 0.0903 - activation_48_loss: 0.0761 - activation_49_categorical_accuracy: 0.9618 - activation_48_categorical_accuracy: 0.9661 - val_loss: 0.8224 - val_activation_49_loss: 0.3223 - val_activation_48_loss: 0.5001 - val_activation_49_categorical_accuracy: 0.9192 - val_activation_48_categorical_accuracy: 0.9135 - lr: 9.0484e-04\n",
            "Epoch 41/251\n",
            "114/114 [==============================] - 23s 206ms/step - loss: 0.1368 - activation_49_loss: 0.0725 - activation_48_loss: 0.0643 - activation_49_categorical_accuracy: 0.9685 - activation_48_categorical_accuracy: 0.9714 - val_loss: 0.6602 - val_activation_49_loss: 0.3151 - val_activation_48_loss: 0.3451 - val_activation_49_categorical_accuracy: 0.9345 - val_activation_48_categorical_accuracy: 0.9287 - lr: 8.9583e-04\n",
            "Epoch 42/251\n",
            "114/114 [==============================] - 22s 190ms/step - loss: 0.1374 - activation_49_loss: 0.0723 - activation_48_loss: 0.0651 - activation_49_categorical_accuracy: 0.9704 - activation_48_categorical_accuracy: 0.9725 - val_loss: 0.6461 - val_activation_49_loss: 0.3056 - val_activation_48_loss: 0.3405 - val_activation_49_categorical_accuracy: 0.9372 - val_activation_48_categorical_accuracy: 0.9355 - lr: 8.8692e-04\n",
            "Epoch 43/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 0.1448 - activation_49_loss: 0.0764 - activation_48_loss: 0.0684 - activation_49_categorical_accuracy: 0.9674 - activation_48_categorical_accuracy: 0.9708 - val_loss: 0.5606 - val_activation_49_loss: 0.2664 - val_activation_48_loss: 0.2941 - val_activation_49_categorical_accuracy: 0.9454 - val_activation_48_categorical_accuracy: 0.9345 - lr: 8.7810e-04\n",
            "Epoch 44/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 0.1507 - activation_49_loss: 0.0803 - activation_48_loss: 0.0704 - activation_49_categorical_accuracy: 0.9657 - activation_48_categorical_accuracy: 0.9692 - val_loss: 0.5946 - val_activation_49_loss: 0.2767 - val_activation_48_loss: 0.3179 - val_activation_49_categorical_accuracy: 0.9389 - val_activation_48_categorical_accuracy: 0.9203 - lr: 8.6936e-04\n",
            "Epoch 45/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 0.1391 - activation_49_loss: 0.0736 - activation_48_loss: 0.0655 - activation_49_categorical_accuracy: 0.9698 - activation_48_categorical_accuracy: 0.9723 - val_loss: 0.7021 - val_activation_49_loss: 0.3247 - val_activation_48_loss: 0.3774 - val_activation_49_categorical_accuracy: 0.9328 - val_activation_48_categorical_accuracy: 0.9304 - lr: 8.6071e-04\n",
            "Epoch 46/251\n",
            "114/114 [==============================] - 19s 168ms/step - loss: 0.1303 - activation_49_loss: 0.0718 - activation_48_loss: 0.0585 - activation_49_categorical_accuracy: 0.9692 - activation_48_categorical_accuracy: 0.9745 - val_loss: 0.8961 - val_activation_49_loss: 0.3878 - val_activation_48_loss: 0.5083 - val_activation_49_categorical_accuracy: 0.9382 - val_activation_48_categorical_accuracy: 0.9355 - lr: 8.5214e-04\n",
            "Epoch 47/251\n",
            "114/114 [==============================] - 25s 216ms/step - loss: 0.1245 - activation_49_loss: 0.0649 - activation_48_loss: 0.0596 - activation_49_categorical_accuracy: 0.9723 - activation_48_categorical_accuracy: 0.9745 - val_loss: 0.5842 - val_activation_49_loss: 0.2940 - val_activation_48_loss: 0.2901 - val_activation_49_categorical_accuracy: 0.9325 - val_activation_48_categorical_accuracy: 0.9277 - lr: 8.4366e-04\n",
            "Epoch 48/251\n",
            "114/114 [==============================] - 18s 162ms/step - loss: 0.1395 - activation_49_loss: 0.0759 - activation_48_loss: 0.0636 - activation_49_categorical_accuracy: 0.9698 - activation_48_categorical_accuracy: 0.9727 - val_loss: 0.8163 - val_activation_49_loss: 0.3820 - val_activation_48_loss: 0.4343 - val_activation_49_categorical_accuracy: 0.9257 - val_activation_48_categorical_accuracy: 0.9270 - lr: 8.3527e-04\n",
            "Epoch 49/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.1467 - activation_49_loss: 0.0764 - activation_48_loss: 0.0703 - activation_49_categorical_accuracy: 0.9701 - activation_48_categorical_accuracy: 0.9719 - val_loss: 0.4524 - val_activation_49_loss: 0.2131 - val_activation_48_loss: 0.2393 - val_activation_49_categorical_accuracy: 0.9488 - val_activation_48_categorical_accuracy: 0.9467 - lr: 8.2696e-04\n",
            "Epoch 50/251\n",
            "114/114 [==============================] - 19s 165ms/step - loss: 0.1213 - activation_49_loss: 0.0657 - activation_48_loss: 0.0556 - activation_49_categorical_accuracy: 0.9723 - activation_48_categorical_accuracy: 0.9768 - val_loss: 0.6461 - val_activation_49_loss: 0.3129 - val_activation_48_loss: 0.3332 - val_activation_49_categorical_accuracy: 0.9328 - val_activation_48_categorical_accuracy: 0.9345 - lr: 8.1873e-04\n",
            "Epoch 51/251\n",
            "114/114 [==============================] - 22s 193ms/step - loss: 0.1098 - activation_49_loss: 0.0590 - activation_48_loss: 0.0509 - activation_49_categorical_accuracy: 0.9783 - activation_48_categorical_accuracy: 0.9807 - val_loss: 0.6608 - val_activation_49_loss: 0.3019 - val_activation_48_loss: 0.3589 - val_activation_49_categorical_accuracy: 0.9328 - val_activation_48_categorical_accuracy: 0.9274 - lr: 8.1058e-04\n",
            "Epoch 52/251\n",
            "114/114 [==============================] - 19s 168ms/step - loss: 0.1097 - activation_49_loss: 0.0581 - activation_48_loss: 0.0516 - activation_49_categorical_accuracy: 0.9755 - activation_48_categorical_accuracy: 0.9783 - val_loss: 0.5916 - val_activation_49_loss: 0.2671 - val_activation_48_loss: 0.3245 - val_activation_49_categorical_accuracy: 0.9460 - val_activation_48_categorical_accuracy: 0.9382 - lr: 8.0252e-04\n",
            "Epoch 53/251\n",
            "114/114 [==============================] - 21s 182ms/step - loss: 0.1041 - activation_49_loss: 0.0559 - activation_48_loss: 0.0482 - activation_49_categorical_accuracy: 0.9777 - activation_48_categorical_accuracy: 0.9800 - val_loss: 0.6391 - val_activation_49_loss: 0.2833 - val_activation_48_loss: 0.3558 - val_activation_49_categorical_accuracy: 0.9494 - val_activation_48_categorical_accuracy: 0.9413 - lr: 7.9453e-04\n",
            "Epoch 54/251\n",
            "114/114 [==============================] - 20s 177ms/step - loss: 0.1053 - activation_49_loss: 0.0568 - activation_48_loss: 0.0485 - activation_49_categorical_accuracy: 0.9767 - activation_48_categorical_accuracy: 0.9801 - val_loss: 0.6878 - val_activation_49_loss: 0.3264 - val_activation_48_loss: 0.3614 - val_activation_49_categorical_accuracy: 0.9399 - val_activation_48_categorical_accuracy: 0.9403 - lr: 7.8663e-04\n",
            "Epoch 55/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.1053 - activation_49_loss: 0.0579 - activation_48_loss: 0.0473 - activation_49_categorical_accuracy: 0.9790 - activation_48_categorical_accuracy: 0.9820 - val_loss: 0.6457 - val_activation_49_loss: 0.2856 - val_activation_48_loss: 0.3601 - val_activation_49_categorical_accuracy: 0.9399 - val_activation_48_categorical_accuracy: 0.9376 - lr: 7.7880e-04\n",
            "Epoch 56/251\n",
            "114/114 [==============================] - 20s 173ms/step - loss: 0.0947 - activation_49_loss: 0.0520 - activation_48_loss: 0.0426 - activation_49_categorical_accuracy: 0.9803 - activation_48_categorical_accuracy: 0.9837 - val_loss: 0.6654 - val_activation_49_loss: 0.2676 - val_activation_48_loss: 0.3979 - val_activation_49_categorical_accuracy: 0.9450 - val_activation_48_categorical_accuracy: 0.9403 - lr: 7.7105e-04\n",
            "Epoch 57/251\n",
            "114/114 [==============================] - 19s 168ms/step - loss: 0.0958 - activation_49_loss: 0.0526 - activation_48_loss: 0.0432 - activation_49_categorical_accuracy: 0.9796 - activation_48_categorical_accuracy: 0.9827 - val_loss: 0.5211 - val_activation_49_loss: 0.2477 - val_activation_48_loss: 0.2734 - val_activation_49_categorical_accuracy: 0.9498 - val_activation_48_categorical_accuracy: 0.9416 - lr: 7.6338e-04\n",
            "Epoch 58/251\n",
            "114/114 [==============================] - 22s 193ms/step - loss: 0.0939 - activation_49_loss: 0.0516 - activation_48_loss: 0.0424 - activation_49_categorical_accuracy: 0.9789 - activation_48_categorical_accuracy: 0.9840 - val_loss: 0.5997 - val_activation_49_loss: 0.2757 - val_activation_48_loss: 0.3240 - val_activation_49_categorical_accuracy: 0.9450 - val_activation_48_categorical_accuracy: 0.9393 - lr: 7.5578e-04\n",
            "Epoch 59/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.0908 - activation_49_loss: 0.0510 - activation_48_loss: 0.0398 - activation_49_categorical_accuracy: 0.9811 - activation_48_categorical_accuracy: 0.9856 - val_loss: 0.7772 - val_activation_49_loss: 0.3406 - val_activation_48_loss: 0.4366 - val_activation_49_categorical_accuracy: 0.9484 - val_activation_48_categorical_accuracy: 0.9332 - lr: 7.4826e-04\n",
            "Epoch 60/251\n",
            "114/114 [==============================] - 18s 161ms/step - loss: 0.0826 - activation_49_loss: 0.0460 - activation_48_loss: 0.0366 - activation_49_categorical_accuracy: 0.9792 - activation_48_categorical_accuracy: 0.9848 - val_loss: 0.9675 - val_activation_49_loss: 0.4554 - val_activation_48_loss: 0.5121 - val_activation_49_categorical_accuracy: 0.9328 - val_activation_48_categorical_accuracy: 0.9342 - lr: 7.4082e-04\n",
            "Epoch 61/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.0774 - activation_49_loss: 0.0437 - activation_48_loss: 0.0337 - activation_49_categorical_accuracy: 0.9842 - activation_48_categorical_accuracy: 0.9873 - val_loss: 0.8461 - val_activation_49_loss: 0.3998 - val_activation_48_loss: 0.4463 - val_activation_49_categorical_accuracy: 0.9311 - val_activation_48_categorical_accuracy: 0.9260 - lr: 7.3345e-04\n",
            "Epoch 62/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.0751 - activation_49_loss: 0.0435 - activation_48_loss: 0.0316 - activation_49_categorical_accuracy: 0.9827 - activation_48_categorical_accuracy: 0.9886 - val_loss: 0.5047 - val_activation_49_loss: 0.2474 - val_activation_48_loss: 0.2572 - val_activation_49_categorical_accuracy: 0.9491 - val_activation_48_categorical_accuracy: 0.9437 - lr: 7.2615e-04\n",
            "Epoch 63/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.0709 - activation_49_loss: 0.0408 - activation_48_loss: 0.0300 - activation_49_categorical_accuracy: 0.9838 - activation_48_categorical_accuracy: 0.9878 - val_loss: 0.7951 - val_activation_49_loss: 0.3451 - val_activation_48_loss: 0.4500 - val_activation_49_categorical_accuracy: 0.9365 - val_activation_48_categorical_accuracy: 0.9308 - lr: 7.1892e-04\n",
            "Epoch 64/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.0683 - activation_49_loss: 0.0392 - activation_48_loss: 0.0290 - activation_49_categorical_accuracy: 0.9834 - activation_48_categorical_accuracy: 0.9875 - val_loss: 0.5070 - val_activation_49_loss: 0.2086 - val_activation_48_loss: 0.2984 - val_activation_49_categorical_accuracy: 0.9515 - val_activation_48_categorical_accuracy: 0.9467 - lr: 7.1177e-04\n",
            "Epoch 65/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 0.0752 - activation_49_loss: 0.0481 - activation_48_loss: 0.0270 - activation_49_categorical_accuracy: 0.9826 - activation_48_categorical_accuracy: 0.9903 - val_loss: 0.9166 - val_activation_49_loss: 0.3594 - val_activation_48_loss: 0.5572 - val_activation_49_categorical_accuracy: 0.9386 - val_activation_48_categorical_accuracy: 0.9365 - lr: 7.0469e-04\n",
            "Epoch 66/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.0764 - activation_49_loss: 0.0422 - activation_48_loss: 0.0342 - activation_49_categorical_accuracy: 0.9857 - activation_48_categorical_accuracy: 0.9883 - val_loss: 0.5056 - val_activation_49_loss: 0.2400 - val_activation_48_loss: 0.2656 - val_activation_49_categorical_accuracy: 0.9477 - val_activation_48_categorical_accuracy: 0.9477 - lr: 6.9768e-04\n",
            "Epoch 67/251\n",
            "114/114 [==============================] - 19s 163ms/step - loss: 0.0616 - activation_49_loss: 0.0368 - activation_48_loss: 0.0248 - activation_49_categorical_accuracy: 0.9855 - activation_48_categorical_accuracy: 0.9897 - val_loss: 0.6354 - val_activation_49_loss: 0.2824 - val_activation_48_loss: 0.3530 - val_activation_49_categorical_accuracy: 0.9464 - val_activation_48_categorical_accuracy: 0.9457 - lr: 6.9073e-04\n",
            "Epoch 68/251\n",
            "114/114 [==============================] - 19s 164ms/step - loss: 0.0479 - activation_49_loss: 0.0288 - activation_48_loss: 0.0191 - activation_49_categorical_accuracy: 0.9888 - activation_48_categorical_accuracy: 0.9927 - val_loss: 0.5031 - val_activation_49_loss: 0.2460 - val_activation_48_loss: 0.2571 - val_activation_49_categorical_accuracy: 0.9488 - val_activation_48_categorical_accuracy: 0.9505 - lr: 6.8386e-04\n",
            "Epoch 69/251\n",
            "114/114 [==============================] - 19s 170ms/step - loss: 0.0487 - activation_49_loss: 0.0288 - activation_48_loss: 0.0199 - activation_49_categorical_accuracy: 0.9885 - activation_48_categorical_accuracy: 0.9930 - val_loss: 0.4497 - val_activation_49_loss: 0.2112 - val_activation_48_loss: 0.2385 - val_activation_49_categorical_accuracy: 0.9572 - val_activation_48_categorical_accuracy: 0.9569 - lr: 6.7706e-04\n",
            "Epoch 70/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.0530 - activation_49_loss: 0.0319 - activation_48_loss: 0.0212 - activation_49_categorical_accuracy: 0.9882 - activation_48_categorical_accuracy: 0.9921 - val_loss: 0.7273 - val_activation_49_loss: 0.3674 - val_activation_48_loss: 0.3599 - val_activation_49_categorical_accuracy: 0.9359 - val_activation_48_categorical_accuracy: 0.9437 - lr: 6.7032e-04\n",
            "Epoch 71/251\n",
            "114/114 [==============================] - 19s 168ms/step - loss: 0.0593 - activation_49_loss: 0.0334 - activation_48_loss: 0.0259 - activation_49_categorical_accuracy: 0.9867 - activation_48_categorical_accuracy: 0.9907 - val_loss: 0.6489 - val_activation_49_loss: 0.2781 - val_activation_48_loss: 0.3707 - val_activation_49_categorical_accuracy: 0.9542 - val_activation_48_categorical_accuracy: 0.9481 - lr: 6.6365e-04\n",
            "Epoch 72/251\n",
            "114/114 [==============================] - 18s 162ms/step - loss: 0.0425 - activation_49_loss: 0.0267 - activation_48_loss: 0.0157 - activation_49_categorical_accuracy: 0.9893 - activation_48_categorical_accuracy: 0.9948 - val_loss: 0.6155 - val_activation_49_loss: 0.3040 - val_activation_48_loss: 0.3115 - val_activation_49_categorical_accuracy: 0.9518 - val_activation_48_categorical_accuracy: 0.9518 - lr: 6.5705e-04\n",
            "Epoch 73/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 0.0410 - activation_49_loss: 0.0248 - activation_48_loss: 0.0162 - activation_49_categorical_accuracy: 0.9908 - activation_48_categorical_accuracy: 0.9944 - val_loss: 0.6408 - val_activation_49_loss: 0.2824 - val_activation_48_loss: 0.3583 - val_activation_49_categorical_accuracy: 0.9511 - val_activation_48_categorical_accuracy: 0.9450 - lr: 6.5051e-04\n",
            "Epoch 74/251\n",
            "114/114 [==============================] - 19s 170ms/step - loss: 0.0476 - activation_49_loss: 0.0285 - activation_48_loss: 0.0191 - activation_49_categorical_accuracy: 0.9899 - activation_48_categorical_accuracy: 0.9926 - val_loss: 0.7334 - val_activation_49_loss: 0.2863 - val_activation_48_loss: 0.4471 - val_activation_49_categorical_accuracy: 0.9542 - val_activation_48_categorical_accuracy: 0.9518 - lr: 6.4404e-04\n",
            "Epoch 75/251\n",
            "114/114 [==============================] - 19s 170ms/step - loss: 0.0307 - activation_49_loss: 0.0198 - activation_48_loss: 0.0108 - activation_49_categorical_accuracy: 0.9933 - activation_48_categorical_accuracy: 0.9958 - val_loss: 0.7964 - val_activation_49_loss: 0.3196 - val_activation_48_loss: 0.4769 - val_activation_49_categorical_accuracy: 0.9508 - val_activation_48_categorical_accuracy: 0.9454 - lr: 6.3763e-04\n",
            "Epoch 76/251\n",
            "114/114 [==============================] - 19s 170ms/step - loss: 0.0281 - activation_49_loss: 0.0181 - activation_48_loss: 0.0101 - activation_49_categorical_accuracy: 0.9937 - activation_48_categorical_accuracy: 0.9968 - val_loss: 0.7788 - val_activation_49_loss: 0.3184 - val_activation_48_loss: 0.4605 - val_activation_49_categorical_accuracy: 0.9518 - val_activation_48_categorical_accuracy: 0.9474 - lr: 6.3128e-04\n",
            "Epoch 77/251\n",
            "114/114 [==============================] - 18s 161ms/step - loss: 0.0361 - activation_49_loss: 0.0218 - activation_48_loss: 0.0143 - activation_49_categorical_accuracy: 0.9921 - activation_48_categorical_accuracy: 0.9953 - val_loss: 0.8211 - val_activation_49_loss: 0.3287 - val_activation_48_loss: 0.4924 - val_activation_49_categorical_accuracy: 0.9535 - val_activation_48_categorical_accuracy: 0.9474 - lr: 6.2500e-04\n",
            "Epoch 78/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.0323 - activation_49_loss: 0.0213 - activation_48_loss: 0.0110 - activation_49_categorical_accuracy: 0.9922 - activation_48_categorical_accuracy: 0.9962 - val_loss: 0.7390 - val_activation_49_loss: 0.3118 - val_activation_48_loss: 0.4272 - val_activation_49_categorical_accuracy: 0.9552 - val_activation_48_categorical_accuracy: 0.9430 - lr: 6.1878e-04\n",
            "Epoch 79/251\n",
            "114/114 [==============================] - 19s 168ms/step - loss: 0.0371 - activation_49_loss: 0.0222 - activation_48_loss: 0.0149 - activation_49_categorical_accuracy: 0.9911 - activation_48_categorical_accuracy: 0.9942 - val_loss: 0.7008 - val_activation_49_loss: 0.3035 - val_activation_48_loss: 0.3972 - val_activation_49_categorical_accuracy: 0.9484 - val_activation_48_categorical_accuracy: 0.9474 - lr: 6.1263e-04\n",
            "Epoch 80/251\n",
            "114/114 [==============================] - 18s 162ms/step - loss: 0.0355 - activation_49_loss: 0.0239 - activation_48_loss: 0.0115 - activation_49_categorical_accuracy: 0.9919 - activation_48_categorical_accuracy: 0.9958 - val_loss: 0.7152 - val_activation_49_loss: 0.2878 - val_activation_48_loss: 0.4274 - val_activation_49_categorical_accuracy: 0.9528 - val_activation_48_categorical_accuracy: 0.9528 - lr: 6.0653e-04\n",
            "Epoch 81/251\n",
            "114/114 [==============================] - 23s 198ms/step - loss: 0.0343 - activation_49_loss: 0.0216 - activation_48_loss: 0.0127 - activation_49_categorical_accuracy: 0.9926 - activation_48_categorical_accuracy: 0.9964 - val_loss: 0.6571 - val_activation_49_loss: 0.2748 - val_activation_48_loss: 0.3823 - val_activation_49_categorical_accuracy: 0.9559 - val_activation_48_categorical_accuracy: 0.9562 - lr: 6.0050e-04\n",
            "Epoch 82/251\n",
            "114/114 [==============================] - 18s 162ms/step - loss: 0.0333 - activation_49_loss: 0.0196 - activation_48_loss: 0.0137 - activation_49_categorical_accuracy: 0.9927 - activation_48_categorical_accuracy: 0.9956 - val_loss: 0.5806 - val_activation_49_loss: 0.2324 - val_activation_48_loss: 0.3482 - val_activation_49_categorical_accuracy: 0.9545 - val_activation_48_categorical_accuracy: 0.9525 - lr: 5.9452e-04\n",
            "Epoch 83/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.0344 - activation_49_loss: 0.0223 - activation_48_loss: 0.0121 - activation_49_categorical_accuracy: 0.9923 - activation_48_categorical_accuracy: 0.9956 - val_loss: 0.5193 - val_activation_49_loss: 0.1963 - val_activation_48_loss: 0.3231 - val_activation_49_categorical_accuracy: 0.9606 - val_activation_48_categorical_accuracy: 0.9552 - lr: 5.8861e-04\n",
            "Epoch 84/251\n",
            "114/114 [==============================] - 21s 185ms/step - loss: 0.0248 - activation_49_loss: 0.0165 - activation_48_loss: 0.0083 - activation_49_categorical_accuracy: 0.9938 - activation_48_categorical_accuracy: 0.9968 - val_loss: 0.5954 - val_activation_49_loss: 0.2553 - val_activation_48_loss: 0.3401 - val_activation_49_categorical_accuracy: 0.9539 - val_activation_48_categorical_accuracy: 0.9511 - lr: 5.8275e-04\n",
            "Epoch 85/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.0304 - activation_49_loss: 0.0195 - activation_48_loss: 0.0109 - activation_49_categorical_accuracy: 0.9931 - activation_48_categorical_accuracy: 0.9958 - val_loss: 0.6881 - val_activation_49_loss: 0.2585 - val_activation_48_loss: 0.4297 - val_activation_49_categorical_accuracy: 0.9552 - val_activation_48_categorical_accuracy: 0.9423 - lr: 5.7695e-04\n",
            "Epoch 86/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.0275 - activation_49_loss: 0.0172 - activation_48_loss: 0.0103 - activation_49_categorical_accuracy: 0.9945 - activation_48_categorical_accuracy: 0.9963 - val_loss: 0.5597 - val_activation_49_loss: 0.2255 - val_activation_48_loss: 0.3341 - val_activation_49_categorical_accuracy: 0.9603 - val_activation_48_categorical_accuracy: 0.9549 - lr: 5.7121e-04\n",
            "Epoch 87/251\n",
            "114/114 [==============================] - 20s 171ms/step - loss: 0.0278 - activation_49_loss: 0.0175 - activation_48_loss: 0.0103 - activation_49_categorical_accuracy: 0.9926 - activation_48_categorical_accuracy: 0.9966 - val_loss: 0.5740 - val_activation_49_loss: 0.2915 - val_activation_48_loss: 0.2825 - val_activation_49_categorical_accuracy: 0.9572 - val_activation_48_categorical_accuracy: 0.9474 - lr: 5.6553e-04\n",
            "Epoch 88/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.0273 - activation_49_loss: 0.0171 - activation_48_loss: 0.0102 - activation_49_categorical_accuracy: 0.9940 - activation_48_categorical_accuracy: 0.9966 - val_loss: 0.6560 - val_activation_49_loss: 0.2724 - val_activation_48_loss: 0.3837 - val_activation_49_categorical_accuracy: 0.9545 - val_activation_48_categorical_accuracy: 0.9471 - lr: 5.5990e-04\n",
            "Epoch 89/251\n",
            "114/114 [==============================] - 23s 199ms/step - loss: 0.0234 - activation_49_loss: 0.0157 - activation_48_loss: 0.0077 - activation_49_categorical_accuracy: 0.9944 - activation_48_categorical_accuracy: 0.9974 - val_loss: 0.5870 - val_activation_49_loss: 0.2453 - val_activation_48_loss: 0.3417 - val_activation_49_categorical_accuracy: 0.9596 - val_activation_48_categorical_accuracy: 0.9572 - lr: 5.5433e-04\n",
            "Epoch 90/251\n",
            "114/114 [==============================] - 19s 165ms/step - loss: 0.0154 - activation_49_loss: 0.0104 - activation_48_loss: 0.0050 - activation_49_categorical_accuracy: 0.9956 - activation_48_categorical_accuracy: 0.9986 - val_loss: 0.7652 - val_activation_49_loss: 0.3352 - val_activation_48_loss: 0.4300 - val_activation_49_categorical_accuracy: 0.9555 - val_activation_48_categorical_accuracy: 0.9494 - lr: 5.4881e-04\n",
            "Epoch 91/251\n",
            "114/114 [==============================] - 19s 163ms/step - loss: 0.0389 - activation_49_loss: 0.0262 - activation_48_loss: 0.0127 - activation_49_categorical_accuracy: 0.9914 - activation_48_categorical_accuracy: 0.9956 - val_loss: 0.9652 - val_activation_49_loss: 0.3903 - val_activation_48_loss: 0.5749 - val_activation_49_categorical_accuracy: 0.9352 - val_activation_48_categorical_accuracy: 0.9430 - lr: 5.4335e-04\n",
            "Epoch 92/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.0202 - activation_49_loss: 0.0142 - activation_48_loss: 0.0060 - activation_49_categorical_accuracy: 0.9945 - activation_48_categorical_accuracy: 0.9977 - val_loss: 0.5956 - val_activation_49_loss: 0.2748 - val_activation_48_loss: 0.3209 - val_activation_49_categorical_accuracy: 0.9542 - val_activation_48_categorical_accuracy: 0.9606 - lr: 5.3794e-04\n",
            "Epoch 93/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.0136 - activation_49_loss: 0.0092 - activation_48_loss: 0.0045 - activation_49_categorical_accuracy: 0.9967 - activation_48_categorical_accuracy: 0.9984 - val_loss: 0.7202 - val_activation_49_loss: 0.3306 - val_activation_48_loss: 0.3896 - val_activation_49_categorical_accuracy: 0.9522 - val_activation_48_categorical_accuracy: 0.9511 - lr: 5.3259e-04\n",
            "Epoch 94/251\n",
            "114/114 [==============================] - 19s 168ms/step - loss: 0.0271 - activation_49_loss: 0.0175 - activation_48_loss: 0.0096 - activation_49_categorical_accuracy: 0.9938 - activation_48_categorical_accuracy: 0.9964 - val_loss: 0.7017 - val_activation_49_loss: 0.3244 - val_activation_48_loss: 0.3774 - val_activation_49_categorical_accuracy: 0.9511 - val_activation_48_categorical_accuracy: 0.9559 - lr: 5.2729e-04\n",
            "Epoch 95/251\n",
            "114/114 [==============================] - 23s 202ms/step - loss: 0.0296 - activation_49_loss: 0.0207 - activation_48_loss: 0.0089 - activation_49_categorical_accuracy: 0.9930 - activation_48_categorical_accuracy: 0.9968 - val_loss: 0.6620 - val_activation_49_loss: 0.2830 - val_activation_48_loss: 0.3790 - val_activation_49_categorical_accuracy: 0.9525 - val_activation_48_categorical_accuracy: 0.9562 - lr: 5.2205e-04\n",
            "Epoch 96/251\n",
            "114/114 [==============================] - 20s 179ms/step - loss: 0.0253 - activation_49_loss: 0.0159 - activation_48_loss: 0.0093 - activation_49_categorical_accuracy: 0.9949 - activation_48_categorical_accuracy: 0.9970 - val_loss: 0.7506 - val_activation_49_loss: 0.3220 - val_activation_48_loss: 0.4286 - val_activation_49_categorical_accuracy: 0.9525 - val_activation_48_categorical_accuracy: 0.9511 - lr: 5.1685e-04\n",
            "Epoch 97/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 0.0146 - activation_49_loss: 0.0095 - activation_48_loss: 0.0050 - activation_49_categorical_accuracy: 0.9963 - activation_48_categorical_accuracy: 0.9986 - val_loss: 0.6919 - val_activation_49_loss: 0.2881 - val_activation_48_loss: 0.4038 - val_activation_49_categorical_accuracy: 0.9562 - val_activation_48_categorical_accuracy: 0.9593 - lr: 5.1171e-04\n",
            "Epoch 98/251\n",
            "114/114 [==============================] - 19s 165ms/step - loss: 0.0141 - activation_49_loss: 0.0097 - activation_48_loss: 0.0044 - activation_49_categorical_accuracy: 0.9966 - activation_48_categorical_accuracy: 0.9986 - val_loss: 0.8367 - val_activation_49_loss: 0.3004 - val_activation_48_loss: 0.5364 - val_activation_49_categorical_accuracy: 0.9572 - val_activation_48_categorical_accuracy: 0.9542 - lr: 5.0662e-04\n",
            "Epoch 99/251\n",
            "114/114 [==============================] - 21s 181ms/step - loss: 0.0149 - activation_49_loss: 0.0098 - activation_48_loss: 0.0051 - activation_49_categorical_accuracy: 0.9962 - activation_48_categorical_accuracy: 0.9984 - val_loss: 0.8246 - val_activation_49_loss: 0.2725 - val_activation_48_loss: 0.5521 - val_activation_49_categorical_accuracy: 0.9552 - val_activation_48_categorical_accuracy: 0.9501 - lr: 5.0158e-04\n",
            "Epoch 100/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.0123 - activation_49_loss: 0.0074 - activation_48_loss: 0.0049 - activation_49_categorical_accuracy: 0.9979 - activation_48_categorical_accuracy: 0.9984 - val_loss: 0.5960 - val_activation_49_loss: 0.2287 - val_activation_48_loss: 0.3673 - val_activation_49_categorical_accuracy: 0.9579 - val_activation_48_categorical_accuracy: 0.9596 - lr: 4.9659e-04\n",
            "Epoch 101/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 0.0199 - activation_49_loss: 0.0125 - activation_48_loss: 0.0075 - activation_49_categorical_accuracy: 0.9962 - activation_48_categorical_accuracy: 0.9981 - val_loss: 1.0548 - val_activation_49_loss: 0.4602 - val_activation_48_loss: 0.5946 - val_activation_49_categorical_accuracy: 0.9515 - val_activation_48_categorical_accuracy: 0.9498 - lr: 4.9164e-04\n",
            "Epoch 102/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 0.0111 - activation_49_loss: 0.0075 - activation_48_loss: 0.0037 - activation_49_categorical_accuracy: 0.9974 - activation_48_categorical_accuracy: 0.9989 - val_loss: 0.8249 - val_activation_49_loss: 0.3510 - val_activation_48_loss: 0.4739 - val_activation_49_categorical_accuracy: 0.9528 - val_activation_48_categorical_accuracy: 0.9488 - lr: 4.8675e-04\n",
            "Epoch 103/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.0103 - activation_49_loss: 0.0057 - activation_48_loss: 0.0046 - activation_49_categorical_accuracy: 0.9984 - activation_48_categorical_accuracy: 0.9986 - val_loss: 0.7390 - val_activation_49_loss: 0.3354 - val_activation_48_loss: 0.4036 - val_activation_49_categorical_accuracy: 0.9515 - val_activation_48_categorical_accuracy: 0.9572 - lr: 4.8191e-04\n",
            "Epoch 104/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 0.0234 - activation_49_loss: 0.0154 - activation_48_loss: 0.0080 - activation_49_categorical_accuracy: 0.9953 - activation_48_categorical_accuracy: 0.9971 - val_loss: 0.6753 - val_activation_49_loss: 0.3081 - val_activation_48_loss: 0.3672 - val_activation_49_categorical_accuracy: 0.9535 - val_activation_48_categorical_accuracy: 0.9627 - lr: 4.7711e-04\n",
            "Epoch 105/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.0129 - activation_49_loss: 0.0093 - activation_48_loss: 0.0036 - activation_49_categorical_accuracy: 0.9973 - activation_48_categorical_accuracy: 0.9984 - val_loss: 0.6625 - val_activation_49_loss: 0.2594 - val_activation_48_loss: 0.4031 - val_activation_49_categorical_accuracy: 0.9613 - val_activation_48_categorical_accuracy: 0.9583 - lr: 4.7237e-04\n",
            "Epoch 106/251\n",
            "114/114 [==============================] - 20s 173ms/step - loss: 0.0110 - activation_49_loss: 0.0081 - activation_48_loss: 0.0028 - activation_49_categorical_accuracy: 0.9974 - activation_48_categorical_accuracy: 0.9989 - val_loss: 0.5825 - val_activation_49_loss: 0.2150 - val_activation_48_loss: 0.3675 - val_activation_49_categorical_accuracy: 0.9650 - val_activation_48_categorical_accuracy: 0.9566 - lr: 4.6767e-04\n",
            "Epoch 107/251\n",
            "114/114 [==============================] - 19s 165ms/step - loss: 0.0172 - activation_49_loss: 0.0087 - activation_48_loss: 0.0084 - activation_49_categorical_accuracy: 0.9975 - activation_48_categorical_accuracy: 0.9978 - val_loss: 0.6612 - val_activation_49_loss: 0.2665 - val_activation_48_loss: 0.3947 - val_activation_49_categorical_accuracy: 0.9596 - val_activation_48_categorical_accuracy: 0.9569 - lr: 4.6301e-04\n",
            "Epoch 108/251\n",
            "114/114 [==============================] - 19s 163ms/step - loss: 0.0165 - activation_49_loss: 0.0106 - activation_48_loss: 0.0060 - activation_49_categorical_accuracy: 0.9963 - activation_48_categorical_accuracy: 0.9977 - val_loss: 1.1573 - val_activation_49_loss: 0.4006 - val_activation_48_loss: 0.7568 - val_activation_49_categorical_accuracy: 0.9559 - val_activation_48_categorical_accuracy: 0.9477 - lr: 4.5841e-04\n",
            "Epoch 109/251\n",
            "114/114 [==============================] - 22s 190ms/step - loss: 0.0067 - activation_49_loss: 0.0046 - activation_48_loss: 0.0021 - activation_49_categorical_accuracy: 0.9985 - activation_48_categorical_accuracy: 0.9993 - val_loss: 1.0732 - val_activation_49_loss: 0.4589 - val_activation_48_loss: 0.6143 - val_activation_49_categorical_accuracy: 0.9515 - val_activation_48_categorical_accuracy: 0.9518 - lr: 4.5384e-04\n",
            "Epoch 110/251\n",
            "114/114 [==============================] - 20s 173ms/step - loss: 0.0301 - activation_49_loss: 0.0190 - activation_48_loss: 0.0111 - activation_49_categorical_accuracy: 0.9933 - activation_48_categorical_accuracy: 0.9959 - val_loss: 1.1503 - val_activation_49_loss: 0.4348 - val_activation_48_loss: 0.7155 - val_activation_49_categorical_accuracy: 0.9491 - val_activation_48_categorical_accuracy: 0.9444 - lr: 4.4933e-04\n",
            "Epoch 111/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.0226 - activation_49_loss: 0.0144 - activation_48_loss: 0.0082 - activation_49_categorical_accuracy: 0.9953 - activation_48_categorical_accuracy: 0.9977 - val_loss: 0.7674 - val_activation_49_loss: 0.3259 - val_activation_48_loss: 0.4415 - val_activation_49_categorical_accuracy: 0.9525 - val_activation_48_categorical_accuracy: 0.9549 - lr: 4.4486e-04\n",
            "Epoch 112/251\n",
            "114/114 [==============================] - 20s 174ms/step - loss: 0.0073 - activation_49_loss: 0.0057 - activation_48_loss: 0.0016 - activation_49_categorical_accuracy: 0.9979 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.6795 - val_activation_49_loss: 0.2993 - val_activation_48_loss: 0.3802 - val_activation_49_categorical_accuracy: 0.9583 - val_activation_48_categorical_accuracy: 0.9579 - lr: 4.4043e-04\n",
            "Epoch 113/251\n",
            "114/114 [==============================] - 19s 164ms/step - loss: 0.0088 - activation_49_loss: 0.0067 - activation_48_loss: 0.0021 - activation_49_categorical_accuracy: 0.9986 - activation_48_categorical_accuracy: 0.9990 - val_loss: 0.6450 - val_activation_49_loss: 0.2833 - val_activation_48_loss: 0.3617 - val_activation_49_categorical_accuracy: 0.9606 - val_activation_48_categorical_accuracy: 0.9576 - lr: 4.3605e-04\n",
            "Epoch 114/251\n",
            "114/114 [==============================] - 19s 170ms/step - loss: 0.0127 - activation_49_loss: 0.0086 - activation_48_loss: 0.0041 - activation_49_categorical_accuracy: 0.9978 - activation_48_categorical_accuracy: 0.9984 - val_loss: 0.7125 - val_activation_49_loss: 0.3444 - val_activation_48_loss: 0.3681 - val_activation_49_categorical_accuracy: 0.9586 - val_activation_48_categorical_accuracy: 0.9545 - lr: 4.3171e-04\n",
            "Epoch 115/251\n",
            "114/114 [==============================] - 21s 179ms/step - loss: 0.0168 - activation_49_loss: 0.0124 - activation_48_loss: 0.0044 - activation_49_categorical_accuracy: 0.9958 - activation_48_categorical_accuracy: 0.9984 - val_loss: 0.4922 - val_activation_49_loss: 0.1791 - val_activation_48_loss: 0.3131 - val_activation_49_categorical_accuracy: 0.9657 - val_activation_48_categorical_accuracy: 0.9654 - lr: 4.2742e-04\n",
            "Epoch 116/251\n",
            "114/114 [==============================] - 19s 164ms/step - loss: 0.0106 - activation_49_loss: 0.0091 - activation_48_loss: 0.0015 - activation_49_categorical_accuracy: 0.9975 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.5530 - val_activation_49_loss: 0.2052 - val_activation_48_loss: 0.3478 - val_activation_49_categorical_accuracy: 0.9610 - val_activation_48_categorical_accuracy: 0.9620 - lr: 4.2316e-04\n",
            "Epoch 117/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.0069 - activation_49_loss: 0.0050 - activation_48_loss: 0.0019 - activation_49_categorical_accuracy: 0.9984 - activation_48_categorical_accuracy: 0.9993 - val_loss: 0.6379 - val_activation_49_loss: 0.2638 - val_activation_48_loss: 0.3741 - val_activation_49_categorical_accuracy: 0.9613 - val_activation_48_categorical_accuracy: 0.9586 - lr: 4.1895e-04\n",
            "Epoch 118/251\n",
            "114/114 [==============================] - 19s 164ms/step - loss: 0.0093 - activation_49_loss: 0.0065 - activation_48_loss: 0.0028 - activation_49_categorical_accuracy: 0.9981 - activation_48_categorical_accuracy: 0.9990 - val_loss: 0.6054 - val_activation_49_loss: 0.2638 - val_activation_48_loss: 0.3416 - val_activation_49_categorical_accuracy: 0.9589 - val_activation_48_categorical_accuracy: 0.9600 - lr: 4.1478e-04\n",
            "Epoch 119/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.0039 - activation_49_loss: 0.0029 - activation_48_loss: 0.0010 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.4666 - val_activation_49_loss: 0.2015 - val_activation_48_loss: 0.2651 - val_activation_49_categorical_accuracy: 0.9627 - val_activation_48_categorical_accuracy: 0.9684 - lr: 4.1066e-04\n",
            "Epoch 120/251\n",
            "114/114 [==============================] - 21s 183ms/step - loss: 0.0068 - activation_49_loss: 0.0053 - activation_48_loss: 0.0015 - activation_49_categorical_accuracy: 0.9979 - activation_48_categorical_accuracy: 0.9996 - val_loss: 0.5893 - val_activation_49_loss: 0.2508 - val_activation_48_loss: 0.3386 - val_activation_49_categorical_accuracy: 0.9589 - val_activation_48_categorical_accuracy: 0.9617 - lr: 4.0657e-04\n",
            "Epoch 121/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.0129 - activation_49_loss: 0.0087 - activation_48_loss: 0.0042 - activation_49_categorical_accuracy: 0.9971 - activation_48_categorical_accuracy: 0.9981 - val_loss: 0.6833 - val_activation_49_loss: 0.2784 - val_activation_48_loss: 0.4049 - val_activation_49_categorical_accuracy: 0.9576 - val_activation_48_categorical_accuracy: 0.9549 - lr: 4.0252e-04\n",
            "Epoch 122/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 0.0114 - activation_49_loss: 0.0082 - activation_48_loss: 0.0032 - activation_49_categorical_accuracy: 0.9971 - activation_48_categorical_accuracy: 0.9989 - val_loss: 0.7140 - val_activation_49_loss: 0.2816 - val_activation_48_loss: 0.4324 - val_activation_49_categorical_accuracy: 0.9549 - val_activation_48_categorical_accuracy: 0.9579 - lr: 3.9852e-04\n",
            "Epoch 123/251\n",
            "114/114 [==============================] - 19s 165ms/step - loss: 0.0167 - activation_49_loss: 0.0108 - activation_48_loss: 0.0059 - activation_49_categorical_accuracy: 0.9962 - activation_48_categorical_accuracy: 0.9978 - val_loss: 0.7330 - val_activation_49_loss: 0.2919 - val_activation_48_loss: 0.4411 - val_activation_49_categorical_accuracy: 0.9549 - val_activation_48_categorical_accuracy: 0.9576 - lr: 3.9455e-04\n",
            "Epoch 124/251\n",
            "114/114 [==============================] - 22s 193ms/step - loss: 0.0139 - activation_49_loss: 0.0092 - activation_48_loss: 0.0047 - activation_49_categorical_accuracy: 0.9975 - activation_48_categorical_accuracy: 0.9988 - val_loss: 0.5537 - val_activation_49_loss: 0.2470 - val_activation_48_loss: 0.3067 - val_activation_49_categorical_accuracy: 0.9586 - val_activation_48_categorical_accuracy: 0.9620 - lr: 3.9063e-04\n",
            "Epoch 125/251\n",
            "114/114 [==============================] - 18s 162ms/step - loss: 0.0047 - activation_49_loss: 0.0033 - activation_48_loss: 0.0014 - activation_49_categorical_accuracy: 0.9988 - activation_48_categorical_accuracy: 0.9996 - val_loss: 0.5986 - val_activation_49_loss: 0.2576 - val_activation_48_loss: 0.3410 - val_activation_49_categorical_accuracy: 0.9603 - val_activation_48_categorical_accuracy: 0.9572 - lr: 3.8674e-04\n",
            "Epoch 126/251\n",
            "114/114 [==============================] - 18s 162ms/step - loss: 0.0113 - activation_49_loss: 0.0073 - activation_48_loss: 0.0040 - activation_49_categorical_accuracy: 0.9981 - activation_48_categorical_accuracy: 0.9985 - val_loss: 0.4594 - val_activation_49_loss: 0.2061 - val_activation_48_loss: 0.2533 - val_activation_49_categorical_accuracy: 0.9664 - val_activation_48_categorical_accuracy: 0.9671 - lr: 3.8289e-04\n",
            "Epoch 127/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.0069 - activation_49_loss: 0.0047 - activation_48_loss: 0.0022 - activation_49_categorical_accuracy: 0.9982 - activation_48_categorical_accuracy: 0.9990 - val_loss: 0.6737 - val_activation_49_loss: 0.2931 - val_activation_48_loss: 0.3806 - val_activation_49_categorical_accuracy: 0.9562 - val_activation_48_categorical_accuracy: 0.9600 - lr: 3.7908e-04\n",
            "Epoch 128/251\n",
            "114/114 [==============================] - 20s 174ms/step - loss: 0.0099 - activation_49_loss: 0.0067 - activation_48_loss: 0.0031 - activation_49_categorical_accuracy: 0.9981 - activation_48_categorical_accuracy: 0.9989 - val_loss: 0.5960 - val_activation_49_loss: 0.2482 - val_activation_48_loss: 0.3478 - val_activation_49_categorical_accuracy: 0.9613 - val_activation_48_categorical_accuracy: 0.9650 - lr: 3.7531e-04\n",
            "Epoch 129/251\n",
            "114/114 [==============================] - 19s 165ms/step - loss: 0.0104 - activation_49_loss: 0.0068 - activation_48_loss: 0.0036 - activation_49_categorical_accuracy: 0.9977 - activation_48_categorical_accuracy: 0.9984 - val_loss: 0.5782 - val_activation_49_loss: 0.2569 - val_activation_48_loss: 0.3213 - val_activation_49_categorical_accuracy: 0.9566 - val_activation_48_categorical_accuracy: 0.9620 - lr: 3.7158e-04\n",
            "Epoch 130/251\n",
            "114/114 [==============================] - 22s 194ms/step - loss: 0.0061 - activation_49_loss: 0.0046 - activation_48_loss: 0.0015 - activation_49_categorical_accuracy: 0.9986 - activation_48_categorical_accuracy: 0.9993 - val_loss: 0.5761 - val_activation_49_loss: 0.2478 - val_activation_48_loss: 0.3283 - val_activation_49_categorical_accuracy: 0.9606 - val_activation_48_categorical_accuracy: 0.9617 - lr: 3.6788e-04\n",
            "Epoch 131/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.0141 - activation_49_loss: 0.0083 - activation_48_loss: 0.0058 - activation_49_categorical_accuracy: 0.9975 - activation_48_categorical_accuracy: 0.9982 - val_loss: 0.5507 - val_activation_49_loss: 0.2430 - val_activation_48_loss: 0.3078 - val_activation_49_categorical_accuracy: 0.9583 - val_activation_48_categorical_accuracy: 0.9627 - lr: 3.6422e-04\n",
            "Epoch 132/251\n",
            "114/114 [==============================] - 19s 170ms/step - loss: 0.0120 - activation_49_loss: 0.0075 - activation_48_loss: 0.0045 - activation_49_categorical_accuracy: 0.9971 - activation_48_categorical_accuracy: 0.9986 - val_loss: 0.6295 - val_activation_49_loss: 0.2698 - val_activation_48_loss: 0.3597 - val_activation_49_categorical_accuracy: 0.9562 - val_activation_48_categorical_accuracy: 0.9572 - lr: 3.6060e-04\n",
            "Epoch 133/251\n",
            "114/114 [==============================] - 21s 183ms/step - loss: 0.0030 - activation_49_loss: 0.0022 - activation_48_loss: 7.7896e-04 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.6226 - val_activation_49_loss: 0.2732 - val_activation_48_loss: 0.3493 - val_activation_49_categorical_accuracy: 0.9596 - val_activation_48_categorical_accuracy: 0.9637 - lr: 3.5701e-04\n",
            "Epoch 134/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.0017 - activation_49_loss: 0.0015 - activation_48_loss: 2.5297e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.5962 - val_activation_49_loss: 0.2664 - val_activation_48_loss: 0.3299 - val_activation_49_categorical_accuracy: 0.9606 - val_activation_48_categorical_accuracy: 0.9620 - lr: 3.5345e-04\n",
            "Epoch 135/251\n",
            "114/114 [==============================] - 20s 171ms/step - loss: 0.0062 - activation_49_loss: 0.0054 - activation_48_loss: 7.2435e-04 - activation_49_categorical_accuracy: 0.9986 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.6884 - val_activation_49_loss: 0.3087 - val_activation_48_loss: 0.3797 - val_activation_49_categorical_accuracy: 0.9562 - val_activation_48_categorical_accuracy: 0.9623 - lr: 3.4994e-04\n",
            "Epoch 136/251\n",
            "114/114 [==============================] - 19s 164ms/step - loss: 0.0118 - activation_49_loss: 0.0076 - activation_48_loss: 0.0042 - activation_49_categorical_accuracy: 0.9973 - activation_48_categorical_accuracy: 0.9986 - val_loss: 0.6385 - val_activation_49_loss: 0.2720 - val_activation_48_loss: 0.3665 - val_activation_49_categorical_accuracy: 0.9620 - val_activation_48_categorical_accuracy: 0.9593 - lr: 3.4646e-04\n",
            "Epoch 137/251\n",
            "114/114 [==============================] - 20s 173ms/step - loss: 0.0235 - activation_49_loss: 0.0122 - activation_48_loss: 0.0113 - activation_49_categorical_accuracy: 0.9964 - activation_48_categorical_accuracy: 0.9960 - val_loss: 0.5486 - val_activation_49_loss: 0.2429 - val_activation_48_loss: 0.3056 - val_activation_49_categorical_accuracy: 0.9566 - val_activation_48_categorical_accuracy: 0.9606 - lr: 3.4301e-04\n",
            "Epoch 138/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.0073 - activation_49_loss: 0.0052 - activation_48_loss: 0.0021 - activation_49_categorical_accuracy: 0.9985 - activation_48_categorical_accuracy: 0.9993 - val_loss: 0.5326 - val_activation_49_loss: 0.2607 - val_activation_48_loss: 0.2719 - val_activation_49_categorical_accuracy: 0.9596 - val_activation_48_categorical_accuracy: 0.9664 - lr: 3.3960e-04\n",
            "Epoch 139/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 0.0060 - activation_49_loss: 0.0034 - activation_48_loss: 0.0026 - activation_49_categorical_accuracy: 0.9990 - activation_48_categorical_accuracy: 0.9990 - val_loss: 0.6088 - val_activation_49_loss: 0.2852 - val_activation_48_loss: 0.3237 - val_activation_49_categorical_accuracy: 0.9559 - val_activation_48_categorical_accuracy: 0.9542 - lr: 3.3622e-04\n",
            "Epoch 140/251\n",
            "114/114 [==============================] - 20s 177ms/step - loss: 0.0098 - activation_49_loss: 0.0068 - activation_48_loss: 0.0030 - activation_49_categorical_accuracy: 0.9975 - activation_48_categorical_accuracy: 0.9992 - val_loss: 1.0253 - val_activation_49_loss: 0.4135 - val_activation_48_loss: 0.6118 - val_activation_49_categorical_accuracy: 0.9562 - val_activation_48_categorical_accuracy: 0.9528 - lr: 3.3287e-04\n",
            "Epoch 141/251\n",
            "114/114 [==============================] - 19s 168ms/step - loss: 0.0148 - activation_49_loss: 0.0102 - activation_48_loss: 0.0046 - activation_49_categorical_accuracy: 0.9959 - activation_48_categorical_accuracy: 0.9981 - val_loss: 0.6216 - val_activation_49_loss: 0.3274 - val_activation_48_loss: 0.2943 - val_activation_49_categorical_accuracy: 0.9606 - val_activation_48_categorical_accuracy: 0.9627 - lr: 3.2956e-04\n",
            "Epoch 142/251\n",
            "114/114 [==============================] - 19s 164ms/step - loss: 0.0079 - activation_49_loss: 0.0055 - activation_48_loss: 0.0024 - activation_49_categorical_accuracy: 0.9982 - activation_48_categorical_accuracy: 0.9989 - val_loss: 0.6321 - val_activation_49_loss: 0.3154 - val_activation_48_loss: 0.3167 - val_activation_49_categorical_accuracy: 0.9600 - val_activation_48_categorical_accuracy: 0.9606 - lr: 3.2628e-04\n",
            "Epoch 143/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.0071 - activation_49_loss: 0.0054 - activation_48_loss: 0.0018 - activation_49_categorical_accuracy: 0.9979 - activation_48_categorical_accuracy: 0.9993 - val_loss: 0.5706 - val_activation_49_loss: 0.2693 - val_activation_48_loss: 0.3014 - val_activation_49_categorical_accuracy: 0.9539 - val_activation_48_categorical_accuracy: 0.9627 - lr: 3.2303e-04\n",
            "Epoch 144/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 0.0028 - activation_49_loss: 0.0023 - activation_48_loss: 4.6528e-04 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.5775 - val_activation_49_loss: 0.2816 - val_activation_48_loss: 0.2959 - val_activation_49_categorical_accuracy: 0.9579 - val_activation_48_categorical_accuracy: 0.9593 - lr: 3.1982e-04\n",
            "Epoch 145/251\n",
            "114/114 [==============================] - 18s 162ms/step - loss: 0.0024 - activation_49_loss: 0.0018 - activation_48_loss: 5.7148e-04 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.6180 - val_activation_49_loss: 0.3039 - val_activation_48_loss: 0.3141 - val_activation_49_categorical_accuracy: 0.9593 - val_activation_48_categorical_accuracy: 0.9627 - lr: 3.1664e-04\n",
            "Epoch 146/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 0.0021 - activation_49_loss: 0.0019 - activation_48_loss: 2.2567e-04 - activation_49_categorical_accuracy: 0.9995 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6812 - val_activation_49_loss: 0.3516 - val_activation_48_loss: 0.3296 - val_activation_49_categorical_accuracy: 0.9583 - val_activation_48_categorical_accuracy: 0.9600 - lr: 3.1349e-04\n",
            "Epoch 147/251\n",
            "114/114 [==============================] - 23s 197ms/step - loss: 0.0056 - activation_49_loss: 0.0029 - activation_48_loss: 0.0027 - activation_49_categorical_accuracy: 0.9989 - activation_48_categorical_accuracy: 0.9989 - val_loss: 0.6537 - val_activation_49_loss: 0.3453 - val_activation_48_loss: 0.3085 - val_activation_49_categorical_accuracy: 0.9555 - val_activation_48_categorical_accuracy: 0.9572 - lr: 3.1037e-04\n",
            "Epoch 148/251\n",
            "114/114 [==============================] - 22s 196ms/step - loss: 0.0039 - activation_49_loss: 0.0027 - activation_48_loss: 0.0012 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9995 - val_loss: 0.7948 - val_activation_49_loss: 0.3967 - val_activation_48_loss: 0.3981 - val_activation_49_categorical_accuracy: 0.9535 - val_activation_48_categorical_accuracy: 0.9576 - lr: 3.0728e-04\n",
            "Epoch 149/251\n",
            "114/114 [==============================] - 25s 220ms/step - loss: 0.0042 - activation_49_loss: 0.0026 - activation_48_loss: 0.0017 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 0.9996 - val_loss: 0.6870 - val_activation_49_loss: 0.3279 - val_activation_48_loss: 0.3591 - val_activation_49_categorical_accuracy: 0.9555 - val_activation_48_categorical_accuracy: 0.9634 - lr: 3.0422e-04\n",
            "Epoch 150/251\n",
            "114/114 [==============================] - 21s 187ms/step - loss: 0.0055 - activation_49_loss: 0.0031 - activation_48_loss: 0.0023 - activation_49_categorical_accuracy: 0.9989 - activation_48_categorical_accuracy: 0.9993 - val_loss: 0.6592 - val_activation_49_loss: 0.3259 - val_activation_48_loss: 0.3333 - val_activation_49_categorical_accuracy: 0.9596 - val_activation_48_categorical_accuracy: 0.9613 - lr: 3.0119e-04\n",
            "Epoch 151/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.0088 - activation_49_loss: 0.0058 - activation_48_loss: 0.0030 - activation_49_categorical_accuracy: 0.9984 - activation_48_categorical_accuracy: 0.9993 - val_loss: 0.6915 - val_activation_49_loss: 0.3333 - val_activation_48_loss: 0.3583 - val_activation_49_categorical_accuracy: 0.9569 - val_activation_48_categorical_accuracy: 0.9572 - lr: 2.9820e-04\n",
            "Epoch 152/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.0135 - activation_49_loss: 0.0086 - activation_48_loss: 0.0049 - activation_49_categorical_accuracy: 0.9971 - activation_48_categorical_accuracy: 0.9986 - val_loss: 0.7138 - val_activation_49_loss: 0.3335 - val_activation_48_loss: 0.3803 - val_activation_49_categorical_accuracy: 0.9572 - val_activation_48_categorical_accuracy: 0.9586 - lr: 2.9523e-04\n",
            "Epoch 153/251\n",
            "114/114 [==============================] - 19s 164ms/step - loss: 0.0061 - activation_49_loss: 0.0043 - activation_48_loss: 0.0018 - activation_49_categorical_accuracy: 0.9990 - activation_48_categorical_accuracy: 0.9996 - val_loss: 0.8716 - val_activation_49_loss: 0.3962 - val_activation_48_loss: 0.4754 - val_activation_49_categorical_accuracy: 0.9566 - val_activation_48_categorical_accuracy: 0.9569 - lr: 2.9229e-04\n",
            "Epoch 154/251\n",
            "114/114 [==============================] - 20s 173ms/step - loss: 0.0036 - activation_49_loss: 0.0026 - activation_48_loss: 0.0010 - activation_49_categorical_accuracy: 0.9990 - activation_48_categorical_accuracy: 0.9996 - val_loss: 0.7026 - val_activation_49_loss: 0.3333 - val_activation_48_loss: 0.3693 - val_activation_49_categorical_accuracy: 0.9606 - val_activation_48_categorical_accuracy: 0.9586 - lr: 2.8938e-04\n",
            "Epoch 155/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 0.0020 - activation_49_loss: 0.0011 - activation_48_loss: 8.6232e-04 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.7346 - val_activation_49_loss: 0.3353 - val_activation_48_loss: 0.3993 - val_activation_49_categorical_accuracy: 0.9603 - val_activation_48_categorical_accuracy: 0.9593 - lr: 2.8651e-04\n",
            "Epoch 156/251\n",
            "114/114 [==============================] - 19s 163ms/step - loss: 0.0080 - activation_49_loss: 0.0055 - activation_48_loss: 0.0026 - activation_49_categorical_accuracy: 0.9984 - activation_48_categorical_accuracy: 0.9993 - val_loss: 0.8564 - val_activation_49_loss: 0.4169 - val_activation_48_loss: 0.4395 - val_activation_49_categorical_accuracy: 0.9576 - val_activation_48_categorical_accuracy: 0.9617 - lr: 2.8365e-04\n",
            "Epoch 157/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.0072 - activation_49_loss: 0.0055 - activation_48_loss: 0.0017 - activation_49_categorical_accuracy: 0.9979 - activation_48_categorical_accuracy: 0.9996 - val_loss: 0.7608 - val_activation_49_loss: 0.3920 - val_activation_48_loss: 0.3688 - val_activation_49_categorical_accuracy: 0.9552 - val_activation_48_categorical_accuracy: 0.9606 - lr: 2.8083e-04\n",
            "Epoch 158/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.0029 - activation_49_loss: 0.0018 - activation_48_loss: 0.0011 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 0.9996 - val_loss: 0.6893 - val_activation_49_loss: 0.3651 - val_activation_48_loss: 0.3242 - val_activation_49_categorical_accuracy: 0.9579 - val_activation_48_categorical_accuracy: 0.9613 - lr: 2.7804e-04\n",
            "Epoch 159/251\n",
            "114/114 [==============================] - 19s 164ms/step - loss: 0.0044 - activation_49_loss: 0.0034 - activation_48_loss: 0.0010 - activation_49_categorical_accuracy: 0.9989 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.5770 - val_activation_49_loss: 0.3018 - val_activation_48_loss: 0.2751 - val_activation_49_categorical_accuracy: 0.9623 - val_activation_48_categorical_accuracy: 0.9688 - lr: 2.7527e-04\n",
            "Epoch 160/251\n",
            "114/114 [==============================] - 23s 200ms/step - loss: 0.0114 - activation_49_loss: 0.0074 - activation_48_loss: 0.0040 - activation_49_categorical_accuracy: 0.9971 - activation_48_categorical_accuracy: 0.9984 - val_loss: 0.6656 - val_activation_49_loss: 0.3307 - val_activation_48_loss: 0.3349 - val_activation_49_categorical_accuracy: 0.9576 - val_activation_48_categorical_accuracy: 0.9589 - lr: 2.7253e-04\n",
            "Epoch 161/251\n",
            "114/114 [==============================] - 20s 177ms/step - loss: 0.0037 - activation_49_loss: 0.0028 - activation_48_loss: 8.9474e-04 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.6827 - val_activation_49_loss: 0.3058 - val_activation_48_loss: 0.3768 - val_activation_49_categorical_accuracy: 0.9566 - val_activation_48_categorical_accuracy: 0.9620 - lr: 2.6982e-04\n",
            "Epoch 162/251\n",
            "114/114 [==============================] - 20s 174ms/step - loss: 0.0026 - activation_49_loss: 0.0021 - activation_48_loss: 4.4183e-04 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.7287 - val_activation_49_loss: 0.3152 - val_activation_48_loss: 0.4135 - val_activation_49_categorical_accuracy: 0.9572 - val_activation_48_categorical_accuracy: 0.9613 - lr: 2.6714e-04\n",
            "Epoch 163/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 0.0023 - activation_49_loss: 0.0017 - activation_48_loss: 6.1016e-04 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.5942 - val_activation_49_loss: 0.2760 - val_activation_48_loss: 0.3182 - val_activation_49_categorical_accuracy: 0.9620 - val_activation_48_categorical_accuracy: 0.9664 - lr: 2.6448e-04\n",
            "Epoch 164/251\n",
            "114/114 [==============================] - 21s 188ms/step - loss: 0.0093 - activation_49_loss: 0.0059 - activation_48_loss: 0.0035 - activation_49_categorical_accuracy: 0.9977 - activation_48_categorical_accuracy: 0.9988 - val_loss: 0.6026 - val_activation_49_loss: 0.2857 - val_activation_48_loss: 0.3169 - val_activation_49_categorical_accuracy: 0.9627 - val_activation_48_categorical_accuracy: 0.9620 - lr: 2.6185e-04\n",
            "Epoch 165/251\n",
            "114/114 [==============================] - 19s 165ms/step - loss: 0.0036 - activation_49_loss: 0.0027 - activation_48_loss: 8.4272e-04 - activation_49_categorical_accuracy: 0.9990 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.6353 - val_activation_49_loss: 0.2961 - val_activation_48_loss: 0.3391 - val_activation_49_categorical_accuracy: 0.9600 - val_activation_48_categorical_accuracy: 0.9644 - lr: 2.5924e-04\n",
            "Epoch 166/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.0025 - activation_49_loss: 0.0022 - activation_48_loss: 2.8741e-04 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6093 - val_activation_49_loss: 0.2870 - val_activation_48_loss: 0.3224 - val_activation_49_categorical_accuracy: 0.9627 - val_activation_48_categorical_accuracy: 0.9620 - lr: 2.5666e-04\n",
            "Epoch 167/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 0.0012 - activation_49_loss: 0.0011 - activation_48_loss: 1.4261e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6344 - val_activation_49_loss: 0.3123 - val_activation_48_loss: 0.3222 - val_activation_49_categorical_accuracy: 0.9596 - val_activation_48_categorical_accuracy: 0.9657 - lr: 2.5411e-04\n",
            "Epoch 168/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.0036 - activation_49_loss: 0.0034 - activation_48_loss: 1.9048e-04 - activation_49_categorical_accuracy: 0.9995 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7189 - val_activation_49_loss: 0.3948 - val_activation_48_loss: 0.3241 - val_activation_49_categorical_accuracy: 0.9603 - val_activation_48_categorical_accuracy: 0.9678 - lr: 2.5158e-04\n",
            "Epoch 169/251\n",
            "114/114 [==============================] - 19s 170ms/step - loss: 0.0053 - activation_49_loss: 0.0042 - activation_48_loss: 0.0011 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9996 - val_loss: 0.7700 - val_activation_49_loss: 0.3950 - val_activation_48_loss: 0.3750 - val_activation_49_categorical_accuracy: 0.9559 - val_activation_48_categorical_accuracy: 0.9630 - lr: 2.4908e-04\n",
            "Epoch 170/251\n",
            "114/114 [==============================] - 19s 165ms/step - loss: 0.0061 - activation_49_loss: 0.0048 - activation_48_loss: 0.0014 - activation_49_categorical_accuracy: 0.9984 - activation_48_categorical_accuracy: 0.9992 - val_loss: 0.7205 - val_activation_49_loss: 0.3724 - val_activation_48_loss: 0.3481 - val_activation_49_categorical_accuracy: 0.9620 - val_activation_48_categorical_accuracy: 0.9657 - lr: 2.4660e-04\n",
            "Epoch 171/251\n",
            "114/114 [==============================] - 22s 196ms/step - loss: 0.0068 - activation_49_loss: 0.0033 - activation_48_loss: 0.0035 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 0.9995 - val_loss: 0.7847 - val_activation_49_loss: 0.4011 - val_activation_48_loss: 0.3836 - val_activation_49_categorical_accuracy: 0.9525 - val_activation_48_categorical_accuracy: 0.9593 - lr: 2.4414e-04\n",
            "Epoch 172/251\n",
            "114/114 [==============================] - 19s 163ms/step - loss: 0.0041 - activation_49_loss: 0.0029 - activation_48_loss: 0.0012 - activation_49_categorical_accuracy: 0.9989 - activation_48_categorical_accuracy: 0.9996 - val_loss: 0.8398 - val_activation_49_loss: 0.4926 - val_activation_48_loss: 0.3472 - val_activation_49_categorical_accuracy: 0.9542 - val_activation_48_categorical_accuracy: 0.9644 - lr: 2.4171e-04\n",
            "Epoch 173/251\n",
            "114/114 [==============================] - 19s 168ms/step - loss: 0.0017 - activation_49_loss: 0.0014 - activation_48_loss: 2.2786e-04 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.9125 - val_activation_49_loss: 0.4819 - val_activation_48_loss: 0.4306 - val_activation_49_categorical_accuracy: 0.9535 - val_activation_48_categorical_accuracy: 0.9634 - lr: 2.3931e-04\n",
            "Epoch 174/251\n",
            "114/114 [==============================] - 23s 198ms/step - loss: 0.0056 - activation_49_loss: 0.0044 - activation_48_loss: 0.0012 - activation_49_categorical_accuracy: 0.9985 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.9742 - val_activation_49_loss: 0.4907 - val_activation_48_loss: 0.4836 - val_activation_49_categorical_accuracy: 0.9528 - val_activation_48_categorical_accuracy: 0.9552 - lr: 2.3693e-04\n",
            "Epoch 175/251\n",
            "114/114 [==============================] - 20s 173ms/step - loss: 0.0039 - activation_49_loss: 0.0027 - activation_48_loss: 0.0012 - activation_49_categorical_accuracy: 0.9995 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.9808 - val_activation_49_loss: 0.4885 - val_activation_48_loss: 0.4924 - val_activation_49_categorical_accuracy: 0.9532 - val_activation_48_categorical_accuracy: 0.9542 - lr: 2.3457e-04\n",
            "Epoch 176/251\n",
            "114/114 [==============================] - 20s 173ms/step - loss: 0.0024 - activation_49_loss: 0.0019 - activation_48_loss: 4.5228e-04 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.6018 - val_activation_49_loss: 0.2987 - val_activation_48_loss: 0.3031 - val_activation_49_categorical_accuracy: 0.9627 - val_activation_48_categorical_accuracy: 0.9637 - lr: 2.3224e-04\n",
            "Epoch 177/251\n",
            "114/114 [==============================] - 20s 179ms/step - loss: 0.0018 - activation_49_loss: 0.0013 - activation_48_loss: 4.7956e-04 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.7228 - val_activation_49_loss: 0.3232 - val_activation_48_loss: 0.3995 - val_activation_49_categorical_accuracy: 0.9600 - val_activation_48_categorical_accuracy: 0.9593 - lr: 2.2993e-04\n",
            "Epoch 178/251\n",
            "114/114 [==============================] - 19s 163ms/step - loss: 9.7655e-04 - activation_49_loss: 7.1512e-04 - activation_48_loss: 2.6144e-04 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.6998 - val_activation_49_loss: 0.3117 - val_activation_48_loss: 0.3881 - val_activation_49_categorical_accuracy: 0.9617 - val_activation_48_categorical_accuracy: 0.9596 - lr: 2.2764e-04\n",
            "Epoch 179/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 6.8088e-04 - activation_49_loss: 4.5561e-04 - activation_48_loss: 2.2527e-04 - activation_49_categorical_accuracy: 1.0000 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.8071 - val_activation_49_loss: 0.3662 - val_activation_48_loss: 0.4409 - val_activation_49_categorical_accuracy: 0.9539 - val_activation_48_categorical_accuracy: 0.9539 - lr: 2.2537e-04\n",
            "Epoch 180/251\n",
            "114/114 [==============================] - 20s 177ms/step - loss: 0.0028 - activation_49_loss: 0.0022 - activation_48_loss: 5.5414e-04 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.5423 - val_activation_49_loss: 0.2406 - val_activation_48_loss: 0.3017 - val_activation_49_categorical_accuracy: 0.9640 - val_activation_48_categorical_accuracy: 0.9657 - lr: 2.2313e-04\n",
            "Epoch 181/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 8.2147e-04 - activation_49_loss: 5.9061e-04 - activation_48_loss: 2.3086e-04 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.5225 - val_activation_49_loss: 0.2408 - val_activation_48_loss: 0.2817 - val_activation_49_categorical_accuracy: 0.9654 - val_activation_48_categorical_accuracy: 0.9691 - lr: 2.2091e-04\n",
            "Epoch 182/251\n",
            "114/114 [==============================] - 20s 173ms/step - loss: 7.8255e-04 - activation_49_loss: 6.0989e-04 - activation_48_loss: 1.7266e-04 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.5239 - val_activation_49_loss: 0.2451 - val_activation_48_loss: 0.2788 - val_activation_49_categorical_accuracy: 0.9661 - val_activation_48_categorical_accuracy: 0.9712 - lr: 2.1871e-04\n",
            "Epoch 183/251\n",
            "114/114 [==============================] - 20s 173ms/step - loss: 0.0027 - activation_49_loss: 0.0025 - activation_48_loss: 1.8154e-04 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.5908 - val_activation_49_loss: 0.2443 - val_activation_48_loss: 0.3465 - val_activation_49_categorical_accuracy: 0.9657 - val_activation_48_categorical_accuracy: 0.9637 - lr: 2.1654e-04\n",
            "Epoch 184/251\n",
            "114/114 [==============================] - 20s 178ms/step - loss: 0.0015 - activation_49_loss: 0.0013 - activation_48_loss: 2.3047e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.6107 - val_activation_49_loss: 0.2731 - val_activation_48_loss: 0.3376 - val_activation_49_categorical_accuracy: 0.9667 - val_activation_48_categorical_accuracy: 0.9637 - lr: 2.1438e-04\n",
            "Epoch 185/251\n",
            "114/114 [==============================] - 20s 171ms/step - loss: 0.0031 - activation_49_loss: 0.0027 - activation_48_loss: 4.4118e-04 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.5829 - val_activation_49_loss: 0.2520 - val_activation_48_loss: 0.3309 - val_activation_49_categorical_accuracy: 0.9678 - val_activation_48_categorical_accuracy: 0.9617 - lr: 2.1225e-04\n",
            "Epoch 186/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 0.0033 - activation_49_loss: 0.0026 - activation_48_loss: 7.4254e-04 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.6048 - val_activation_49_loss: 0.2452 - val_activation_48_loss: 0.3597 - val_activation_49_categorical_accuracy: 0.9671 - val_activation_48_categorical_accuracy: 0.9606 - lr: 2.1014e-04\n",
            "Epoch 187/251\n",
            "114/114 [==============================] - 20s 173ms/step - loss: 9.2669e-04 - activation_49_loss: 7.1898e-04 - activation_48_loss: 2.0771e-04 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6220 - val_activation_49_loss: 0.2508 - val_activation_48_loss: 0.3712 - val_activation_49_categorical_accuracy: 0.9647 - val_activation_48_categorical_accuracy: 0.9637 - lr: 2.0805e-04\n",
            "Epoch 188/251\n",
            "114/114 [==============================] - 23s 204ms/step - loss: 3.2779e-04 - activation_49_loss: 2.5828e-04 - activation_48_loss: 6.9512e-05 - activation_49_categorical_accuracy: 1.0000 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6213 - val_activation_49_loss: 0.2546 - val_activation_48_loss: 0.3666 - val_activation_49_categorical_accuracy: 0.9647 - val_activation_48_categorical_accuracy: 0.9627 - lr: 2.0598e-04\n",
            "Epoch 189/251\n",
            "114/114 [==============================] - 22s 197ms/step - loss: 0.0024 - activation_49_loss: 0.0016 - activation_48_loss: 8.1358e-04 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.8011 - val_activation_49_loss: 0.3271 - val_activation_48_loss: 0.4740 - val_activation_49_categorical_accuracy: 0.9542 - val_activation_48_categorical_accuracy: 0.9562 - lr: 2.0393e-04\n",
            "Epoch 190/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 0.0086 - activation_49_loss: 0.0059 - activation_48_loss: 0.0028 - activation_49_categorical_accuracy: 0.9975 - activation_48_categorical_accuracy: 0.9993 - val_loss: 0.7382 - val_activation_49_loss: 0.3052 - val_activation_48_loss: 0.4330 - val_activation_49_categorical_accuracy: 0.9623 - val_activation_48_categorical_accuracy: 0.9627 - lr: 2.0190e-04\n",
            "Epoch 191/251\n",
            "114/114 [==============================] - 21s 180ms/step - loss: 0.0028 - activation_49_loss: 0.0023 - activation_48_loss: 5.3517e-04 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.5943 - val_activation_49_loss: 0.2444 - val_activation_48_loss: 0.3499 - val_activation_49_categorical_accuracy: 0.9644 - val_activation_48_categorical_accuracy: 0.9674 - lr: 1.9989e-04\n",
            "Epoch 192/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 0.0043 - activation_49_loss: 0.0028 - activation_48_loss: 0.0015 - activation_49_categorical_accuracy: 0.9990 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.6090 - val_activation_49_loss: 0.2508 - val_activation_48_loss: 0.3582 - val_activation_49_categorical_accuracy: 0.9613 - val_activation_48_categorical_accuracy: 0.9681 - lr: 1.9790e-04\n",
            "Epoch 193/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.0047 - activation_49_loss: 0.0034 - activation_48_loss: 0.0013 - activation_49_categorical_accuracy: 0.9988 - activation_48_categorical_accuracy: 0.9995 - val_loss: 0.6824 - val_activation_49_loss: 0.2635 - val_activation_48_loss: 0.4188 - val_activation_49_categorical_accuracy: 0.9617 - val_activation_48_categorical_accuracy: 0.9664 - lr: 1.9593e-04\n",
            "Epoch 194/251\n",
            "114/114 [==============================] - 22s 189ms/step - loss: 0.0017 - activation_49_loss: 0.0016 - activation_48_loss: 1.1406e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7022 - val_activation_49_loss: 0.2855 - val_activation_48_loss: 0.4168 - val_activation_49_categorical_accuracy: 0.9596 - val_activation_48_categorical_accuracy: 0.9667 - lr: 1.9398e-04\n",
            "Epoch 195/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 6.7083e-04 - activation_49_loss: 6.1307e-04 - activation_48_loss: 5.7760e-05 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6701 - val_activation_49_loss: 0.2806 - val_activation_48_loss: 0.3895 - val_activation_49_categorical_accuracy: 0.9610 - val_activation_48_categorical_accuracy: 0.9678 - lr: 1.9205e-04\n",
            "Epoch 196/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 0.0020 - activation_49_loss: 0.0016 - activation_48_loss: 3.8538e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.6895 - val_activation_49_loss: 0.2756 - val_activation_48_loss: 0.4139 - val_activation_49_categorical_accuracy: 0.9606 - val_activation_48_categorical_accuracy: 0.9634 - lr: 1.9014e-04\n",
            "Epoch 197/251\n",
            "114/114 [==============================] - 19s 165ms/step - loss: 0.0023 - activation_49_loss: 0.0017 - activation_48_loss: 6.3880e-04 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.6276 - val_activation_49_loss: 0.2566 - val_activation_48_loss: 0.3709 - val_activation_49_categorical_accuracy: 0.9630 - val_activation_48_categorical_accuracy: 0.9637 - lr: 1.8825e-04\n",
            "Epoch 198/251\n",
            "114/114 [==============================] - 22s 196ms/step - loss: 0.0018 - activation_49_loss: 0.0014 - activation_48_loss: 3.4910e-04 - activation_49_categorical_accuracy: 0.9995 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.6464 - val_activation_49_loss: 0.2664 - val_activation_48_loss: 0.3800 - val_activation_49_categorical_accuracy: 0.9637 - val_activation_48_categorical_accuracy: 0.9617 - lr: 1.8637e-04\n",
            "Epoch 199/251\n",
            "114/114 [==============================] - 23s 198ms/step - loss: 0.0018 - activation_49_loss: 0.0015 - activation_48_loss: 2.8992e-04 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.5202 - val_activation_49_loss: 0.2141 - val_activation_48_loss: 0.3061 - val_activation_49_categorical_accuracy: 0.9695 - val_activation_48_categorical_accuracy: 0.9708 - lr: 1.8452e-04\n",
            "Epoch 200/251\n",
            "114/114 [==============================] - 19s 170ms/step - loss: 0.0011 - activation_49_loss: 9.5775e-04 - activation_48_loss: 1.0513e-04 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6797 - val_activation_49_loss: 0.2661 - val_activation_48_loss: 0.4136 - val_activation_49_categorical_accuracy: 0.9610 - val_activation_48_categorical_accuracy: 0.9647 - lr: 1.8268e-04\n",
            "Epoch 201/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.0020 - activation_49_loss: 0.0016 - activation_48_loss: 3.9535e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.7049 - val_activation_49_loss: 0.2949 - val_activation_48_loss: 0.4100 - val_activation_49_categorical_accuracy: 0.9606 - val_activation_48_categorical_accuracy: 0.9661 - lr: 1.8087e-04\n",
            "Epoch 202/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.0029 - activation_49_loss: 0.0021 - activation_48_loss: 7.9970e-04 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 0.9996 - val_loss: 0.5828 - val_activation_49_loss: 0.2530 - val_activation_48_loss: 0.3298 - val_activation_49_categorical_accuracy: 0.9691 - val_activation_48_categorical_accuracy: 0.9695 - lr: 1.7907e-04\n",
            "Epoch 203/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 0.0037 - activation_49_loss: 0.0026 - activation_48_loss: 0.0011 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.8273 - val_activation_49_loss: 0.3670 - val_activation_48_loss: 0.4603 - val_activation_49_categorical_accuracy: 0.9576 - val_activation_48_categorical_accuracy: 0.9555 - lr: 1.7728e-04\n",
            "Epoch 204/251\n",
            "114/114 [==============================] - 20s 177ms/step - loss: 0.0016 - activation_49_loss: 0.0014 - activation_48_loss: 1.1025e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.8040 - val_activation_49_loss: 0.3427 - val_activation_48_loss: 0.4613 - val_activation_49_categorical_accuracy: 0.9589 - val_activation_48_categorical_accuracy: 0.9559 - lr: 1.7552e-04\n",
            "Epoch 205/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.0029 - activation_49_loss: 0.0024 - activation_48_loss: 5.6158e-04 - activation_49_categorical_accuracy: 0.9990 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.6669 - val_activation_49_loss: 0.2776 - val_activation_48_loss: 0.3894 - val_activation_49_categorical_accuracy: 0.9664 - val_activation_48_categorical_accuracy: 0.9613 - lr: 1.7377e-04\n",
            "Epoch 206/251\n",
            "114/114 [==============================] - 19s 165ms/step - loss: 4.8628e-04 - activation_49_loss: 4.5288e-04 - activation_48_loss: 3.3398e-05 - activation_49_categorical_accuracy: 1.0000 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7600 - val_activation_49_loss: 0.3133 - val_activation_48_loss: 0.4467 - val_activation_49_categorical_accuracy: 0.9617 - val_activation_48_categorical_accuracy: 0.9589 - lr: 1.7205e-04\n",
            "Epoch 207/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.0011 - activation_49_loss: 8.7513e-04 - activation_48_loss: 2.0617e-04 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.7783 - val_activation_49_loss: 0.3394 - val_activation_48_loss: 0.4389 - val_activation_49_categorical_accuracy: 0.9647 - val_activation_48_categorical_accuracy: 0.9606 - lr: 1.7033e-04\n",
            "Epoch 208/251\n",
            "114/114 [==============================] - 20s 179ms/step - loss: 3.9211e-04 - activation_49_loss: 3.6115e-04 - activation_48_loss: 3.0958e-05 - activation_49_categorical_accuracy: 1.0000 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.8085 - val_activation_49_loss: 0.3503 - val_activation_48_loss: 0.4582 - val_activation_49_categorical_accuracy: 0.9630 - val_activation_48_categorical_accuracy: 0.9586 - lr: 1.6864e-04\n",
            "Epoch 209/251\n",
            "114/114 [==============================] - 22s 191ms/step - loss: 3.1086e-04 - activation_49_loss: 2.8622e-04 - activation_48_loss: 2.4637e-05 - activation_49_categorical_accuracy: 1.0000 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7893 - val_activation_49_loss: 0.3293 - val_activation_48_loss: 0.4601 - val_activation_49_categorical_accuracy: 0.9650 - val_activation_48_categorical_accuracy: 0.9589 - lr: 1.6696e-04\n",
            "Epoch 210/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 4.2339e-04 - activation_49_loss: 3.3900e-04 - activation_48_loss: 8.4387e-05 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.8070 - val_activation_49_loss: 0.3450 - val_activation_48_loss: 0.4619 - val_activation_49_categorical_accuracy: 0.9630 - val_activation_48_categorical_accuracy: 0.9593 - lr: 1.6530e-04\n",
            "Epoch 211/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.0028 - activation_49_loss: 0.0020 - activation_48_loss: 7.8635e-04 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.8671 - val_activation_49_loss: 0.3804 - val_activation_48_loss: 0.4867 - val_activation_49_categorical_accuracy: 0.9589 - val_activation_48_categorical_accuracy: 0.9576 - lr: 1.6365e-04\n",
            "Epoch 212/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 0.0013 - activation_49_loss: 0.0012 - activation_48_loss: 1.5762e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7819 - val_activation_49_loss: 0.3443 - val_activation_48_loss: 0.4376 - val_activation_49_categorical_accuracy: 0.9623 - val_activation_48_categorical_accuracy: 0.9579 - lr: 1.6203e-04\n",
            "Epoch 213/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 0.0017 - activation_49_loss: 0.0014 - activation_48_loss: 2.3665e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7114 - val_activation_49_loss: 0.3257 - val_activation_48_loss: 0.3857 - val_activation_49_categorical_accuracy: 0.9681 - val_activation_48_categorical_accuracy: 0.9610 - lr: 1.6041e-04\n",
            "Epoch 214/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 0.0015 - activation_49_loss: 0.0013 - activation_48_loss: 1.8621e-04 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7801 - val_activation_49_loss: 0.3529 - val_activation_48_loss: 0.4272 - val_activation_49_categorical_accuracy: 0.9600 - val_activation_48_categorical_accuracy: 0.9586 - lr: 1.5882e-04\n",
            "Epoch 215/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 8.7640e-04 - activation_49_loss: 6.8742e-04 - activation_48_loss: 1.8898e-04 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7157 - val_activation_49_loss: 0.3251 - val_activation_48_loss: 0.3906 - val_activation_49_categorical_accuracy: 0.9623 - val_activation_48_categorical_accuracy: 0.9613 - lr: 1.5724e-04\n",
            "Epoch 216/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 0.0013 - activation_49_loss: 0.0011 - activation_48_loss: 1.4059e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7030 - val_activation_49_loss: 0.2966 - val_activation_48_loss: 0.4064 - val_activation_49_categorical_accuracy: 0.9637 - val_activation_48_categorical_accuracy: 0.9606 - lr: 1.5567e-04\n",
            "Epoch 217/251\n",
            "114/114 [==============================] - 23s 205ms/step - loss: 0.0079 - activation_49_loss: 0.0051 - activation_48_loss: 0.0028 - activation_49_categorical_accuracy: 0.9986 - activation_48_categorical_accuracy: 0.9993 - val_loss: 0.6441 - val_activation_49_loss: 0.2647 - val_activation_48_loss: 0.3795 - val_activation_49_categorical_accuracy: 0.9667 - val_activation_48_categorical_accuracy: 0.9596 - lr: 1.5412e-04\n",
            "Epoch 218/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.0015 - activation_49_loss: 0.0013 - activation_48_loss: 2.1692e-04 - activation_49_categorical_accuracy: 0.9995 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6455 - val_activation_49_loss: 0.2697 - val_activation_48_loss: 0.3758 - val_activation_49_categorical_accuracy: 0.9664 - val_activation_48_categorical_accuracy: 0.9617 - lr: 1.5259e-04\n",
            "Epoch 219/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.0017 - activation_49_loss: 0.0011 - activation_48_loss: 5.3036e-04 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.6450 - val_activation_49_loss: 0.2631 - val_activation_48_loss: 0.3819 - val_activation_49_categorical_accuracy: 0.9647 - val_activation_48_categorical_accuracy: 0.9610 - lr: 1.5107e-04\n",
            "Epoch 220/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 0.0014 - activation_49_loss: 0.0012 - activation_48_loss: 2.5170e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.6991 - val_activation_49_loss: 0.2912 - val_activation_48_loss: 0.4079 - val_activation_49_categorical_accuracy: 0.9634 - val_activation_48_categorical_accuracy: 0.9596 - lr: 1.4957e-04\n",
            "Epoch 221/251\n",
            "114/114 [==============================] - 20s 174ms/step - loss: 0.0012 - activation_49_loss: 0.0012 - activation_48_loss: 5.7685e-05 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7050 - val_activation_49_loss: 0.2952 - val_activation_48_loss: 0.4098 - val_activation_49_categorical_accuracy: 0.9640 - val_activation_48_categorical_accuracy: 0.9606 - lr: 1.4808e-04\n",
            "Epoch 222/251\n",
            "114/114 [==============================] - 19s 166ms/step - loss: 8.7360e-04 - activation_49_loss: 6.4782e-04 - activation_48_loss: 2.2577e-04 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.7662 - val_activation_49_loss: 0.3061 - val_activation_48_loss: 0.4602 - val_activation_49_categorical_accuracy: 0.9644 - val_activation_48_categorical_accuracy: 0.9566 - lr: 1.4661e-04\n",
            "Epoch 223/251\n",
            "114/114 [==============================] - 23s 203ms/step - loss: 4.2557e-04 - activation_49_loss: 3.9335e-04 - activation_48_loss: 3.2226e-05 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6815 - val_activation_49_loss: 0.2832 - val_activation_48_loss: 0.3983 - val_activation_49_categorical_accuracy: 0.9650 - val_activation_48_categorical_accuracy: 0.9606 - lr: 1.4515e-04\n",
            "Epoch 224/251\n",
            "114/114 [==============================] - 22s 196ms/step - loss: 0.0013 - activation_49_loss: 0.0012 - activation_48_loss: 9.9515e-05 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7203 - val_activation_49_loss: 0.2666 - val_activation_48_loss: 0.4537 - val_activation_49_categorical_accuracy: 0.9637 - val_activation_48_categorical_accuracy: 0.9606 - lr: 1.4370e-04\n",
            "Epoch 225/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 8.0067e-04 - activation_49_loss: 6.7201e-04 - activation_48_loss: 1.2866e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7279 - val_activation_49_loss: 0.2832 - val_activation_48_loss: 0.4447 - val_activation_49_categorical_accuracy: 0.9623 - val_activation_48_categorical_accuracy: 0.9610 - lr: 1.4227e-04\n",
            "Epoch 226/251\n",
            "114/114 [==============================] - 19s 168ms/step - loss: 7.3674e-04 - activation_49_loss: 7.1508e-04 - activation_48_loss: 2.1665e-05 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7193 - val_activation_49_loss: 0.2748 - val_activation_48_loss: 0.4445 - val_activation_49_categorical_accuracy: 0.9637 - val_activation_48_categorical_accuracy: 0.9600 - lr: 1.4086e-04\n",
            "Epoch 227/251\n",
            "114/114 [==============================] - 20s 177ms/step - loss: 0.0011 - activation_49_loss: 0.0010 - activation_48_loss: 6.5906e-05 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.8661 - val_activation_49_loss: 0.3740 - val_activation_48_loss: 0.4921 - val_activation_49_categorical_accuracy: 0.9606 - val_activation_48_categorical_accuracy: 0.9583 - lr: 1.3946e-04\n",
            "Epoch 228/251\n",
            "114/114 [==============================] - 22s 195ms/step - loss: 5.3742e-04 - activation_49_loss: 5.2900e-04 - activation_48_loss: 8.4112e-06 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.9159 - val_activation_49_loss: 0.3923 - val_activation_48_loss: 0.5236 - val_activation_49_categorical_accuracy: 0.9603 - val_activation_48_categorical_accuracy: 0.9572 - lr: 1.3807e-04\n",
            "Epoch 229/251\n",
            "114/114 [==============================] - 19s 169ms/step - loss: 0.0026 - activation_49_loss: 0.0017 - activation_48_loss: 8.7365e-04 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.7561 - val_activation_49_loss: 0.3506 - val_activation_48_loss: 0.4055 - val_activation_49_categorical_accuracy: 0.9650 - val_activation_48_categorical_accuracy: 0.9610 - lr: 1.3670e-04\n",
            "Epoch 230/251\n",
            "114/114 [==============================] - 20s 179ms/step - loss: 0.0020 - activation_49_loss: 0.0016 - activation_48_loss: 3.6026e-04 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.7962 - val_activation_49_loss: 0.3656 - val_activation_48_loss: 0.4306 - val_activation_49_categorical_accuracy: 0.9627 - val_activation_48_categorical_accuracy: 0.9606 - lr: 1.3534e-04\n",
            "Epoch 231/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 8.1939e-04 - activation_49_loss: 7.2464e-04 - activation_48_loss: 9.4753e-05 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.8123 - val_activation_49_loss: 0.3540 - val_activation_48_loss: 0.4583 - val_activation_49_categorical_accuracy: 0.9620 - val_activation_48_categorical_accuracy: 0.9610 - lr: 1.3399e-04\n",
            "Epoch 232/251\n",
            "114/114 [==============================] - 19s 168ms/step - loss: 5.6872e-04 - activation_49_loss: 5.2719e-04 - activation_48_loss: 4.1533e-05 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.8826 - val_activation_49_loss: 0.3852 - val_activation_48_loss: 0.4974 - val_activation_49_categorical_accuracy: 0.9576 - val_activation_48_categorical_accuracy: 0.9613 - lr: 1.3266e-04\n",
            "Epoch 233/251\n",
            "114/114 [==============================] - 21s 182ms/step - loss: 0.0013 - activation_49_loss: 0.0013 - activation_48_loss: 4.0536e-05 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6711 - val_activation_49_loss: 0.2822 - val_activation_48_loss: 0.3889 - val_activation_49_categorical_accuracy: 0.9657 - val_activation_48_categorical_accuracy: 0.9630 - lr: 1.3134e-04\n",
            "Epoch 234/251\n",
            "114/114 [==============================] - 21s 182ms/step - loss: 3.9277e-04 - activation_49_loss: 3.6233e-04 - activation_48_loss: 3.0443e-05 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6794 - val_activation_49_loss: 0.2981 - val_activation_48_loss: 0.3813 - val_activation_49_categorical_accuracy: 0.9647 - val_activation_48_categorical_accuracy: 0.9606 - lr: 1.3003e-04\n",
            "Epoch 235/251\n",
            "114/114 [==============================] - 20s 174ms/step - loss: 0.0012 - activation_49_loss: 9.9876e-04 - activation_48_loss: 2.4191e-04 - activation_49_categorical_accuracy: 0.9995 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.6980 - val_activation_49_loss: 0.3115 - val_activation_48_loss: 0.3865 - val_activation_49_categorical_accuracy: 0.9647 - val_activation_48_categorical_accuracy: 0.9613 - lr: 1.2874e-04\n",
            "Epoch 236/251\n",
            "114/114 [==============================] - 20s 174ms/step - loss: 4.6212e-04 - activation_49_loss: 4.4176e-04 - activation_48_loss: 2.0369e-05 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7692 - val_activation_49_loss: 0.3367 - val_activation_48_loss: 0.4325 - val_activation_49_categorical_accuracy: 0.9640 - val_activation_48_categorical_accuracy: 0.9606 - lr: 1.2745e-04\n",
            "Epoch 237/251\n",
            "114/114 [==============================] - 21s 183ms/step - loss: 5.7779e-04 - activation_49_loss: 4.8503e-04 - activation_48_loss: 9.2759e-05 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.8372 - val_activation_49_loss: 0.3582 - val_activation_48_loss: 0.4790 - val_activation_49_categorical_accuracy: 0.9617 - val_activation_48_categorical_accuracy: 0.9593 - lr: 1.2619e-04\n",
            "Epoch 238/251\n",
            "114/114 [==============================] - 20s 179ms/step - loss: 0.0018 - activation_49_loss: 0.0017 - activation_48_loss: 1.1457e-04 - activation_49_categorical_accuracy: 0.9995 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.8672 - val_activation_49_loss: 0.3851 - val_activation_48_loss: 0.4821 - val_activation_49_categorical_accuracy: 0.9637 - val_activation_48_categorical_accuracy: 0.9620 - lr: 1.2493e-04\n",
            "Epoch 239/251\n",
            "114/114 [==============================] - 19s 171ms/step - loss: 0.0025 - activation_49_loss: 0.0016 - activation_48_loss: 9.0356e-04 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.8975 - val_activation_49_loss: 0.3610 - val_activation_48_loss: 0.5365 - val_activation_49_categorical_accuracy: 0.9596 - val_activation_48_categorical_accuracy: 0.9559 - lr: 1.2369e-04\n",
            "Epoch 240/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 0.0015 - activation_49_loss: 0.0012 - activation_48_loss: 2.8592e-04 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.8307 - val_activation_49_loss: 0.3359 - val_activation_48_loss: 0.4948 - val_activation_49_categorical_accuracy: 0.9637 - val_activation_48_categorical_accuracy: 0.9586 - lr: 1.2246e-04\n",
            "Epoch 241/251\n",
            "114/114 [==============================] - 20s 175ms/step - loss: 0.0012 - activation_49_loss: 8.6323e-04 - activation_48_loss: 3.6073e-04 - activation_49_categorical_accuracy: 0.9996 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.7748 - val_activation_49_loss: 0.3327 - val_activation_48_loss: 0.4421 - val_activation_49_categorical_accuracy: 0.9634 - val_activation_48_categorical_accuracy: 0.9603 - lr: 1.2124e-04\n",
            "Epoch 242/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 0.0036 - activation_49_loss: 0.0023 - activation_48_loss: 0.0013 - activation_49_categorical_accuracy: 0.9992 - activation_48_categorical_accuracy: 0.9997 - val_loss: 0.7916 - val_activation_49_loss: 0.3354 - val_activation_48_loss: 0.4561 - val_activation_49_categorical_accuracy: 0.9606 - val_activation_48_categorical_accuracy: 0.9600 - lr: 1.2003e-04\n",
            "Epoch 243/251\n",
            "114/114 [==============================] - 19s 168ms/step - loss: 6.8418e-04 - activation_49_loss: 6.5271e-04 - activation_48_loss: 3.1471e-05 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6654 - val_activation_49_loss: 0.2906 - val_activation_48_loss: 0.3749 - val_activation_49_categorical_accuracy: 0.9640 - val_activation_48_categorical_accuracy: 0.9606 - lr: 1.1884e-04\n",
            "Epoch 244/251\n",
            "114/114 [==============================] - 20s 176ms/step - loss: 9.2085e-04 - activation_49_loss: 8.5090e-04 - activation_48_loss: 6.9947e-05 - activation_49_categorical_accuracy: 0.9995 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7324 - val_activation_49_loss: 0.3102 - val_activation_48_loss: 0.4222 - val_activation_49_categorical_accuracy: 0.9634 - val_activation_48_categorical_accuracy: 0.9620 - lr: 1.1765e-04\n",
            "Epoch 245/251\n",
            "114/114 [==============================] - 22s 190ms/step - loss: 6.0091e-04 - activation_49_loss: 4.5668e-04 - activation_48_loss: 1.4423e-04 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 0.9999 - val_loss: 0.7224 - val_activation_49_loss: 0.3120 - val_activation_48_loss: 0.4104 - val_activation_49_categorical_accuracy: 0.9627 - val_activation_48_categorical_accuracy: 0.9654 - lr: 1.1648e-04\n",
            "Epoch 246/251\n",
            "114/114 [==============================] - 21s 187ms/step - loss: 0.0020 - activation_49_loss: 0.0019 - activation_48_loss: 1.1953e-04 - activation_49_categorical_accuracy: 0.9993 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.6664 - val_activation_49_loss: 0.2699 - val_activation_48_loss: 0.3965 - val_activation_49_categorical_accuracy: 0.9657 - val_activation_48_categorical_accuracy: 0.9674 - lr: 1.1533e-04\n",
            "Epoch 247/251\n",
            "114/114 [==============================] - 23s 198ms/step - loss: 5.3826e-04 - activation_49_loss: 4.8188e-04 - activation_48_loss: 5.6387e-05 - activation_49_categorical_accuracy: 0.9997 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7763 - val_activation_49_loss: 0.3054 - val_activation_48_loss: 0.4709 - val_activation_49_categorical_accuracy: 0.9620 - val_activation_48_categorical_accuracy: 0.9589 - lr: 1.1418e-04\n",
            "Epoch 248/251\n",
            "114/114 [==============================] - 19s 170ms/step - loss: 9.1110e-04 - activation_49_loss: 8.0808e-04 - activation_48_loss: 1.0302e-04 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7445 - val_activation_49_loss: 0.3044 - val_activation_48_loss: 0.4401 - val_activation_49_categorical_accuracy: 0.9617 - val_activation_48_categorical_accuracy: 0.9603 - lr: 1.1304e-04\n",
            "Epoch 249/251\n",
            "114/114 [==============================] - 19s 167ms/step - loss: 1.7218e-04 - activation_49_loss: 1.4562e-04 - activation_48_loss: 2.6561e-05 - activation_49_categorical_accuracy: 1.0000 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7412 - val_activation_49_loss: 0.3021 - val_activation_48_loss: 0.4392 - val_activation_49_categorical_accuracy: 0.9623 - val_activation_48_categorical_accuracy: 0.9613 - lr: 1.1192e-04\n",
            "Epoch 250/251\n",
            "114/114 [==============================] - 20s 178ms/step - loss: 1.2831e-04 - activation_49_loss: 1.1730e-04 - activation_48_loss: 1.1004e-05 - activation_49_categorical_accuracy: 1.0000 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7324 - val_activation_49_loss: 0.2977 - val_activation_48_loss: 0.4347 - val_activation_49_categorical_accuracy: 0.9627 - val_activation_48_categorical_accuracy: 0.9620 - lr: 1.1080e-04\n",
            "Epoch 251/251\n",
            "114/114 [==============================] - 20s 172ms/step - loss: 4.4471e-04 - activation_49_loss: 4.3302e-04 - activation_48_loss: 1.1688e-05 - activation_49_categorical_accuracy: 0.9999 - activation_48_categorical_accuracy: 1.0000 - val_loss: 0.7962 - val_activation_49_loss: 0.3115 - val_activation_48_loss: 0.4847 - val_activation_49_categorical_accuracy: 0.9610 - val_activation_48_categorical_accuracy: 0.9606 - lr: 1.0970e-04\n"
          ]
        }
      ],
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "num_classes = y_train.shape[-1]\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "print(X_train.shape,y_train.shape)\n",
        "\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 30:\n",
        "    return lr\n",
        "  else:\n",
        "     return lr * tf.math.exp(-0.01)\n",
        "\n",
        "dg = DataGenerator(X_train,y_train,batch_size=batch_size,input_shape=X_train.shape[1:])\n",
        "model,pred_model = buildAE(X_train.shape[1:],y_train.shape[-1],learning_rate)\n",
        "# log = MyLogger(n=1, validation_data=(x_test,y_test), AE=model)\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "print('Params', model.count_params())\n",
        "\n",
        "history = model.fit(dg, epochs=251, verbose=1,callbacks = [lr_scheduler], validation_data=(x_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "AgsDULrES_9K",
        "outputId": "96840dbf-054a-4b90-d466-cf436034f2b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "93/93 [==============================] - 4s 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97       496\n",
            "           1       0.99      0.97      0.98       471\n",
            "           2       0.96      1.00      0.98       420\n",
            "           3       0.91      0.94      0.92       491\n",
            "           4       0.97      0.95      0.96       532\n",
            "           5       0.98      1.00      0.99       537\n",
            "\n",
            "    accuracy                           0.97      2947\n",
            "   macro avg       0.97      0.97      0.97      2947\n",
            "weighted avg       0.97      0.97      0.97      2947\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "model = tf.keras.models.load_model('/content/Adaptive_multifilter_CNN_Bi-LSTM.h5')\n",
        "outs = model.predict(x_test)\n",
        "\n",
        "\n",
        "class_names= np.unique(np.argmax(y_test,axis=1))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(np.argmax(y_test,axis=1), np.argmax(outs[0],axis=1))\n",
        "\n",
        "print(classification_report(np.argmax(y_test,axis=1), np.argmax(outs[0],axis=1)))\n",
        "\n",
        "\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "# set the axis labels and title of the plot\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "plt.savefig('confusion_matrix.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDTmweK7wdXA"
      },
      "source": [
        "##Multikernel_ResNetV2_BI-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI5RdIbZlD1n",
        "outputId": "279c2495-1a67-4dfd-c9ca-91218834c5f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params 111660\n",
            "Model: \"model_29\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_18 (InputLayer)          [(None, 128, 9)]     0           []                               \n",
            "                                                                                                  \n",
            " zero_padding1d_16 (ZeroPadding  (None, 130, 9)      0           ['input_18[0][0]']               \n",
            " 1D)                                                                                              \n",
            "                                                                                                  \n",
            " conv1d_387 (Conv1D)            (None, 128, 16)      448         ['zero_padding1d_16[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_382 (Batch  (None, 128, 16)     64          ['conv1d_387[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_381 (Activation)    (None, 128, 16)      0           ['batch_normalization_382[0][0]']\n",
            "                                                                                                  \n",
            " max_pooling1d_27 (MaxPooling1D  (None, 63, 16)      0           ['activation_381[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_388 (Conv1D)            (None, 63, 16)       784         ['max_pooling1d_27[0][0]']       \n",
            "                                                                                                  \n",
            " conv1d_389 (Conv1D)            (None, 63, 16)       1296        ['max_pooling1d_27[0][0]']       \n",
            "                                                                                                  \n",
            " conv1d_390 (Conv1D)            (None, 63, 16)       1808        ['max_pooling1d_27[0][0]']       \n",
            "                                                                                                  \n",
            " conv1d_391 (Conv1D)            (None, 63, 16)       2320        ['max_pooling1d_27[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_383 (Batch  (None, 63, 16)      64          ['conv1d_388[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_384 (Batch  (None, 63, 16)      64          ['conv1d_389[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_385 (Batch  (None, 63, 16)      64          ['conv1d_390[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_386 (Batch  (None, 63, 16)      64          ['conv1d_391[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_382 (Activation)    (None, 63, 16)       0           ['batch_normalization_383[0][0]']\n",
            "                                                                                                  \n",
            " activation_383 (Activation)    (None, 63, 16)       0           ['batch_normalization_384[0][0]']\n",
            "                                                                                                  \n",
            " activation_384 (Activation)    (None, 63, 16)       0           ['batch_normalization_385[0][0]']\n",
            "                                                                                                  \n",
            " activation_385 (Activation)    (None, 63, 16)       0           ['batch_normalization_386[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_30 (Concatenate)   (None, 63, 64)       0           ['activation_382[0][0]',         \n",
            "                                                                  'activation_383[0][0]',         \n",
            "                                                                  'activation_384[0][0]',         \n",
            "                                                                  'activation_385[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_392 (Conv1D)            (None, 63, 16)       1040        ['concatenate_30[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_387 (Batch  (None, 63, 16)      64          ['conv1d_392[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_386 (Activation)    (None, 63, 16)       0           ['batch_normalization_387[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_393 (Conv1D)            (None, 31, 16)       784         ['activation_386[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_388 (Batch  (None, 31, 16)      64          ['conv1d_393[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_387 (Activation)    (None, 31, 16)       0           ['batch_normalization_388[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_395 (Conv1D)            (None, 63, 32)       2080        ['concatenate_30[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_394 (Conv1D)            (None, 31, 32)       1568        ['activation_387[0][0]']         \n",
            "                                                                                                  \n",
            " max_pooling1d_28 (MaxPooling1D  (None, 31, 32)      0           ['conv1d_395[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " add_83 (Add)                   (None, 31, 32)       0           ['conv1d_394[0][0]',             \n",
            "                                                                  'max_pooling1d_28[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_389 (Batch  (None, 31, 32)      128         ['add_83[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_388 (Activation)    (None, 31, 32)       0           ['batch_normalization_389[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_396 (Conv1D)            (None, 31, 16)       528         ['activation_388[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_390 (Batch  (None, 31, 16)      64          ['conv1d_396[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_389 (Activation)    (None, 31, 16)       0           ['batch_normalization_390[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_397 (Conv1D)            (None, 31, 16)       784         ['activation_389[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_391 (Batch  (None, 31, 16)      64          ['conv1d_397[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_390 (Activation)    (None, 31, 16)       0           ['batch_normalization_391[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_398 (Conv1D)            (None, 31, 32)       544         ['activation_390[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_392 (Batch  (None, 31, 32)      128         ['conv1d_398[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_391 (Activation)    (None, 31, 32)       0           ['batch_normalization_392[0][0]']\n",
            "                                                                                                  \n",
            " add_84 (Add)                   (None, 31, 32)       0           ['activation_391[0][0]',         \n",
            "                                                                  'activation_388[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_399 (Conv1D)            (None, 31, 32)       3104        ['add_84[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_400 (Conv1D)            (None, 31, 32)       5152        ['add_84[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_401 (Conv1D)            (None, 31, 32)       7200        ['add_84[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_402 (Conv1D)            (None, 31, 32)       9248        ['add_84[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_393 (Batch  (None, 31, 32)      128         ['conv1d_399[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_394 (Batch  (None, 31, 32)      128         ['conv1d_400[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_395 (Batch  (None, 31, 32)      128         ['conv1d_401[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_396 (Batch  (None, 31, 32)      128         ['conv1d_402[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_392 (Activation)    (None, 31, 32)       0           ['batch_normalization_393[0][0]']\n",
            "                                                                                                  \n",
            " activation_393 (Activation)    (None, 31, 32)       0           ['batch_normalization_394[0][0]']\n",
            "                                                                                                  \n",
            " activation_394 (Activation)    (None, 31, 32)       0           ['batch_normalization_395[0][0]']\n",
            "                                                                                                  \n",
            " activation_395 (Activation)    (None, 31, 32)       0           ['batch_normalization_396[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_31 (Concatenate)   (None, 31, 128)      0           ['activation_392[0][0]',         \n",
            "                                                                  'activation_393[0][0]',         \n",
            "                                                                  'activation_394[0][0]',         \n",
            "                                                                  'activation_395[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_403 (Conv1D)            (None, 31, 32)       4128        ['concatenate_31[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_397 (Batch  (None, 31, 32)      128         ['conv1d_403[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_396 (Activation)    (None, 31, 32)       0           ['batch_normalization_397[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_404 (Conv1D)            (None, 15, 32)       3104        ['activation_396[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_398 (Batch  (None, 15, 32)      128         ['conv1d_404[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_397 (Activation)    (None, 15, 32)       0           ['batch_normalization_398[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_406 (Conv1D)            (None, 31, 64)       8256        ['concatenate_31[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_405 (Conv1D)            (None, 15, 64)       6208        ['activation_397[0][0]']         \n",
            "                                                                                                  \n",
            " max_pooling1d_29 (MaxPooling1D  (None, 15, 64)      0           ['conv1d_406[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " add_85 (Add)                   (None, 15, 64)       0           ['conv1d_405[0][0]',             \n",
            "                                                                  'max_pooling1d_29[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_399 (Batch  (None, 15, 64)      256         ['add_85[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_398 (Activation)    (None, 15, 64)       0           ['batch_normalization_399[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_407 (Conv1D)            (None, 15, 32)       2080        ['activation_398[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_400 (Batch  (None, 15, 32)      128         ['conv1d_407[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_399 (Activation)    (None, 15, 32)       0           ['batch_normalization_400[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_408 (Conv1D)            (None, 15, 32)       3104        ['activation_399[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_401 (Batch  (None, 15, 32)      128         ['conv1d_408[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_400 (Activation)    (None, 15, 32)       0           ['batch_normalization_401[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_409 (Conv1D)            (None, 15, 64)       2112        ['activation_400[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_402 (Batch  (None, 15, 64)      256         ['conv1d_409[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_401 (Activation)    (None, 15, 64)       0           ['batch_normalization_402[0][0]']\n",
            "                                                                                                  \n",
            " add_86 (Add)                   (None, 15, 64)       0           ['activation_401[0][0]',         \n",
            "                                                                  'activation_398[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_410 (Conv1D)            (None, 15, 32)       2080        ['add_86[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_403 (Batch  (None, 15, 32)      128         ['conv1d_410[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_402 (Activation)    (None, 15, 32)       0           ['batch_normalization_403[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_411 (Conv1D)            (None, 15, 32)       3104        ['activation_402[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_404 (Batch  (None, 15, 32)      128         ['conv1d_411[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_403 (Activation)    (None, 15, 32)       0           ['batch_normalization_404[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_412 (Conv1D)            (None, 15, 64)       2112        ['activation_403[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_405 (Batch  (None, 15, 64)      256         ['conv1d_412[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_404 (Activation)    (None, 15, 64)       0           ['batch_normalization_405[0][0]']\n",
            "                                                                                                  \n",
            " add_87 (Add)                   (None, 15, 64)       0           ['activation_404[0][0]',         \n",
            "                                                                  'add_86[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_406 (Batch  (None, 15, 64)      256         ['add_87[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_405 (Activation)    (None, 15, 64)       0           ['batch_normalization_406[0][0]']\n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)           (None, 15, 64)       0           ['activation_405[0][0]']         \n",
            "                                                                                                  \n",
            " bidirectional_28 (Bidirectiona  (None, 15, 32)      10368       ['dropout_28[0][0]']             \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_407 (Batch  (None, 15, 32)      128         ['bidirectional_28[0][0]']       \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)           (None, 15, 32)       0           ['batch_normalization_407[0][0]']\n",
            "                                                                                                  \n",
            " bidirectional_29 (Bidirectiona  (None, 64)          16640       ['dropout_29[0][0]']             \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_408 (Batch  (None, 64)          256         ['bidirectional_29[0][0]']       \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " flatten_14 (Flatten)           (None, 960)          0           ['activation_405[0][0]']         \n",
            "                                                                                                  \n",
            " dense_57 (Dense)               (None, 6)            390         ['batch_normalization_408[0][0]']\n",
            "                                                                                                  \n",
            " dense_56 (Dense)               (None, 6)            5766        ['flatten_14[0][0]']             \n",
            "                                                                                                  \n",
            " activation_407 (Activation)    (None, 6)            0           ['dense_57[0][0]']               \n",
            "                                                                                                  \n",
            " activation_406 (Activation)    (None, 6)            0           ['dense_56[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 111,660\n",
            "Trainable params: 109,900\n",
            "Non-trainable params: 1,760\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def res_identity(x, filters): \n",
        "  #renet block where dimension doesnot change.\n",
        "  #The skip connection is just simple identity conncection\n",
        "  #we will have 3 blocks and then input will be added\n",
        "\n",
        "  x_skip = x # this will be used for addition with the residual block \n",
        "  f1, f2 = filters\n",
        "\n",
        "  #first block \n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=1, strides=1, padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  \n",
        "  \n",
        "\n",
        "  #second block # bottleneck (but size kept same with padding)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same', )(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  \n",
        "  \n",
        "\n",
        "  # third block activation used after adding the input\n",
        "  x = tf.keras.layers.Conv1D(f2, kernel_size=1, strides=1, padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  \n",
        "\n",
        "  # add the input \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def conv_skip(x, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=1, strides=1, padding='same')(x)\n",
        "  # when s = 2 then it is like downsizing the feature map\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  # x = tf.keras.layers.ZeroPadding1D(padding=(1,1))(x)\n",
        "  \n",
        "\n",
        "  # second block\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=2, padding='valid')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  #third block\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "  x = tf.keras.layers.Conv1D(f2, kernel_size=3, strides=1, padding='same')(x)\n",
        "  # x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "\n",
        "  # shortcut \n",
        "  x_skip = tf.keras.layers.Conv1D(f2, kernel_size=1, strides=1, padding='same')(x_skip)\n",
        "  x_skip = tf.keras.layers.MaxPool1D(pool_size=3,strides=2)(x_skip)\n",
        "  # x_skip = tf.keras.layers.BatchNormalization()(x_skip)\n",
        "\n",
        "  # add \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def multi_fiter_conv(x, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x1 = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "  x1 = tf.keras.layers.Activation(tf.keras.activations.relu)(x1)\n",
        "  \n",
        "\n",
        "  # second block\n",
        "  x2 = tf.keras.layers.Conv1D(f1, kernel_size=5, strides=1, padding='same')(x)\n",
        "  x2 = tf.keras.layers.BatchNormalization()(x2)\n",
        "  x2 = tf.keras.layers.Activation(tf.keras.activations.relu)(x2)\n",
        "\n",
        "  #third block\n",
        "  x3 = tf.keras.layers.Conv1D(f1, kernel_size=7, strides=1, padding='same')(x)\n",
        "  x3 = tf.keras.layers.BatchNormalization()(x3)\n",
        "  x3 = tf.keras.layers.Activation(tf.keras.activations.relu)(x3)\n",
        "\n",
        "\n",
        "  # forth block\n",
        "  x4 = tf.keras.layers.Conv1D(f1, kernel_size=9, strides=1, padding='same')(x)\n",
        "  x4 = tf.keras.layers.BatchNormalization()(x4)\n",
        "  x4 = tf.keras.layers.Activation(tf.keras.activations.relu)(x4)\n",
        "\n",
        "  # concatenate \n",
        "  x = tf.keras.layers.Concatenate()([x1,x2,x3,x4])\n",
        "  # x = tf.keras.layers.Conv1D(f2,kernel_size=3,strides=1,padding='same')(x)\n",
        "  # x = tf.keras.layers.BatchNormalization()(x)\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  # #add\n",
        "  # x = tf.keras.layers.Add()([x,x_skip])\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def buildAE(input_shape,classes,learning_rate):\n",
        "\n",
        "    input = tf.keras.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.ZeroPadding1D(padding=(1,1))(input)\n",
        "    x = tf.keras.layers.Conv1D(16, kernel_size=3, strides=1,padding='valid',)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)(x)\n",
        "\n",
        "    x = multi_fiter_conv(x,filters=(16,16))\n",
        "\n",
        "    x = conv_skip(x ,filters=(16,32))\n",
        "    # print(x.shape)\n",
        "    x = res_identity(x,filters=(16,32))\n",
        "    x = multi_fiter_conv(x,filters=(32,32))\n",
        "    x = conv_skip(x ,filters=(32,64))\n",
        "    x = res_identity(x,filters=(32,64))\n",
        "    x = res_identity(x,filters=(32,64))\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "    # x = maxpool_skip(x,filters=(16,16))\n",
        "\n",
        "    output_opt = tf.keras.layers.Flatten()(x)\n",
        "    # output_opt = tf.keras.layers.Dense(units=64)(output_opt)\n",
        "    output_opt = tf.keras.layers.Dense(units=classes)(output_opt)\n",
        "    output_opt = tf.keras.layers.Activation('softmax')(output_opt)\n",
        "\n",
        "\n",
        "\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=16,activation='tanh',return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=32,activation='tanh'))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = tf.keras.layers.Dense(units=64)(x)\n",
        "    x = tf.keras.layers.Dense(units=classes)(x)\n",
        "    output = tf.keras.layers.Activation('softmax')(x)\n",
        "\n",
        "\n",
        "    # model = tf.keras.Model(input,output)\n",
        "\n",
        "    pred_model = tf.keras.Model(input,output)\n",
        "    model = tf.keras.Model(input,[output,output_opt])\n",
        "\n",
        "\n",
        "    \n",
        "    print('Params', model.count_params())\n",
        "    model.compile(loss = [tf.keras.losses.CategoricalCrossentropy(),\n",
        "                          tf.keras.losses.CategoricalCrossentropy()],\n",
        "                  metrics=[tf.keras.metrics.CategoricalAccuracy(),],\n",
        "                   optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
        "\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "model = buildAE((128,9),6,0.0005)\n",
        "# tf.keras.utils.plot_model(model,show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRjtBBde3Wfo",
        "outputId": "e901cb96-adeb-4dc0-daaf-54ee3f578442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7352, 128, 9) (7352, 6)\n",
            "Params 111660\n",
            "Model: \"model_31\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_19 (InputLayer)          [(None, 128, 9)]     0           []                               \n",
            "                                                                                                  \n",
            " zero_padding1d_17 (ZeroPadding  (None, 130, 9)      0           ['input_19[0][0]']               \n",
            " 1D)                                                                                              \n",
            "                                                                                                  \n",
            " conv1d_413 (Conv1D)            (None, 128, 16)      448         ['zero_padding1d_17[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_409 (Batch  (None, 128, 16)     64          ['conv1d_413[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_408 (Activation)    (None, 128, 16)      0           ['batch_normalization_409[0][0]']\n",
            "                                                                                                  \n",
            " max_pooling1d_30 (MaxPooling1D  (None, 63, 16)      0           ['activation_408[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_414 (Conv1D)            (None, 63, 16)       784         ['max_pooling1d_30[0][0]']       \n",
            "                                                                                                  \n",
            " conv1d_415 (Conv1D)            (None, 63, 16)       1296        ['max_pooling1d_30[0][0]']       \n",
            "                                                                                                  \n",
            " conv1d_416 (Conv1D)            (None, 63, 16)       1808        ['max_pooling1d_30[0][0]']       \n",
            "                                                                                                  \n",
            " conv1d_417 (Conv1D)            (None, 63, 16)       2320        ['max_pooling1d_30[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_410 (Batch  (None, 63, 16)      64          ['conv1d_414[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_411 (Batch  (None, 63, 16)      64          ['conv1d_415[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_412 (Batch  (None, 63, 16)      64          ['conv1d_416[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_413 (Batch  (None, 63, 16)      64          ['conv1d_417[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_409 (Activation)    (None, 63, 16)       0           ['batch_normalization_410[0][0]']\n",
            "                                                                                                  \n",
            " activation_410 (Activation)    (None, 63, 16)       0           ['batch_normalization_411[0][0]']\n",
            "                                                                                                  \n",
            " activation_411 (Activation)    (None, 63, 16)       0           ['batch_normalization_412[0][0]']\n",
            "                                                                                                  \n",
            " activation_412 (Activation)    (None, 63, 16)       0           ['batch_normalization_413[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_32 (Concatenate)   (None, 63, 64)       0           ['activation_409[0][0]',         \n",
            "                                                                  'activation_410[0][0]',         \n",
            "                                                                  'activation_411[0][0]',         \n",
            "                                                                  'activation_412[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_418 (Conv1D)            (None, 63, 16)       1040        ['concatenate_32[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_414 (Batch  (None, 63, 16)      64          ['conv1d_418[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_413 (Activation)    (None, 63, 16)       0           ['batch_normalization_414[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_419 (Conv1D)            (None, 31, 16)       784         ['activation_413[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_415 (Batch  (None, 31, 16)      64          ['conv1d_419[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_414 (Activation)    (None, 31, 16)       0           ['batch_normalization_415[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_421 (Conv1D)            (None, 63, 32)       2080        ['concatenate_32[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_420 (Conv1D)            (None, 31, 32)       1568        ['activation_414[0][0]']         \n",
            "                                                                                                  \n",
            " max_pooling1d_31 (MaxPooling1D  (None, 31, 32)      0           ['conv1d_421[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " add_88 (Add)                   (None, 31, 32)       0           ['conv1d_420[0][0]',             \n",
            "                                                                  'max_pooling1d_31[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_416 (Batch  (None, 31, 32)      128         ['add_88[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_415 (Activation)    (None, 31, 32)       0           ['batch_normalization_416[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_422 (Conv1D)            (None, 31, 16)       528         ['activation_415[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_417 (Batch  (None, 31, 16)      64          ['conv1d_422[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_416 (Activation)    (None, 31, 16)       0           ['batch_normalization_417[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_423 (Conv1D)            (None, 31, 16)       784         ['activation_416[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_418 (Batch  (None, 31, 16)      64          ['conv1d_423[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_417 (Activation)    (None, 31, 16)       0           ['batch_normalization_418[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_424 (Conv1D)            (None, 31, 32)       544         ['activation_417[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_419 (Batch  (None, 31, 32)      128         ['conv1d_424[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_418 (Activation)    (None, 31, 32)       0           ['batch_normalization_419[0][0]']\n",
            "                                                                                                  \n",
            " add_89 (Add)                   (None, 31, 32)       0           ['activation_418[0][0]',         \n",
            "                                                                  'activation_415[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_425 (Conv1D)            (None, 31, 32)       3104        ['add_89[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_426 (Conv1D)            (None, 31, 32)       5152        ['add_89[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_427 (Conv1D)            (None, 31, 32)       7200        ['add_89[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_428 (Conv1D)            (None, 31, 32)       9248        ['add_89[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_420 (Batch  (None, 31, 32)      128         ['conv1d_425[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_421 (Batch  (None, 31, 32)      128         ['conv1d_426[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_422 (Batch  (None, 31, 32)      128         ['conv1d_427[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_423 (Batch  (None, 31, 32)      128         ['conv1d_428[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_419 (Activation)    (None, 31, 32)       0           ['batch_normalization_420[0][0]']\n",
            "                                                                                                  \n",
            " activation_420 (Activation)    (None, 31, 32)       0           ['batch_normalization_421[0][0]']\n",
            "                                                                                                  \n",
            " activation_421 (Activation)    (None, 31, 32)       0           ['batch_normalization_422[0][0]']\n",
            "                                                                                                  \n",
            " activation_422 (Activation)    (None, 31, 32)       0           ['batch_normalization_423[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_33 (Concatenate)   (None, 31, 128)      0           ['activation_419[0][0]',         \n",
            "                                                                  'activation_420[0][0]',         \n",
            "                                                                  'activation_421[0][0]',         \n",
            "                                                                  'activation_422[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_429 (Conv1D)            (None, 31, 32)       4128        ['concatenate_33[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_424 (Batch  (None, 31, 32)      128         ['conv1d_429[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_423 (Activation)    (None, 31, 32)       0           ['batch_normalization_424[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_430 (Conv1D)            (None, 15, 32)       3104        ['activation_423[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_425 (Batch  (None, 15, 32)      128         ['conv1d_430[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_424 (Activation)    (None, 15, 32)       0           ['batch_normalization_425[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_432 (Conv1D)            (None, 31, 64)       8256        ['concatenate_33[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_431 (Conv1D)            (None, 15, 64)       6208        ['activation_424[0][0]']         \n",
            "                                                                                                  \n",
            " max_pooling1d_32 (MaxPooling1D  (None, 15, 64)      0           ['conv1d_432[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " add_90 (Add)                   (None, 15, 64)       0           ['conv1d_431[0][0]',             \n",
            "                                                                  'max_pooling1d_32[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_426 (Batch  (None, 15, 64)      256         ['add_90[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_425 (Activation)    (None, 15, 64)       0           ['batch_normalization_426[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_433 (Conv1D)            (None, 15, 32)       2080        ['activation_425[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_427 (Batch  (None, 15, 32)      128         ['conv1d_433[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_426 (Activation)    (None, 15, 32)       0           ['batch_normalization_427[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_434 (Conv1D)            (None, 15, 32)       3104        ['activation_426[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_428 (Batch  (None, 15, 32)      128         ['conv1d_434[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_427 (Activation)    (None, 15, 32)       0           ['batch_normalization_428[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_435 (Conv1D)            (None, 15, 64)       2112        ['activation_427[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_429 (Batch  (None, 15, 64)      256         ['conv1d_435[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_428 (Activation)    (None, 15, 64)       0           ['batch_normalization_429[0][0]']\n",
            "                                                                                                  \n",
            " add_91 (Add)                   (None, 15, 64)       0           ['activation_428[0][0]',         \n",
            "                                                                  'activation_425[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_436 (Conv1D)            (None, 15, 32)       2080        ['add_91[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_430 (Batch  (None, 15, 32)      128         ['conv1d_436[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_429 (Activation)    (None, 15, 32)       0           ['batch_normalization_430[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_437 (Conv1D)            (None, 15, 32)       3104        ['activation_429[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_431 (Batch  (None, 15, 32)      128         ['conv1d_437[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_430 (Activation)    (None, 15, 32)       0           ['batch_normalization_431[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_438 (Conv1D)            (None, 15, 64)       2112        ['activation_430[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_432 (Batch  (None, 15, 64)      256         ['conv1d_438[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_431 (Activation)    (None, 15, 64)       0           ['batch_normalization_432[0][0]']\n",
            "                                                                                                  \n",
            " add_92 (Add)                   (None, 15, 64)       0           ['activation_431[0][0]',         \n",
            "                                                                  'add_91[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_433 (Batch  (None, 15, 64)      256         ['add_92[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_432 (Activation)    (None, 15, 64)       0           ['batch_normalization_433[0][0]']\n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)           (None, 15, 64)       0           ['activation_432[0][0]']         \n",
            "                                                                                                  \n",
            " bidirectional_30 (Bidirectiona  (None, 15, 32)      10368       ['dropout_30[0][0]']             \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_434 (Batch  (None, 15, 32)      128         ['bidirectional_30[0][0]']       \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_31 (Dropout)           (None, 15, 32)       0           ['batch_normalization_434[0][0]']\n",
            "                                                                                                  \n",
            " bidirectional_31 (Bidirectiona  (None, 64)          16640       ['dropout_31[0][0]']             \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_435 (Batch  (None, 64)          256         ['bidirectional_31[0][0]']       \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " flatten_15 (Flatten)           (None, 960)          0           ['activation_432[0][0]']         \n",
            "                                                                                                  \n",
            " dense_59 (Dense)               (None, 6)            390         ['batch_normalization_435[0][0]']\n",
            "                                                                                                  \n",
            " dense_58 (Dense)               (None, 6)            5766        ['flatten_15[0][0]']             \n",
            "                                                                                                  \n",
            " activation_434 (Activation)    (None, 6)            0           ['dense_59[0][0]']               \n",
            "                                                                                                  \n",
            " activation_433 (Activation)    (None, 6)            0           ['dense_58[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 111,660\n",
            "Trainable params: 109,900\n",
            "Non-trainable params: 1,760\n",
            "__________________________________________________________________________________________________\n",
            "Params 111660\n",
            "Epoch 1/101\n",
            "114/114 [==============================] - 40s 77ms/step - loss: 2.8800 - activation_434_loss: 1.4716 - activation_433_loss: 1.4084 - activation_434_categorical_accuracy: 0.4735 - activation_433_categorical_accuracy: 0.5170 - val_loss: 3.1460 - val_activation_434_loss: 1.6602 - val_activation_433_loss: 1.4857 - val_activation_434_categorical_accuracy: 0.3627 - val_activation_433_categorical_accuracy: 0.3502 - lr: 0.0010\n",
            "Epoch 2/101\n",
            "114/114 [==============================] - 7s 66ms/step - loss: 0.6950 - activation_434_loss: 0.3770 - activation_433_loss: 0.3180 - activation_434_categorical_accuracy: 0.8661 - activation_433_categorical_accuracy: 0.8882 - val_loss: 1.4687 - val_activation_434_loss: 0.7481 - val_activation_433_loss: 0.7206 - val_activation_434_categorical_accuracy: 0.7414 - val_activation_433_categorical_accuracy: 0.7391 - lr: 0.0010\n",
            "Epoch 3/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.3275 - activation_434_loss: 0.1752 - activation_433_loss: 0.1523 - activation_434_categorical_accuracy: 0.9333 - activation_433_categorical_accuracy: 0.9445 - val_loss: 0.5414 - val_activation_434_loss: 0.2728 - val_activation_433_loss: 0.2686 - val_activation_434_categorical_accuracy: 0.9050 - val_activation_433_categorical_accuracy: 0.9033 - lr: 0.0010\n",
            "Epoch 4/101\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.2775 - activation_434_loss: 0.1493 - activation_433_loss: 0.1282 - activation_434_categorical_accuracy: 0.9411 - activation_433_categorical_accuracy: 0.9487 - val_loss: 0.4149 - val_activation_434_loss: 0.2135 - val_activation_433_loss: 0.2014 - val_activation_434_categorical_accuracy: 0.9298 - val_activation_433_categorical_accuracy: 0.9308 - lr: 0.0010\n",
            "Epoch 5/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.2725 - activation_434_loss: 0.1452 - activation_433_loss: 0.1273 - activation_434_categorical_accuracy: 0.9374 - activation_433_categorical_accuracy: 0.9456 - val_loss: 0.4246 - val_activation_434_loss: 0.2279 - val_activation_433_loss: 0.1967 - val_activation_434_categorical_accuracy: 0.9294 - val_activation_433_categorical_accuracy: 0.9321 - lr: 0.0010\n",
            "Epoch 6/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.2412 - activation_434_loss: 0.1266 - activation_433_loss: 0.1146 - activation_434_categorical_accuracy: 0.9424 - activation_433_categorical_accuracy: 0.9475 - val_loss: 0.4624 - val_activation_434_loss: 0.2530 - val_activation_433_loss: 0.2094 - val_activation_434_categorical_accuracy: 0.9335 - val_activation_433_categorical_accuracy: 0.9342 - lr: 0.0010\n",
            "Epoch 7/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.2391 - activation_434_loss: 0.1268 - activation_433_loss: 0.1123 - activation_434_categorical_accuracy: 0.9456 - activation_433_categorical_accuracy: 0.9527 - val_loss: 0.4385 - val_activation_434_loss: 0.2304 - val_activation_433_loss: 0.2081 - val_activation_434_categorical_accuracy: 0.9237 - val_activation_433_categorical_accuracy: 0.9189 - lr: 0.0010\n",
            "Epoch 8/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.2270 - activation_434_loss: 0.1203 - activation_433_loss: 0.1066 - activation_434_categorical_accuracy: 0.9482 - activation_433_categorical_accuracy: 0.9552 - val_loss: 0.4458 - val_activation_434_loss: 0.2463 - val_activation_433_loss: 0.1995 - val_activation_434_categorical_accuracy: 0.9281 - val_activation_433_categorical_accuracy: 0.9294 - lr: 0.0010\n",
            "Epoch 9/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.2334 - activation_434_loss: 0.1248 - activation_433_loss: 0.1085 - activation_434_categorical_accuracy: 0.9441 - activation_433_categorical_accuracy: 0.9513 - val_loss: 0.6916 - val_activation_434_loss: 0.4067 - val_activation_433_loss: 0.2849 - val_activation_434_categorical_accuracy: 0.9002 - val_activation_433_categorical_accuracy: 0.9023 - lr: 0.0010\n",
            "Epoch 10/101\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.2214 - activation_434_loss: 0.1173 - activation_433_loss: 0.1041 - activation_434_categorical_accuracy: 0.9493 - activation_433_categorical_accuracy: 0.9511 - val_loss: 0.4903 - val_activation_434_loss: 0.2545 - val_activation_433_loss: 0.2357 - val_activation_434_categorical_accuracy: 0.9260 - val_activation_433_categorical_accuracy: 0.9240 - lr: 0.0010\n",
            "Epoch 11/101\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.2103 - activation_434_loss: 0.1101 - activation_433_loss: 0.1002 - activation_434_categorical_accuracy: 0.9520 - activation_433_categorical_accuracy: 0.9563 - val_loss: 0.4322 - val_activation_434_loss: 0.2300 - val_activation_433_loss: 0.2022 - val_activation_434_categorical_accuracy: 0.9192 - val_activation_433_categorical_accuracy: 0.9257 - lr: 0.0010\n",
            "Epoch 12/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.2283 - activation_434_loss: 0.1225 - activation_433_loss: 0.1058 - activation_434_categorical_accuracy: 0.9471 - activation_433_categorical_accuracy: 0.9548 - val_loss: 0.5296 - val_activation_434_loss: 0.2869 - val_activation_433_loss: 0.2427 - val_activation_434_categorical_accuracy: 0.9138 - val_activation_433_categorical_accuracy: 0.9169 - lr: 0.0010\n",
            "Epoch 13/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.2133 - activation_434_loss: 0.1113 - activation_433_loss: 0.1020 - activation_434_categorical_accuracy: 0.9509 - activation_433_categorical_accuracy: 0.9552 - val_loss: 0.5049 - val_activation_434_loss: 0.2640 - val_activation_433_loss: 0.2409 - val_activation_434_categorical_accuracy: 0.9220 - val_activation_433_categorical_accuracy: 0.9182 - lr: 0.0010\n",
            "Epoch 14/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.2202 - activation_434_loss: 0.1152 - activation_433_loss: 0.1050 - activation_434_categorical_accuracy: 0.9512 - activation_433_categorical_accuracy: 0.9529 - val_loss: 0.4998 - val_activation_434_loss: 0.2771 - val_activation_433_loss: 0.2227 - val_activation_434_categorical_accuracy: 0.8992 - val_activation_433_categorical_accuracy: 0.9199 - lr: 0.0010\n",
            "Epoch 15/101\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.2269 - activation_434_loss: 0.1193 - activation_433_loss: 0.1075 - activation_434_categorical_accuracy: 0.9493 - activation_433_categorical_accuracy: 0.9541 - val_loss: 0.5616 - val_activation_434_loss: 0.2863 - val_activation_433_loss: 0.2753 - val_activation_434_categorical_accuracy: 0.9108 - val_activation_433_categorical_accuracy: 0.8989 - lr: 0.0010\n",
            "Epoch 16/101\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.2019 - activation_434_loss: 0.1073 - activation_433_loss: 0.0946 - activation_434_categorical_accuracy: 0.9541 - activation_433_categorical_accuracy: 0.9593 - val_loss: 0.3674 - val_activation_434_loss: 0.1847 - val_activation_433_loss: 0.1827 - val_activation_434_categorical_accuracy: 0.9386 - val_activation_433_categorical_accuracy: 0.9393 - lr: 0.0010\n",
            "Epoch 17/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.2086 - activation_434_loss: 0.1088 - activation_433_loss: 0.0998 - activation_434_categorical_accuracy: 0.9505 - activation_433_categorical_accuracy: 0.9546 - val_loss: 0.5078 - val_activation_434_loss: 0.2769 - val_activation_433_loss: 0.2309 - val_activation_434_categorical_accuracy: 0.9046 - val_activation_433_categorical_accuracy: 0.9053 - lr: 0.0010\n",
            "Epoch 18/101\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.2010 - activation_434_loss: 0.1069 - activation_433_loss: 0.0941 - activation_434_categorical_accuracy: 0.9519 - activation_433_categorical_accuracy: 0.9575 - val_loss: 0.4773 - val_activation_434_loss: 0.2533 - val_activation_433_loss: 0.2240 - val_activation_434_categorical_accuracy: 0.9420 - val_activation_433_categorical_accuracy: 0.9230 - lr: 0.0010\n",
            "Epoch 19/101\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 0.2005 - activation_434_loss: 0.1051 - activation_433_loss: 0.0954 - activation_434_categorical_accuracy: 0.9504 - activation_433_categorical_accuracy: 0.9570 - val_loss: 0.7132 - val_activation_434_loss: 0.4038 - val_activation_433_loss: 0.3094 - val_activation_434_categorical_accuracy: 0.9216 - val_activation_433_categorical_accuracy: 0.9186 - lr: 0.0010\n",
            "Epoch 20/101\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.2118 - activation_434_loss: 0.1123 - activation_433_loss: 0.0995 - activation_434_categorical_accuracy: 0.9501 - activation_433_categorical_accuracy: 0.9548 - val_loss: 0.7454 - val_activation_434_loss: 0.4669 - val_activation_433_loss: 0.2784 - val_activation_434_categorical_accuracy: 0.9138 - val_activation_433_categorical_accuracy: 0.9199 - lr: 0.0010\n",
            "Epoch 21/101\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.1866 - activation_434_loss: 0.0972 - activation_433_loss: 0.0893 - activation_434_categorical_accuracy: 0.9559 - activation_433_categorical_accuracy: 0.9593 - val_loss: 0.4967 - val_activation_434_loss: 0.2762 - val_activation_433_loss: 0.2205 - val_activation_434_categorical_accuracy: 0.9359 - val_activation_433_categorical_accuracy: 0.9291 - lr: 0.0010\n",
            "Epoch 22/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.1906 - activation_434_loss: 0.1004 - activation_433_loss: 0.0902 - activation_434_categorical_accuracy: 0.9555 - activation_433_categorical_accuracy: 0.9616 - val_loss: 0.4045 - val_activation_434_loss: 0.2122 - val_activation_433_loss: 0.1923 - val_activation_434_categorical_accuracy: 0.9335 - val_activation_433_categorical_accuracy: 0.9342 - lr: 0.0010\n",
            "Epoch 23/101\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.1869 - activation_434_loss: 0.0990 - activation_433_loss: 0.0879 - activation_434_categorical_accuracy: 0.9545 - activation_433_categorical_accuracy: 0.9587 - val_loss: 0.4616 - val_activation_434_loss: 0.2542 - val_activation_433_loss: 0.2074 - val_activation_434_categorical_accuracy: 0.9253 - val_activation_433_categorical_accuracy: 0.9291 - lr: 0.0010\n",
            "Epoch 24/101\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.1988 - activation_434_loss: 0.1055 - activation_433_loss: 0.0933 - activation_434_categorical_accuracy: 0.9531 - activation_433_categorical_accuracy: 0.9579 - val_loss: 0.6037 - val_activation_434_loss: 0.3230 - val_activation_433_loss: 0.2807 - val_activation_434_categorical_accuracy: 0.9203 - val_activation_433_categorical_accuracy: 0.9186 - lr: 0.0010\n",
            "Epoch 25/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.2142 - activation_434_loss: 0.1130 - activation_433_loss: 0.1011 - activation_434_categorical_accuracy: 0.9518 - activation_433_categorical_accuracy: 0.9571 - val_loss: 0.4234 - val_activation_434_loss: 0.2102 - val_activation_433_loss: 0.2131 - val_activation_434_categorical_accuracy: 0.9270 - val_activation_433_categorical_accuracy: 0.9284 - lr: 0.0010\n",
            "Epoch 26/101\n",
            "114/114 [==============================] - 8s 66ms/step - loss: 0.2010 - activation_434_loss: 0.1068 - activation_433_loss: 0.0942 - activation_434_categorical_accuracy: 0.9549 - activation_433_categorical_accuracy: 0.9597 - val_loss: 0.7496 - val_activation_434_loss: 0.4357 - val_activation_433_loss: 0.3139 - val_activation_434_categorical_accuracy: 0.9125 - val_activation_433_categorical_accuracy: 0.9186 - lr: 0.0010\n",
            "Epoch 27/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1817 - activation_434_loss: 0.0963 - activation_433_loss: 0.0853 - activation_434_categorical_accuracy: 0.9560 - activation_433_categorical_accuracy: 0.9604 - val_loss: 0.4839 - val_activation_434_loss: 0.2811 - val_activation_433_loss: 0.2028 - val_activation_434_categorical_accuracy: 0.9308 - val_activation_433_categorical_accuracy: 0.9308 - lr: 0.0010\n",
            "Epoch 28/101\n",
            "114/114 [==============================] - 8s 67ms/step - loss: 0.1932 - activation_434_loss: 0.1030 - activation_433_loss: 0.0902 - activation_434_categorical_accuracy: 0.9563 - activation_433_categorical_accuracy: 0.9611 - val_loss: 0.4476 - val_activation_434_loss: 0.2475 - val_activation_433_loss: 0.2001 - val_activation_434_categorical_accuracy: 0.9345 - val_activation_433_categorical_accuracy: 0.9332 - lr: 0.0010\n",
            "Epoch 29/101\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.1768 - activation_434_loss: 0.0931 - activation_433_loss: 0.0837 - activation_434_categorical_accuracy: 0.9608 - activation_433_categorical_accuracy: 0.9640 - val_loss: 0.5651 - val_activation_434_loss: 0.2883 - val_activation_433_loss: 0.2768 - val_activation_434_categorical_accuracy: 0.9203 - val_activation_433_categorical_accuracy: 0.9175 - lr: 0.0010\n",
            "Epoch 30/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.1754 - activation_434_loss: 0.0924 - activation_433_loss: 0.0830 - activation_434_categorical_accuracy: 0.9605 - activation_433_categorical_accuracy: 0.9638 - val_loss: 0.5790 - val_activation_434_loss: 0.3161 - val_activation_433_loss: 0.2628 - val_activation_434_categorical_accuracy: 0.9220 - val_activation_433_categorical_accuracy: 0.9199 - lr: 0.0010\n",
            "Epoch 31/101\n",
            "114/114 [==============================] - 7s 57ms/step - loss: 0.1601 - activation_434_loss: 0.0841 - activation_433_loss: 0.0759 - activation_434_categorical_accuracy: 0.9648 - activation_433_categorical_accuracy: 0.9682 - val_loss: 0.4641 - val_activation_434_loss: 0.2516 - val_activation_433_loss: 0.2125 - val_activation_434_categorical_accuracy: 0.9301 - val_activation_433_categorical_accuracy: 0.9318 - lr: 9.9005e-04\n",
            "Epoch 32/101\n",
            "114/114 [==============================] - 7s 57ms/step - loss: 0.1627 - activation_434_loss: 0.0859 - activation_433_loss: 0.0768 - activation_434_categorical_accuracy: 0.9634 - activation_433_categorical_accuracy: 0.9670 - val_loss: 0.4124 - val_activation_434_loss: 0.2209 - val_activation_433_loss: 0.1916 - val_activation_434_categorical_accuracy: 0.9332 - val_activation_433_categorical_accuracy: 0.9355 - lr: 9.8020e-04\n",
            "Epoch 33/101\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 0.1547 - activation_434_loss: 0.0807 - activation_433_loss: 0.0740 - activation_434_categorical_accuracy: 0.9635 - activation_433_categorical_accuracy: 0.9663 - val_loss: 0.5291 - val_activation_434_loss: 0.2673 - val_activation_433_loss: 0.2617 - val_activation_434_categorical_accuracy: 0.9345 - val_activation_433_categorical_accuracy: 0.9335 - lr: 9.7045e-04\n",
            "Epoch 34/101\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.1858 - activation_434_loss: 0.0984 - activation_433_loss: 0.0873 - activation_434_categorical_accuracy: 0.9582 - activation_433_categorical_accuracy: 0.9607 - val_loss: 0.4478 - val_activation_434_loss: 0.2371 - val_activation_433_loss: 0.2106 - val_activation_434_categorical_accuracy: 0.9311 - val_activation_433_categorical_accuracy: 0.9308 - lr: 9.6079e-04\n",
            "Epoch 35/101\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.1801 - activation_434_loss: 0.0955 - activation_433_loss: 0.0846 - activation_434_categorical_accuracy: 0.9600 - activation_433_categorical_accuracy: 0.9646 - val_loss: 0.5445 - val_activation_434_loss: 0.2647 - val_activation_433_loss: 0.2798 - val_activation_434_categorical_accuracy: 0.9365 - val_activation_433_categorical_accuracy: 0.9355 - lr: 9.5123e-04\n",
            "Epoch 36/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.1509 - activation_434_loss: 0.0788 - activation_433_loss: 0.0721 - activation_434_categorical_accuracy: 0.9674 - activation_433_categorical_accuracy: 0.9696 - val_loss: 0.4834 - val_activation_434_loss: 0.2551 - val_activation_433_loss: 0.2283 - val_activation_434_categorical_accuracy: 0.9393 - val_activation_433_categorical_accuracy: 0.9365 - lr: 9.4176e-04\n",
            "Epoch 37/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1446 - activation_434_loss: 0.0762 - activation_433_loss: 0.0684 - activation_434_categorical_accuracy: 0.9657 - activation_433_categorical_accuracy: 0.9703 - val_loss: 0.5698 - val_activation_434_loss: 0.3207 - val_activation_433_loss: 0.2490 - val_activation_434_categorical_accuracy: 0.9325 - val_activation_433_categorical_accuracy: 0.9342 - lr: 9.3239e-04\n",
            "Epoch 38/101\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.1430 - activation_434_loss: 0.0749 - activation_433_loss: 0.0681 - activation_434_categorical_accuracy: 0.9670 - activation_433_categorical_accuracy: 0.9698 - val_loss: 0.3507 - val_activation_434_loss: 0.1734 - val_activation_433_loss: 0.1773 - val_activation_434_categorical_accuracy: 0.9488 - val_activation_433_categorical_accuracy: 0.9396 - lr: 9.2312e-04\n",
            "Epoch 39/101\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.1550 - activation_434_loss: 0.0818 - activation_433_loss: 0.0733 - activation_434_categorical_accuracy: 0.9670 - activation_433_categorical_accuracy: 0.9690 - val_loss: 0.5439 - val_activation_434_loss: 0.3027 - val_activation_433_loss: 0.2412 - val_activation_434_categorical_accuracy: 0.9284 - val_activation_433_categorical_accuracy: 0.9284 - lr: 9.1393e-04\n",
            "Epoch 40/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.1442 - activation_434_loss: 0.0765 - activation_433_loss: 0.0678 - activation_434_categorical_accuracy: 0.9667 - activation_433_categorical_accuracy: 0.9694 - val_loss: 0.5004 - val_activation_434_loss: 0.2834 - val_activation_433_loss: 0.2171 - val_activation_434_categorical_accuracy: 0.9403 - val_activation_433_categorical_accuracy: 0.9389 - lr: 9.0484e-04\n",
            "Epoch 41/101\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.1507 - activation_434_loss: 0.0793 - activation_433_loss: 0.0714 - activation_434_categorical_accuracy: 0.9671 - activation_433_categorical_accuracy: 0.9714 - val_loss: 0.4145 - val_activation_434_loss: 0.2263 - val_activation_433_loss: 0.1882 - val_activation_434_categorical_accuracy: 0.9444 - val_activation_433_categorical_accuracy: 0.9440 - lr: 8.9583e-04\n",
            "Epoch 42/101\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.1299 - activation_434_loss: 0.0682 - activation_433_loss: 0.0617 - activation_434_categorical_accuracy: 0.9711 - activation_433_categorical_accuracy: 0.9746 - val_loss: 0.5342 - val_activation_434_loss: 0.2895 - val_activation_433_loss: 0.2447 - val_activation_434_categorical_accuracy: 0.9294 - val_activation_433_categorical_accuracy: 0.9315 - lr: 8.8692e-04\n",
            "Epoch 43/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.1464 - activation_434_loss: 0.0771 - activation_433_loss: 0.0693 - activation_434_categorical_accuracy: 0.9677 - activation_433_categorical_accuracy: 0.9716 - val_loss: 0.6210 - val_activation_434_loss: 0.3288 - val_activation_433_loss: 0.2922 - val_activation_434_categorical_accuracy: 0.9284 - val_activation_433_categorical_accuracy: 0.9274 - lr: 8.7810e-04\n",
            "Epoch 44/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.1458 - activation_434_loss: 0.0791 - activation_433_loss: 0.0668 - activation_434_categorical_accuracy: 0.9661 - activation_433_categorical_accuracy: 0.9707 - val_loss: 0.5829 - val_activation_434_loss: 0.3137 - val_activation_433_loss: 0.2692 - val_activation_434_categorical_accuracy: 0.9389 - val_activation_433_categorical_accuracy: 0.9376 - lr: 8.6936e-04\n",
            "Epoch 45/101\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.1295 - activation_434_loss: 0.0680 - activation_433_loss: 0.0615 - activation_434_categorical_accuracy: 0.9700 - activation_433_categorical_accuracy: 0.9733 - val_loss: 0.4731 - val_activation_434_loss: 0.2369 - val_activation_433_loss: 0.2362 - val_activation_434_categorical_accuracy: 0.9464 - val_activation_433_categorical_accuracy: 0.9474 - lr: 8.6071e-04\n",
            "Epoch 46/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1406 - activation_434_loss: 0.0747 - activation_433_loss: 0.0659 - activation_434_categorical_accuracy: 0.9685 - activation_433_categorical_accuracy: 0.9715 - val_loss: 0.4112 - val_activation_434_loss: 0.2286 - val_activation_433_loss: 0.1826 - val_activation_434_categorical_accuracy: 0.9382 - val_activation_433_categorical_accuracy: 0.9403 - lr: 8.5214e-04\n",
            "Epoch 47/101\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.1328 - activation_434_loss: 0.0706 - activation_433_loss: 0.0622 - activation_434_categorical_accuracy: 0.9700 - activation_433_categorical_accuracy: 0.9726 - val_loss: 0.4452 - val_activation_434_loss: 0.2293 - val_activation_433_loss: 0.2158 - val_activation_434_categorical_accuracy: 0.9376 - val_activation_433_categorical_accuracy: 0.9379 - lr: 8.4366e-04\n",
            "Epoch 48/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.1220 - activation_434_loss: 0.0655 - activation_433_loss: 0.0565 - activation_434_categorical_accuracy: 0.9729 - activation_433_categorical_accuracy: 0.9763 - val_loss: 0.5290 - val_activation_434_loss: 0.2867 - val_activation_433_loss: 0.2423 - val_activation_434_categorical_accuracy: 0.9399 - val_activation_433_categorical_accuracy: 0.9410 - lr: 8.3527e-04\n",
            "Epoch 49/101\n",
            "114/114 [==============================] - 7s 64ms/step - loss: 0.1237 - activation_434_loss: 0.0647 - activation_433_loss: 0.0590 - activation_434_categorical_accuracy: 0.9730 - activation_433_categorical_accuracy: 0.9755 - val_loss: 0.4896 - val_activation_434_loss: 0.2434 - val_activation_433_loss: 0.2463 - val_activation_434_categorical_accuracy: 0.9477 - val_activation_433_categorical_accuracy: 0.9484 - lr: 8.2696e-04\n",
            "Epoch 50/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.1157 - activation_434_loss: 0.0604 - activation_433_loss: 0.0553 - activation_434_categorical_accuracy: 0.9763 - activation_433_categorical_accuracy: 0.9781 - val_loss: 0.5419 - val_activation_434_loss: 0.2828 - val_activation_433_loss: 0.2591 - val_activation_434_categorical_accuracy: 0.9396 - val_activation_433_categorical_accuracy: 0.9430 - lr: 8.1873e-04\n",
            "Epoch 51/101\n",
            "114/114 [==============================] - 8s 68ms/step - loss: 0.1238 - activation_434_loss: 0.0653 - activation_433_loss: 0.0584 - activation_434_categorical_accuracy: 0.9760 - activation_433_categorical_accuracy: 0.9772 - val_loss: 0.4638 - val_activation_434_loss: 0.2240 - val_activation_433_loss: 0.2399 - val_activation_434_categorical_accuracy: 0.9457 - val_activation_433_categorical_accuracy: 0.9403 - lr: 8.1058e-04\n",
            "Epoch 52/101\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.1188 - activation_434_loss: 0.0645 - activation_433_loss: 0.0543 - activation_434_categorical_accuracy: 0.9735 - activation_433_categorical_accuracy: 0.9768 - val_loss: 0.4735 - val_activation_434_loss: 0.2482 - val_activation_433_loss: 0.2253 - val_activation_434_categorical_accuracy: 0.9396 - val_activation_433_categorical_accuracy: 0.9382 - lr: 8.0252e-04\n",
            "Epoch 53/101\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.1244 - activation_434_loss: 0.0671 - activation_433_loss: 0.0573 - activation_434_categorical_accuracy: 0.9705 - activation_433_categorical_accuracy: 0.9755 - val_loss: 0.4118 - val_activation_434_loss: 0.2081 - val_activation_433_loss: 0.2037 - val_activation_434_categorical_accuracy: 0.9508 - val_activation_433_categorical_accuracy: 0.9437 - lr: 7.9453e-04\n",
            "Epoch 54/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1249 - activation_434_loss: 0.0669 - activation_433_loss: 0.0580 - activation_434_categorical_accuracy: 0.9752 - activation_433_categorical_accuracy: 0.9779 - val_loss: 0.4668 - val_activation_434_loss: 0.2462 - val_activation_433_loss: 0.2206 - val_activation_434_categorical_accuracy: 0.9345 - val_activation_433_categorical_accuracy: 0.9345 - lr: 7.8663e-04\n",
            "Epoch 55/101\n",
            "114/114 [==============================] - 8s 68ms/step - loss: 0.1143 - activation_434_loss: 0.0631 - activation_433_loss: 0.0512 - activation_434_categorical_accuracy: 0.9755 - activation_433_categorical_accuracy: 0.9808 - val_loss: 0.7363 - val_activation_434_loss: 0.4001 - val_activation_433_loss: 0.3362 - val_activation_434_categorical_accuracy: 0.9243 - val_activation_433_categorical_accuracy: 0.9294 - lr: 7.7880e-04\n",
            "Epoch 56/101\n",
            "114/114 [==============================] - 8s 66ms/step - loss: 0.1127 - activation_434_loss: 0.0604 - activation_433_loss: 0.0523 - activation_434_categorical_accuracy: 0.9766 - activation_433_categorical_accuracy: 0.9789 - val_loss: 0.4930 - val_activation_434_loss: 0.2553 - val_activation_433_loss: 0.2377 - val_activation_434_categorical_accuracy: 0.9335 - val_activation_433_categorical_accuracy: 0.9403 - lr: 7.7105e-04\n",
            "Epoch 57/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.1045 - activation_434_loss: 0.0564 - activation_433_loss: 0.0481 - activation_434_categorical_accuracy: 0.9771 - activation_433_categorical_accuracy: 0.9803 - val_loss: 0.4874 - val_activation_434_loss: 0.2460 - val_activation_433_loss: 0.2413 - val_activation_434_categorical_accuracy: 0.9393 - val_activation_433_categorical_accuracy: 0.9372 - lr: 7.6338e-04\n",
            "Epoch 58/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.1102 - activation_434_loss: 0.0604 - activation_433_loss: 0.0498 - activation_434_categorical_accuracy: 0.9756 - activation_433_categorical_accuracy: 0.9799 - val_loss: 0.5072 - val_activation_434_loss: 0.2393 - val_activation_433_loss: 0.2679 - val_activation_434_categorical_accuracy: 0.9460 - val_activation_433_categorical_accuracy: 0.9471 - lr: 7.5578e-04\n",
            "Epoch 59/101\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.1012 - activation_434_loss: 0.0568 - activation_433_loss: 0.0444 - activation_434_categorical_accuracy: 0.9768 - activation_433_categorical_accuracy: 0.9818 - val_loss: 0.4978 - val_activation_434_loss: 0.2705 - val_activation_433_loss: 0.2273 - val_activation_434_categorical_accuracy: 0.9396 - val_activation_433_categorical_accuracy: 0.9410 - lr: 7.4826e-04\n",
            "Epoch 60/101\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.0982 - activation_434_loss: 0.0540 - activation_433_loss: 0.0442 - activation_434_categorical_accuracy: 0.9796 - activation_433_categorical_accuracy: 0.9815 - val_loss: 0.3938 - val_activation_434_loss: 0.1959 - val_activation_433_loss: 0.1978 - val_activation_434_categorical_accuracy: 0.9444 - val_activation_433_categorical_accuracy: 0.9403 - lr: 7.4082e-04\n",
            "Epoch 61/101\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.0929 - activation_434_loss: 0.0509 - activation_433_loss: 0.0420 - activation_434_categorical_accuracy: 0.9799 - activation_433_categorical_accuracy: 0.9820 - val_loss: 0.3989 - val_activation_434_loss: 0.2040 - val_activation_433_loss: 0.1949 - val_activation_434_categorical_accuracy: 0.9413 - val_activation_433_categorical_accuracy: 0.9386 - lr: 7.3345e-04\n",
            "Epoch 62/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0847 - activation_434_loss: 0.0479 - activation_433_loss: 0.0368 - activation_434_categorical_accuracy: 0.9805 - activation_433_categorical_accuracy: 0.9857 - val_loss: 0.6681 - val_activation_434_loss: 0.3482 - val_activation_433_loss: 0.3199 - val_activation_434_categorical_accuracy: 0.9359 - val_activation_433_categorical_accuracy: 0.9359 - lr: 7.2615e-04\n",
            "Epoch 63/101\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.0933 - activation_434_loss: 0.0520 - activation_433_loss: 0.0413 - activation_434_categorical_accuracy: 0.9801 - activation_433_categorical_accuracy: 0.9841 - val_loss: 0.3923 - val_activation_434_loss: 0.2034 - val_activation_433_loss: 0.1888 - val_activation_434_categorical_accuracy: 0.9467 - val_activation_433_categorical_accuracy: 0.9481 - lr: 7.1892e-04\n",
            "Epoch 64/101\n",
            "114/114 [==============================] - 8s 66ms/step - loss: 0.0776 - activation_434_loss: 0.0427 - activation_433_loss: 0.0348 - activation_434_categorical_accuracy: 0.9820 - activation_433_categorical_accuracy: 0.9862 - val_loss: 0.4882 - val_activation_434_loss: 0.2375 - val_activation_433_loss: 0.2507 - val_activation_434_categorical_accuracy: 0.9433 - val_activation_433_categorical_accuracy: 0.9406 - lr: 7.1177e-04\n",
            "Epoch 65/101\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.0938 - activation_434_loss: 0.0517 - activation_433_loss: 0.0422 - activation_434_categorical_accuracy: 0.9781 - activation_433_categorical_accuracy: 0.9818 - val_loss: 0.5529 - val_activation_434_loss: 0.2722 - val_activation_433_loss: 0.2807 - val_activation_434_categorical_accuracy: 0.9413 - val_activation_433_categorical_accuracy: 0.9433 - lr: 7.0469e-04\n",
            "Epoch 66/101\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.0920 - activation_434_loss: 0.0513 - activation_433_loss: 0.0407 - activation_434_categorical_accuracy: 0.9794 - activation_433_categorical_accuracy: 0.9838 - val_loss: 0.4456 - val_activation_434_loss: 0.2357 - val_activation_433_loss: 0.2098 - val_activation_434_categorical_accuracy: 0.9430 - val_activation_433_categorical_accuracy: 0.9430 - lr: 6.9768e-04\n",
            "Epoch 67/101\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.0724 - activation_434_loss: 0.0408 - activation_433_loss: 0.0316 - activation_434_categorical_accuracy: 0.9834 - activation_433_categorical_accuracy: 0.9873 - val_loss: 0.3612 - val_activation_434_loss: 0.1805 - val_activation_433_loss: 0.1807 - val_activation_434_categorical_accuracy: 0.9552 - val_activation_433_categorical_accuracy: 0.9576 - lr: 6.9073e-04\n",
            "Epoch 68/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0785 - activation_434_loss: 0.0446 - activation_433_loss: 0.0339 - activation_434_categorical_accuracy: 0.9812 - activation_433_categorical_accuracy: 0.9862 - val_loss: 0.4350 - val_activation_434_loss: 0.2218 - val_activation_433_loss: 0.2132 - val_activation_434_categorical_accuracy: 0.9467 - val_activation_433_categorical_accuracy: 0.9450 - lr: 6.8386e-04\n",
            "Epoch 69/101\n",
            "114/114 [==============================] - 8s 67ms/step - loss: 0.0729 - activation_434_loss: 0.0408 - activation_433_loss: 0.0321 - activation_434_categorical_accuracy: 0.9829 - activation_433_categorical_accuracy: 0.9867 - val_loss: 0.5622 - val_activation_434_loss: 0.2745 - val_activation_433_loss: 0.2876 - val_activation_434_categorical_accuracy: 0.9433 - val_activation_433_categorical_accuracy: 0.9413 - lr: 6.7706e-04\n",
            "Epoch 70/101\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0700 - activation_434_loss: 0.0400 - activation_433_loss: 0.0299 - activation_434_categorical_accuracy: 0.9848 - activation_433_categorical_accuracy: 0.9883 - val_loss: 0.4210 - val_activation_434_loss: 0.2006 - val_activation_433_loss: 0.2204 - val_activation_434_categorical_accuracy: 0.9450 - val_activation_433_categorical_accuracy: 0.9488 - lr: 6.7032e-04\n",
            "Epoch 71/101\n",
            "114/114 [==============================] - 7s 64ms/step - loss: 0.0758 - activation_434_loss: 0.0432 - activation_433_loss: 0.0325 - activation_434_categorical_accuracy: 0.9837 - activation_433_categorical_accuracy: 0.9879 - val_loss: 0.6283 - val_activation_434_loss: 0.3199 - val_activation_433_loss: 0.3084 - val_activation_434_categorical_accuracy: 0.9335 - val_activation_433_categorical_accuracy: 0.9338 - lr: 6.6365e-04\n",
            "Epoch 72/101\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.0643 - activation_434_loss: 0.0378 - activation_433_loss: 0.0265 - activation_434_categorical_accuracy: 0.9844 - activation_433_categorical_accuracy: 0.9893 - val_loss: 0.5204 - val_activation_434_loss: 0.2712 - val_activation_433_loss: 0.2492 - val_activation_434_categorical_accuracy: 0.9539 - val_activation_433_categorical_accuracy: 0.9511 - lr: 6.5705e-04\n",
            "Epoch 73/101\n",
            "114/114 [==============================] - 7s 66ms/step - loss: 0.0709 - activation_434_loss: 0.0407 - activation_433_loss: 0.0302 - activation_434_categorical_accuracy: 0.9851 - activation_433_categorical_accuracy: 0.9878 - val_loss: 0.5371 - val_activation_434_loss: 0.2780 - val_activation_433_loss: 0.2591 - val_activation_434_categorical_accuracy: 0.9365 - val_activation_433_categorical_accuracy: 0.9427 - lr: 6.5051e-04\n",
            "Epoch 74/101\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.0647 - activation_434_loss: 0.0378 - activation_433_loss: 0.0269 - activation_434_categorical_accuracy: 0.9867 - activation_433_categorical_accuracy: 0.9903 - val_loss: 0.5169 - val_activation_434_loss: 0.2700 - val_activation_433_loss: 0.2469 - val_activation_434_categorical_accuracy: 0.9389 - val_activation_433_categorical_accuracy: 0.9464 - lr: 6.4404e-04\n",
            "Epoch 75/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0750 - activation_434_loss: 0.0452 - activation_433_loss: 0.0298 - activation_434_categorical_accuracy: 0.9844 - activation_433_categorical_accuracy: 0.9885 - val_loss: 0.6870 - val_activation_434_loss: 0.3240 - val_activation_433_loss: 0.3630 - val_activation_434_categorical_accuracy: 0.9406 - val_activation_433_categorical_accuracy: 0.9437 - lr: 6.3763e-04\n",
            "Epoch 76/101\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.0630 - activation_434_loss: 0.0351 - activation_433_loss: 0.0279 - activation_434_categorical_accuracy: 0.9853 - activation_433_categorical_accuracy: 0.9896 - val_loss: 0.4903 - val_activation_434_loss: 0.2509 - val_activation_433_loss: 0.2394 - val_activation_434_categorical_accuracy: 0.9467 - val_activation_433_categorical_accuracy: 0.9484 - lr: 6.3128e-04\n",
            "Epoch 77/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0648 - activation_434_loss: 0.0383 - activation_433_loss: 0.0265 - activation_434_categorical_accuracy: 0.9855 - activation_433_categorical_accuracy: 0.9903 - val_loss: 0.4182 - val_activation_434_loss: 0.1964 - val_activation_433_loss: 0.2218 - val_activation_434_categorical_accuracy: 0.9532 - val_activation_433_categorical_accuracy: 0.9501 - lr: 6.2500e-04\n",
            "Epoch 78/101\n",
            "114/114 [==============================] - 8s 70ms/step - loss: 0.0589 - activation_434_loss: 0.0351 - activation_433_loss: 0.0238 - activation_434_categorical_accuracy: 0.9870 - activation_433_categorical_accuracy: 0.9926 - val_loss: 0.4766 - val_activation_434_loss: 0.2491 - val_activation_433_loss: 0.2274 - val_activation_434_categorical_accuracy: 0.9457 - val_activation_433_categorical_accuracy: 0.9444 - lr: 6.1878e-04\n",
            "Epoch 79/101\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.0640 - activation_434_loss: 0.0376 - activation_433_loss: 0.0264 - activation_434_categorical_accuracy: 0.9860 - activation_433_categorical_accuracy: 0.9907 - val_loss: 0.5399 - val_activation_434_loss: 0.2749 - val_activation_433_loss: 0.2650 - val_activation_434_categorical_accuracy: 0.9467 - val_activation_433_categorical_accuracy: 0.9420 - lr: 6.1263e-04\n",
            "Epoch 80/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0510 - activation_434_loss: 0.0305 - activation_433_loss: 0.0204 - activation_434_categorical_accuracy: 0.9893 - activation_433_categorical_accuracy: 0.9926 - val_loss: 0.3661 - val_activation_434_loss: 0.1848 - val_activation_433_loss: 0.1814 - val_activation_434_categorical_accuracy: 0.9518 - val_activation_433_categorical_accuracy: 0.9498 - lr: 6.0653e-04\n",
            "Epoch 81/101\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0497 - activation_434_loss: 0.0304 - activation_433_loss: 0.0192 - activation_434_categorical_accuracy: 0.9897 - activation_433_categorical_accuracy: 0.9934 - val_loss: 0.3276 - val_activation_434_loss: 0.1638 - val_activation_433_loss: 0.1639 - val_activation_434_categorical_accuracy: 0.9593 - val_activation_433_categorical_accuracy: 0.9549 - lr: 6.0050e-04\n",
            "Epoch 82/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0508 - activation_434_loss: 0.0301 - activation_433_loss: 0.0206 - activation_434_categorical_accuracy: 0.9888 - activation_433_categorical_accuracy: 0.9923 - val_loss: 0.4175 - val_activation_434_loss: 0.1996 - val_activation_433_loss: 0.2179 - val_activation_434_categorical_accuracy: 0.9586 - val_activation_433_categorical_accuracy: 0.9545 - lr: 5.9452e-04\n",
            "Epoch 83/101\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.0416 - activation_434_loss: 0.0246 - activation_433_loss: 0.0171 - activation_434_categorical_accuracy: 0.9905 - activation_433_categorical_accuracy: 0.9944 - val_loss: 0.4491 - val_activation_434_loss: 0.2224 - val_activation_433_loss: 0.2267 - val_activation_434_categorical_accuracy: 0.9501 - val_activation_433_categorical_accuracy: 0.9542 - lr: 5.8861e-04\n",
            "Epoch 84/101\n",
            "114/114 [==============================] - 8s 69ms/step - loss: 0.0384 - activation_434_loss: 0.0229 - activation_433_loss: 0.0155 - activation_434_categorical_accuracy: 0.9915 - activation_433_categorical_accuracy: 0.9941 - val_loss: 0.3816 - val_activation_434_loss: 0.1915 - val_activation_433_loss: 0.1901 - val_activation_434_categorical_accuracy: 0.9583 - val_activation_433_categorical_accuracy: 0.9559 - lr: 5.8275e-04\n",
            "Epoch 85/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0373 - activation_434_loss: 0.0217 - activation_433_loss: 0.0156 - activation_434_categorical_accuracy: 0.9923 - activation_433_categorical_accuracy: 0.9948 - val_loss: 0.4357 - val_activation_434_loss: 0.2109 - val_activation_433_loss: 0.2248 - val_activation_434_categorical_accuracy: 0.9522 - val_activation_433_categorical_accuracy: 0.9579 - lr: 5.7695e-04\n",
            "Epoch 86/101\n",
            "114/114 [==============================] - 8s 68ms/step - loss: 0.0590 - activation_434_loss: 0.0353 - activation_433_loss: 0.0236 - activation_434_categorical_accuracy: 0.9890 - activation_433_categorical_accuracy: 0.9929 - val_loss: 0.9385 - val_activation_434_loss: 0.3680 - val_activation_433_loss: 0.5705 - val_activation_434_categorical_accuracy: 0.9247 - val_activation_433_categorical_accuracy: 0.9247 - lr: 5.7121e-04\n",
            "Epoch 87/101\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.0434 - activation_434_loss: 0.0261 - activation_433_loss: 0.0174 - activation_434_categorical_accuracy: 0.9908 - activation_433_categorical_accuracy: 0.9945 - val_loss: 0.5186 - val_activation_434_loss: 0.2301 - val_activation_433_loss: 0.2884 - val_activation_434_categorical_accuracy: 0.9627 - val_activation_433_categorical_accuracy: 0.9586 - lr: 5.6553e-04\n",
            "Epoch 88/101\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.0383 - activation_434_loss: 0.0251 - activation_433_loss: 0.0132 - activation_434_categorical_accuracy: 0.9915 - activation_433_categorical_accuracy: 0.9956 - val_loss: 0.4557 - val_activation_434_loss: 0.2305 - val_activation_433_loss: 0.2253 - val_activation_434_categorical_accuracy: 0.9484 - val_activation_433_categorical_accuracy: 0.9454 - lr: 5.5990e-04\n",
            "Epoch 89/101\n",
            "114/114 [==============================] - 8s 69ms/step - loss: 0.0369 - activation_434_loss: 0.0232 - activation_433_loss: 0.0137 - activation_434_categorical_accuracy: 0.9921 - activation_433_categorical_accuracy: 0.9951 - val_loss: 0.5435 - val_activation_434_loss: 0.2623 - val_activation_433_loss: 0.2812 - val_activation_434_categorical_accuracy: 0.9515 - val_activation_433_categorical_accuracy: 0.9501 - lr: 5.5433e-04\n",
            "Epoch 90/101\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0367 - activation_434_loss: 0.0226 - activation_433_loss: 0.0141 - activation_434_categorical_accuracy: 0.9921 - activation_433_categorical_accuracy: 0.9951 - val_loss: 0.3344 - val_activation_434_loss: 0.1519 - val_activation_433_loss: 0.1825 - val_activation_434_categorical_accuracy: 0.9627 - val_activation_433_categorical_accuracy: 0.9552 - lr: 5.4881e-04\n",
            "Epoch 91/101\n",
            "114/114 [==============================] - 8s 66ms/step - loss: 0.0298 - activation_434_loss: 0.0193 - activation_433_loss: 0.0105 - activation_434_categorical_accuracy: 0.9922 - activation_433_categorical_accuracy: 0.9960 - val_loss: 0.5230 - val_activation_434_loss: 0.2540 - val_activation_433_loss: 0.2689 - val_activation_434_categorical_accuracy: 0.9498 - val_activation_433_categorical_accuracy: 0.9454 - lr: 5.4335e-04\n",
            "Epoch 92/101\n",
            "114/114 [==============================] - 8s 69ms/step - loss: 0.0295 - activation_434_loss: 0.0186 - activation_433_loss: 0.0109 - activation_434_categorical_accuracy: 0.9936 - activation_433_categorical_accuracy: 0.9966 - val_loss: 0.3839 - val_activation_434_loss: 0.1805 - val_activation_433_loss: 0.2034 - val_activation_434_categorical_accuracy: 0.9596 - val_activation_433_categorical_accuracy: 0.9569 - lr: 5.3794e-04\n",
            "Epoch 93/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0333 - activation_434_loss: 0.0191 - activation_433_loss: 0.0143 - activation_434_categorical_accuracy: 0.9940 - activation_433_categorical_accuracy: 0.9955 - val_loss: 0.5133 - val_activation_434_loss: 0.2267 - val_activation_433_loss: 0.2865 - val_activation_434_categorical_accuracy: 0.9572 - val_activation_433_categorical_accuracy: 0.9511 - lr: 5.3259e-04\n",
            "Epoch 94/101\n",
            "114/114 [==============================] - 8s 70ms/step - loss: 0.0287 - activation_434_loss: 0.0182 - activation_433_loss: 0.0105 - activation_434_categorical_accuracy: 0.9938 - activation_433_categorical_accuracy: 0.9971 - val_loss: 0.4369 - val_activation_434_loss: 0.2119 - val_activation_433_loss: 0.2249 - val_activation_434_categorical_accuracy: 0.9576 - val_activation_433_categorical_accuracy: 0.9525 - lr: 5.2729e-04\n",
            "Epoch 95/101\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0232 - activation_434_loss: 0.0152 - activation_433_loss: 0.0080 - activation_434_categorical_accuracy: 0.9951 - activation_433_categorical_accuracy: 0.9973 - val_loss: 0.2909 - val_activation_434_loss: 0.1339 - val_activation_433_loss: 0.1569 - val_activation_434_categorical_accuracy: 0.9627 - val_activation_433_categorical_accuracy: 0.9620 - lr: 5.2205e-04\n",
            "Epoch 96/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0328 - activation_434_loss: 0.0198 - activation_433_loss: 0.0130 - activation_434_categorical_accuracy: 0.9936 - activation_433_categorical_accuracy: 0.9956 - val_loss: 0.5255 - val_activation_434_loss: 0.2170 - val_activation_433_loss: 0.3085 - val_activation_434_categorical_accuracy: 0.9606 - val_activation_433_categorical_accuracy: 0.9559 - lr: 5.1685e-04\n",
            "Epoch 97/101\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0262 - activation_434_loss: 0.0158 - activation_433_loss: 0.0104 - activation_434_categorical_accuracy: 0.9941 - activation_433_categorical_accuracy: 0.9963 - val_loss: 0.6743 - val_activation_434_loss: 0.2255 - val_activation_433_loss: 0.4487 - val_activation_434_categorical_accuracy: 0.9562 - val_activation_433_categorical_accuracy: 0.9566 - lr: 5.1171e-04\n",
            "Epoch 98/101\n",
            "114/114 [==============================] - 8s 68ms/step - loss: 0.0188 - activation_434_loss: 0.0132 - activation_433_loss: 0.0056 - activation_434_categorical_accuracy: 0.9953 - activation_433_categorical_accuracy: 0.9982 - val_loss: 0.4860 - val_activation_434_loss: 0.2137 - val_activation_433_loss: 0.2724 - val_activation_434_categorical_accuracy: 0.9572 - val_activation_433_categorical_accuracy: 0.9508 - lr: 5.0662e-04\n",
            "Epoch 99/101\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0169 - activation_434_loss: 0.0104 - activation_433_loss: 0.0065 - activation_434_categorical_accuracy: 0.9963 - activation_433_categorical_accuracy: 0.9970 - val_loss: 0.5800 - val_activation_434_loss: 0.2356 - val_activation_433_loss: 0.3445 - val_activation_434_categorical_accuracy: 0.9596 - val_activation_433_categorical_accuracy: 0.9532 - lr: 5.0158e-04\n",
            "Epoch 100/101\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.0221 - activation_434_loss: 0.0153 - activation_433_loss: 0.0068 - activation_434_categorical_accuracy: 0.9941 - activation_433_categorical_accuracy: 0.9977 - val_loss: 0.7043 - val_activation_434_loss: 0.2871 - val_activation_433_loss: 0.4172 - val_activation_434_categorical_accuracy: 0.9535 - val_activation_433_categorical_accuracy: 0.9484 - lr: 4.9659e-04\n",
            "Epoch 101/101\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.0424 - activation_434_loss: 0.0273 - activation_433_loss: 0.0151 - activation_434_categorical_accuracy: 0.9912 - activation_433_categorical_accuracy: 0.9951 - val_loss: 0.3186 - val_activation_434_loss: 0.1454 - val_activation_433_loss: 0.1732 - val_activation_434_categorical_accuracy: 0.9586 - val_activation_433_categorical_accuracy: 0.9569 - lr: 4.9164e-04\n"
          ]
        }
      ],
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "num_classes = y_train.shape[-1]\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "print(X_train.shape,y_train.shape)\n",
        "\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 30:\n",
        "    return lr\n",
        "  else:\n",
        "     return lr * tf.math.exp(-0.01)\n",
        "\n",
        "dg = DataGenerator(X_train,y_train,batch_size=batch_size,input_shape=X_train.shape[1:])\n",
        "model = buildAE(X_train.shape[1:],y_train.shape[-1],learning_rate)\n",
        "# log = MyLogger(n=1, validation_data=(x_test,y_test), AE=model)\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "print('Params', model.count_params())\n",
        "\n",
        "history = model.fit(dg, epochs=101, verbose=1,callbacks = [lr_scheduler], validation_data=(x_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "RrM6sczAB-G7",
        "outputId": "d2f00b31-0869-401c-d74b-c8df58f972af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "93/93 [==============================] - 3s 8ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96       496\n",
            "           1       0.98      0.96      0.97       471\n",
            "           2       0.97      0.99      0.98       420\n",
            "           3       0.97      0.86      0.91       491\n",
            "           4       0.88      0.98      0.93       532\n",
            "           5       1.00      1.00      1.00       537\n",
            "\n",
            "    accuracy                           0.96      2947\n",
            "   macro avg       0.96      0.96      0.96      2947\n",
            "weighted avg       0.96      0.96      0.96      2947\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.save('Adaptive_Multikernel_ResNetV2_BI-LSTM.h5')\n",
        "\n",
        "model = tf.keras.models.load_model('/content/Adaptive_Multikernel_ResNetV2_BI-LSTM.h5')\n",
        "outs = model.predict(x_test)\n",
        "\n",
        "\n",
        "class_names= np.unique(np.argmax(y_test,axis=1))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(np.argmax(y_test,axis=1), np.argmax(outs[0],axis=1))\n",
        "\n",
        "print(classification_report(np.argmax(y_test,axis=1), np.argmax(outs[0],axis=1)))\n",
        "\n",
        "\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "# set the axis labels and title of the plot\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "plt.savefig('confusion_matrix.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAlRzsmk5DFv"
      },
      "source": [
        "##Multifilter_resnet_Bi-LSTM_self_attention 97.26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBkCgdjks1pI",
        "outputId": "998249e2-4733-48f3-9c1a-8b1a0fedee07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params 146892\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 9)]     0           []                               \n",
            "                                                                                                  \n",
            " zero_padding1d (ZeroPadding1D)  (None, 130, 9)      0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 128, 16)      448         ['zero_padding1d[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 128, 16)     64          ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 128, 16)      0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 63, 16)       0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 63, 16)       784         ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 63, 16)       1296        ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 63, 16)       1808        ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 63, 16)      64          ['conv1d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 63, 16)      64          ['conv1d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 63, 16)      64          ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 63, 16)       0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 63, 16)       0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 63, 16)       0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 63, 48)       0           ['activation_1[0][0]',           \n",
            "                                                                  'activation_2[0][0]',           \n",
            "                                                                  'activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 63, 16)       2320        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 63, 16)      64          ['conv1d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 63, 16)       0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 63, 16)       0           ['activation_4[0][0]',           \n",
            "                                                                  'max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 31, 16)       784         ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 31, 16)      64          ['conv1d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 31, 16)       0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 31, 16)       784         ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 31, 16)      64          ['conv1d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 31, 16)       0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 31, 32)       1568        ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 31, 32)       1568        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 31, 32)       0           ['conv1d_7[0][0]',               \n",
            "                                                                  'conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 31, 32)      128         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 31, 32)       0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 31, 16)       1552        ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 31, 16)      64          ['conv1d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 31, 16)       0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 31, 16)       784         ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 31, 16)      64          ['conv1d_10[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 31, 16)       0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 31, 32)       1568        ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 31, 32)       0           ['conv1d_11[0][0]',              \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 31, 32)       3104        ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 31, 32)       5152        ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 31, 32)       7200        ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 31, 32)      128         ['conv1d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 31, 32)      128         ['conv1d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 31, 32)      128         ['conv1d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 31, 32)       0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 31, 32)       0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 31, 32)       0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 31, 96)       0           ['activation_10[0][0]',          \n",
            "                                                                  'activation_11[0][0]',          \n",
            "                                                                  'activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 31, 32)       9248        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 31, 32)      128         ['conv1d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 31, 32)       0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 31, 32)       0           ['activation_13[0][0]',          \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 15, 32)       3104        ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 15, 32)      128         ['conv1d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 15, 32)       0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 15, 32)       3104        ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 15, 32)      128         ['conv1d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 15, 32)       0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 15, 64)       6208        ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 15, 64)       6208        ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 15, 64)       0           ['conv1d_18[0][0]',              \n",
            "                                                                  'conv1d_19[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 15, 64)      256         ['add_4[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 15, 64)       0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)             (None, 15, 32)       6176        ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 15, 32)      128         ['conv1d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 15, 32)       0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_21 (Conv1D)             (None, 15, 32)       3104        ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 15, 32)      128         ['conv1d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 15, 32)       0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_22 (Conv1D)             (None, 15, 64)       6208        ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 15, 64)       0           ['conv1d_22[0][0]',              \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 15, 64)      256         ['add_5[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 15, 64)       0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_23 (Conv1D)             (None, 15, 32)       6176        ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 15, 32)      128         ['conv1d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 15, 32)       0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_24 (Conv1D)             (None, 15, 32)       3104        ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 15, 32)      128         ['conv1d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 15, 32)       0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_25 (Conv1D)             (None, 15, 64)       6208        ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 15, 64)       0           ['conv1d_25[0][0]',              \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 15, 64)      256         ['add_6[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 15, 64)       0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 15, 64)       0           ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 15, 32)       10368       ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 15, 32)      128         ['bidirectional[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " self_attention (SelfAttention)  (None, 15, 32)      3168        ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 15, 32)       0           ['self_attention[0][0]']         \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 15, 64)      16640       ['dropout_1[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 15, 64)      256         ['bidirectional_1[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " self_attention_1 (SelfAttentio  (None, 15, 64)      12480       ['batch_normalization_24[0][0]'] \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 960)          0           ['self_attention_1[0][0]']       \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 960)          0           ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 6)            5766        ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 6)            5766        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " Output (Activation)            (None, 6)            0           ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " Opt_Output (Activation)        (None, 6)            0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 146,892\n",
            "Trainable params: 145,324\n",
            "Non-trainable params: 1,568\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def res_identity(x, filters): \n",
        "  #renet block where dimension doesnot change.\n",
        "  #The skip connection is just simple identity conncection\n",
        "  #we will have 3 blocks and then input will be added\n",
        "\n",
        "  x_skip = x # this will be used for addition with the residual block \n",
        "  f1, f2 = filters\n",
        "\n",
        "  #first block \n",
        "  #kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  \n",
        "\n",
        "  #second block # bottleneck (but size kept same with padding)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same', )(x)\n",
        "  \n",
        "\n",
        "  # third block activation used after adding the input\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.Conv1D(f2, kernel_size=3, strides=1, padding='same')(x)\n",
        "\n",
        "  # add the input \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def conv_skip(x, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=2, padding='valid')(x)\n",
        "  # when s = 2 then it is like downsizing the feature map\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  # x = tf.keras.layers.ZeroPadding1D(padding=(1,1))(x)\n",
        "  \n",
        "\n",
        "  # second block\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  #third block\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "  x = tf.keras.layers.Conv1D(f2, kernel_size=3, strides=1, padding='same')(x)\n",
        "  # x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "\n",
        "  # shortcut \n",
        "  x_skip = tf.keras.layers.Conv1D(f2, kernel_size=3, strides=2, padding='valid')(x_skip)\n",
        "  # x_skip = tf.keras.layers.MaxPool1D(pool_size=3,strides=2)(x_skip)\n",
        "  # x_skip = tf.keras.layers.BatchNormalization()(x_skip)\n",
        "\n",
        "  # add \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def multi_fiter_conv(x, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x1 = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "  x1 = tf.keras.layers.Activation(tf.keras.activations.relu)(x1)\n",
        "  \n",
        "\n",
        "  # second block\n",
        "  x2 = tf.keras.layers.Conv1D(f1, kernel_size=5, strides=1, padding='same')(x)\n",
        "  x2 = tf.keras.layers.BatchNormalization()(x2)\n",
        "  x2 = tf.keras.layers.Activation(tf.keras.activations.relu)(x2)\n",
        "\n",
        "  #third block\n",
        "  x3 = tf.keras.layers.Conv1D(f1, kernel_size=7, strides=1, padding='same')(x)\n",
        "  x3 = tf.keras.layers.BatchNormalization()(x3)\n",
        "  x3 = tf.keras.layers.Activation(tf.keras.activations.relu)(x3)\n",
        "\n",
        "\n",
        "  # # forth block\n",
        "  # x4 = tf.keras.layers.Conv1D(f1, kernel_size=9, strides=1, padding='same')(x)\n",
        "  # x4 = tf.keras.layers.BatchNormalization()(x4)\n",
        "  # x4 = tf.keras.layers.Activation(tf.keras.activations.relu)(x4)\n",
        "\n",
        "  # concatenate \n",
        "  x = tf.keras.layers.Concatenate()([x1,x2,x3])\n",
        "  x = tf.keras.layers.Conv1D(f2,kernel_size=3,strides=1,padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  #add\n",
        "  x = tf.keras.layers.Add()([x,x_skip])\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "\n",
        "class SelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_units):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_units = num_units\n",
        "        self.WQ = tf.keras.layers.Dense(units=num_units)\n",
        "        self.WK = tf.keras.layers.Dense(units=num_units)\n",
        "        self.WV = tf.keras.layers.Dense(units=num_units)\n",
        "        self.softmax = tf.keras.layers.Softmax()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Q = self.WQ(inputs)\n",
        "        K = self.WK(inputs)\n",
        "        V = self.WV(inputs)\n",
        "        attention_logits = tf.matmul(Q, K, transpose_b=True)\n",
        "        attention_weights = self.softmax(attention_logits)\n",
        "        attention_output = tf.matmul(attention_weights, V)\n",
        "        return attention_output\n",
        "\n",
        "def buildAE(input_shape,classes,learning_rate):\n",
        "\n",
        "    input = tf.keras.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.ZeroPadding1D(padding=(1,1))(input)\n",
        "    x = tf.keras.layers.Conv1D(16, kernel_size=3, strides=1,padding='valid',)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)(x)\n",
        "\n",
        "    x = multi_fiter_conv(x,filters=(16,16))\n",
        "\n",
        "    x = conv_skip(x ,filters=(16,32))\n",
        "    # print(x.shape)\n",
        "    x = res_identity(x,filters=(16,32))\n",
        "    x = multi_fiter_conv(x,filters=(32,32))\n",
        "    x = conv_skip(x ,filters=(32,64))\n",
        "    x = res_identity(x,filters=(32,64))\n",
        "    x = res_identity(x,filters=(32,64))\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "    # x = maxpool_skip(x,filters=(16,16))\n",
        "    \n",
        "    # output_opt = SelfAttention(32)(x)\n",
        "    output_opt = tf.keras.layers.Flatten()(x)\n",
        "    # output_opt = tf.keras.layers.Dense(units=64)(output_opt)\n",
        "    output_opt = tf.keras.layers.Dense(units=classes)(output_opt)\n",
        "    output_opt = tf.keras.layers.Activation('softmax',name='Opt_Output')(output_opt)\n",
        "\n",
        "\n",
        "\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=16,activation='tanh',return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = SelfAttention(num_units=32)(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=32,activation='tanh',return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = SelfAttention(num_units=64)(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    \n",
        "\n",
        "    # x = tf.keras.layers.Dense(units=64)(x)\n",
        "    x = tf.keras.layers.Dense(units=classes)(x)\n",
        "    output = tf.keras.layers.Activation('softmax',name='Output')(x)\n",
        "\n",
        "\n",
        "    # model = tf.keras.Model(input,output)\n",
        "\n",
        "    pred_model = tf.keras.Model(input,output)\n",
        "    model = tf.keras.Model(input,[output,output_opt])\n",
        "\n",
        "\n",
        "    \n",
        "    print('Params', model.count_params())\n",
        "    model.compile(loss = [tf.keras.losses.CategoricalCrossentropy(),\n",
        "                          tf.keras.losses.CategoricalCrossentropy()],\n",
        "                  metrics=[tf.keras.metrics.CategoricalAccuracy(),],\n",
        "                   optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
        "\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    return model, pred_model\n",
        "model = buildAE((128,9),6,0.0005)\n",
        "# tf.keras.utils.plot_model(model,show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4KWDNyU-8IA",
        "outputId": "3ea25e4d-09ee-43a7-ac41-70aa48636ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(36300, 128, 6) (36300, 18)\n",
            "Params 169812\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 128, 6)]     0           []                               \n",
            "                                                                                                  \n",
            " zero_padding1d_1 (ZeroPadding1  (None, 130, 6)      0           ['input_2[0][0]']                \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv1d_26 (Conv1D)             (None, 128, 16)      304         ['zero_padding1d_1[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 128, 16)     64          ['conv1d_26[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 128, 16)      0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 63, 16)      0           ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_27 (Conv1D)             (None, 63, 16)       784         ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_28 (Conv1D)             (None, 63, 16)       1296        ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_29 (Conv1D)             (None, 63, 16)       1808        ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 63, 16)      64          ['conv1d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 63, 16)      64          ['conv1d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 63, 16)      64          ['conv1d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 63, 16)       0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 63, 16)       0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 63, 16)       0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 63, 48)       0           ['activation_24[0][0]',          \n",
            "                                                                  'activation_25[0][0]',          \n",
            "                                                                  'activation_26[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_30 (Conv1D)             (None, 63, 16)       2320        ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 63, 16)      64          ['conv1d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 63, 16)       0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 63, 16)       0           ['activation_27[0][0]',          \n",
            "                                                                  'max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_31 (Conv1D)             (None, 31, 16)       784         ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 31, 16)      64          ['conv1d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 31, 16)       0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_32 (Conv1D)             (None, 31, 16)       784         ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 31, 16)      64          ['conv1d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 31, 16)       0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_33 (Conv1D)             (None, 31, 32)       1568        ['activation_29[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_34 (Conv1D)             (None, 31, 32)       1568        ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 31, 32)       0           ['conv1d_33[0][0]',              \n",
            "                                                                  'conv1d_34[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 31, 32)      128         ['add_8[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 31, 32)       0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_35 (Conv1D)             (None, 31, 16)       1552        ['activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 31, 16)      64          ['conv1d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 31, 16)       0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_36 (Conv1D)             (None, 31, 16)       784         ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 31, 16)      64          ['conv1d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 31, 16)       0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_37 (Conv1D)             (None, 31, 32)       1568        ['activation_32[0][0]']          \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 31, 32)       0           ['conv1d_37[0][0]',              \n",
            "                                                                  'add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_38 (Conv1D)             (None, 31, 32)       3104        ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_39 (Conv1D)             (None, 31, 32)       5152        ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_40 (Conv1D)             (None, 31, 32)       7200        ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 31, 32)      128         ['conv1d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 31, 32)      128         ['conv1d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 31, 32)      128         ['conv1d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 31, 32)       0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 31, 32)       0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 31, 32)       0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 31, 96)       0           ['activation_33[0][0]',          \n",
            "                                                                  'activation_34[0][0]',          \n",
            "                                                                  'activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_41 (Conv1D)             (None, 31, 32)       9248        ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 31, 32)      128         ['conv1d_41[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 31, 32)       0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 31, 32)       0           ['activation_36[0][0]',          \n",
            "                                                                  'add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_42 (Conv1D)             (None, 15, 32)       3104        ['add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 15, 32)      128         ['conv1d_42[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 15, 32)       0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_43 (Conv1D)             (None, 15, 32)       3104        ['activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 15, 32)      128         ['conv1d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 15, 32)       0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_44 (Conv1D)             (None, 15, 64)       6208        ['activation_38[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_45 (Conv1D)             (None, 15, 64)       6208        ['add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 15, 64)       0           ['conv1d_44[0][0]',              \n",
            "                                                                  'conv1d_45[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 15, 64)      256         ['add_11[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 15, 64)       0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_46 (Conv1D)             (None, 15, 32)       6176        ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 15, 32)      128         ['conv1d_46[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 15, 32)       0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_47 (Conv1D)             (None, 15, 32)       3104        ['activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 15, 32)      128         ['conv1d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 15, 32)       0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_48 (Conv1D)             (None, 15, 64)       6208        ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 15, 64)       0           ['conv1d_48[0][0]',              \n",
            "                                                                  'add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 15, 64)      256         ['add_12[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 15, 64)       0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_49 (Conv1D)             (None, 15, 32)       6176        ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 15, 32)      128         ['conv1d_49[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 15, 32)       0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_50 (Conv1D)             (None, 15, 32)       3104        ['activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 15, 32)      128         ['conv1d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 15, 32)       0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_51 (Conv1D)             (None, 15, 64)       6208        ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 15, 64)       0           ['conv1d_51[0][0]',              \n",
            "                                                                  'add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 15, 64)      256         ['add_13[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 15, 64)       0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 15, 64)       0           ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirectional  (None, 15, 32)      10368       ['dropout_2[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 15, 32)      128         ['bidirectional_2[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " self_attention_2 (SelfAttentio  (None, 15, 32)      3168        ['batch_normalization_48[0][0]'] \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 15, 32)       0           ['self_attention_2[0][0]']       \n",
            "                                                                                                  \n",
            " bidirectional_3 (Bidirectional  (None, 15, 64)      16640       ['dropout_3[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 15, 64)      256         ['bidirectional_3[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " self_attention_3 (SelfAttentio  (None, 15, 64)      12480       ['batch_normalization_49[0][0]'] \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 960)          0           ['self_attention_3[0][0]']       \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 960)          0           ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 18)           17298       ['flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 18)           17298       ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " Output (Activation)            (None, 18)           0           ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " Opt_Output (Activation)        (None, 18)           0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 169,812\n",
            "Trainable params: 168,244\n",
            "Non-trainable params: 1,568\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/251\n",
            "305/567 [===============>..............] - ETA: 32s - loss: 6.3162 - Output_loss: 3.1131 - Opt_Output_loss: 3.2031 - Output_categorical_accuracy: 0.0599 - Opt_Output_categorical_accuracy: 0.0776"
          ]
        }
      ],
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "num_classes = y_train.shape[-1]\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "print(X_train.shape,y_train.shape)\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 30:\n",
        "    return lr\n",
        "  else:\n",
        "     return lr * tf.math.exp(-0.01)\n",
        "\n",
        "dg = DataGenerator(X_train,y_train,batch_size=batch_size,input_shape=X_train.shape[1:])\n",
        "model,pred_model = buildAE(X_train.shape[1:],y_train.shape[-1],learning_rate)\n",
        "# log = MyLogger(n=1, validation_data=(x_test,y_test), AE=model)\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "history = model.fit(dg, epochs=251, verbose=1,callbacks = [lr_scheduler], validation_data=(x_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "XHHAW1JM9i5n",
        "outputId": "13c96748-a305-4829-e606-cddd3fc6d06f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "93/93 [==============================] - 5s 23ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.97       496\n",
            "           1       0.99      0.96      0.98       471\n",
            "           2       0.98      1.00      0.99       420\n",
            "           3       0.94      0.93      0.94       491\n",
            "           4       0.95      0.98      0.97       532\n",
            "           5       0.99      1.00      0.99       537\n",
            "\n",
            "    accuracy                           0.97      2947\n",
            "   macro avg       0.97      0.97      0.97      2947\n",
            "weighted avg       0.97      0.97      0.97      2947\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.save('Adaptive_Multikernel_ResNetV2_BI-LSTM_Attention.h5')\n",
        "\n",
        "# model = load_model('/content/Adaptive_Multikernel_ResNetV2_BI-LSTM_Attention.h5', custom_objects={'AttentionLayer': AttentionLayer})\n",
        "\n",
        "model = tf.keras.models.load_model('/content/Adaptive_Multikernel_ResNetV2_BI-LSTM_Attention.h5',\n",
        "                                   custom_objects= {'SelfAttention':SelfAttention})\n",
        "outs = model.predict(x_test)\n",
        "\n",
        "\n",
        "class_names= np.unique(np.argmax(y_test,axis=1))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(np.argmax(y_test,axis=1), np.argmax(outs[0],axis=1))\n",
        "\n",
        "print(classification_report(np.argmax(y_test,axis=1), np.argmax(outs[0],axis=1)))\n",
        "\n",
        "\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "# set the axis labels and title of the plot\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "plt.savefig('confusion_matrix.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3EwpQSlZ7zR"
      },
      "source": [
        "##extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKkiYjbFYqVW",
        "outputId": "603a8218-560c-4b5d-a4e8-bc26efd7f15b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7352, 128, 9) (7352, 6)\n",
            "Params 158172\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 128, 9)]     0           []                               \n",
            "                                                                                                  \n",
            " zero_padding1d_2 (ZeroPadding1  (None, 130, 9)      0           ['input_3[0][0]']                \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv1d_52 (Conv1D)             (None, 128, 16)      448         ['zero_padding1d_2[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 128, 16)     64          ['conv1d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 128, 16)      0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 63, 16)      0           ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_53 (Conv1D)             (None, 63, 16)       784         ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_54 (Conv1D)             (None, 63, 16)       1296        ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_55 (Conv1D)             (None, 63, 16)       1808        ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 63, 16)      64          ['conv1d_53[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 63, 16)      64          ['conv1d_54[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 63, 16)      64          ['conv1d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 63, 16)       0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 63, 16)       0           ['batch_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 63, 16)       0           ['batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 63, 48)       0           ['activation_47[0][0]',          \n",
            "                                                                  'activation_48[0][0]',          \n",
            "                                                                  'activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_56 (Conv1D)             (None, 31, 16)       2320        ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 31, 16)      64          ['conv1d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 31, 16)       0           ['batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_57 (Conv1D)             (None, 31, 16)       784         ['activation_50[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, 31, 16)      64          ['conv1d_57[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 31, 16)       0           ['batch_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_58 (Conv1D)             (None, 31, 32)       1568        ['activation_51[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_59 (Conv1D)             (None, 31, 32)       4640        ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 31, 32)       0           ['conv1d_58[0][0]',              \n",
            "                                                                  'conv1d_59[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, 31, 32)      128         ['add_14[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, 31, 32)       0           ['batch_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_60 (Conv1D)             (None, 31, 16)       1552        ['activation_52[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, 31, 16)      64          ['conv1d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_53 (Activation)     (None, 31, 16)       0           ['batch_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_61 (Conv1D)             (None, 31, 16)       784         ['activation_53[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, 31, 16)      64          ['conv1d_61[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_54 (Activation)     (None, 31, 16)       0           ['batch_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_62 (Conv1D)             (None, 31, 32)       1568        ['activation_54[0][0]']          \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 31, 32)       0           ['conv1d_62[0][0]',              \n",
            "                                                                  'add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_63 (Conv1D)             (None, 31, 32)       3104        ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_64 (Conv1D)             (None, 31, 32)       5152        ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_65 (Conv1D)             (None, 31, 32)       7200        ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_59 (BatchN  (None, 31, 32)      128         ['conv1d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_60 (BatchN  (None, 31, 32)      128         ['conv1d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_61 (BatchN  (None, 31, 32)      128         ['conv1d_65[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_55 (Activation)     (None, 31, 32)       0           ['batch_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " activation_56 (Activation)     (None, 31, 32)       0           ['batch_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " activation_57 (Activation)     (None, 31, 32)       0           ['batch_normalization_61[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 31, 96)       0           ['activation_55[0][0]',          \n",
            "                                                                  'activation_56[0][0]',          \n",
            "                                                                  'activation_57[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_66 (Conv1D)             (None, 15, 32)       9248        ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_62 (BatchN  (None, 15, 32)      128         ['conv1d_66[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_58 (Activation)     (None, 15, 32)       0           ['batch_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_67 (Conv1D)             (None, 15, 32)       3104        ['activation_58[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_63 (BatchN  (None, 15, 32)      128         ['conv1d_67[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_59 (Activation)     (None, 15, 32)       0           ['batch_normalization_63[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_68 (Conv1D)             (None, 15, 64)       6208        ['activation_59[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_69 (Conv1D)             (None, 15, 64)       18496       ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 15, 64)       0           ['conv1d_68[0][0]',              \n",
            "                                                                  'conv1d_69[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_64 (BatchN  (None, 15, 64)      256         ['add_16[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_60 (Activation)     (None, 15, 64)       0           ['batch_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_70 (Conv1D)             (None, 15, 32)       6176        ['activation_60[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_65 (BatchN  (None, 15, 32)      128         ['conv1d_70[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_61 (Activation)     (None, 15, 32)       0           ['batch_normalization_65[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_71 (Conv1D)             (None, 15, 32)       3104        ['activation_61[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, 15, 32)      128         ['conv1d_71[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, 15, 32)       0           ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_72 (Conv1D)             (None, 15, 64)       6208        ['activation_62[0][0]']          \n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 15, 64)       0           ['conv1d_72[0][0]',              \n",
            "                                                                  'add_16[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, 15, 64)      256         ['add_17[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, 15, 64)       0           ['batch_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_73 (Conv1D)             (None, 15, 32)       6176        ['activation_63[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, 15, 32)      128         ['conv1d_73[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_64 (Activation)     (None, 15, 32)       0           ['batch_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_74 (Conv1D)             (None, 15, 32)       3104        ['activation_64[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_69 (BatchN  (None, 15, 32)      128         ['conv1d_74[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_65 (Activation)     (None, 15, 32)       0           ['batch_normalization_69[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_75 (Conv1D)             (None, 15, 64)       6208        ['activation_65[0][0]']          \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 15, 64)       0           ['conv1d_75[0][0]',              \n",
            "                                                                  'add_17[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_70 (BatchN  (None, 15, 64)      256         ['add_18[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_66 (Activation)     (None, 15, 64)       0           ['batch_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 15, 64)       0           ['activation_66[0][0]']          \n",
            "                                                                                                  \n",
            " bidirectional_4 (Bidirectional  (None, 15, 32)      10368       ['dropout_4[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_71 (BatchN  (None, 15, 32)      128         ['bidirectional_4[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " self_attention_4 (SelfAttentio  (None, 15, 32)      3168        ['batch_normalization_71[0][0]'] \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 15, 32)       0           ['self_attention_4[0][0]']       \n",
            "                                                                                                  \n",
            " bidirectional_5 (Bidirectional  (None, 15, 64)      16640       ['dropout_5[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_72 (BatchN  (None, 15, 64)      256         ['bidirectional_5[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " self_attention_5 (SelfAttentio  (None, 15, 64)      12480       ['batch_normalization_72[0][0]'] \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 960)          0           ['self_attention_5[0][0]']       \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 960)          0           ['activation_66[0][0]']          \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 6)            5766        ['flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 6)            5766        ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " Output (Activation)            (None, 6)            0           ['dense_23[0][0]']               \n",
            "                                                                                                  \n",
            " Opt_Output (Activation)        (None, 6)            0           ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 158,172\n",
            "Trainable params: 156,700\n",
            "Non-trainable params: 1,472\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/151\n",
            "114/114 [==============================] - 39s 74ms/step - loss: 2.8473 - Output_loss: 1.5184 - Opt_Output_loss: 1.3289 - Output_categorical_accuracy: 0.4563 - Opt_Output_categorical_accuracy: 0.5069 - val_loss: 3.3531 - val_Output_loss: 1.6639 - val_Opt_Output_loss: 1.6892 - val_Output_categorical_accuracy: 0.3210 - val_Opt_Output_categorical_accuracy: 0.3271 - lr: 0.0010\n",
            "Epoch 2/151\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.8479 - Output_loss: 0.4611 - Opt_Output_loss: 0.3868 - Output_categorical_accuracy: 0.8159 - Opt_Output_categorical_accuracy: 0.8561 - val_loss: 1.6769 - val_Output_loss: 0.9229 - val_Opt_Output_loss: 0.7540 - val_Output_categorical_accuracy: 0.6630 - val_Opt_Output_categorical_accuracy: 0.7234 - lr: 0.0010\n",
            "Epoch 3/151\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.3897 - Output_loss: 0.2177 - Opt_Output_loss: 0.1719 - Output_categorical_accuracy: 0.9123 - Opt_Output_categorical_accuracy: 0.9339 - val_loss: 0.5915 - val_Output_loss: 0.2884 - val_Opt_Output_loss: 0.3031 - val_Output_categorical_accuracy: 0.8958 - val_Opt_Output_categorical_accuracy: 0.8911 - lr: 0.0010\n",
            "Epoch 4/151\n",
            "114/114 [==============================] - 7s 57ms/step - loss: 0.3226 - Output_loss: 0.1808 - Opt_Output_loss: 0.1418 - Output_categorical_accuracy: 0.9257 - Opt_Output_categorical_accuracy: 0.9433 - val_loss: 0.5797 - val_Output_loss: 0.2993 - val_Opt_Output_loss: 0.2804 - val_Output_categorical_accuracy: 0.8901 - val_Opt_Output_categorical_accuracy: 0.8850 - lr: 0.0010\n",
            "Epoch 5/151\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.2820 - Output_loss: 0.1542 - Opt_Output_loss: 0.1277 - Output_categorical_accuracy: 0.9349 - Opt_Output_categorical_accuracy: 0.9454 - val_loss: 0.5812 - val_Output_loss: 0.3129 - val_Opt_Output_loss: 0.2683 - val_Output_categorical_accuracy: 0.9023 - val_Opt_Output_categorical_accuracy: 0.9060 - lr: 0.0010\n",
            "Epoch 6/151\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.2520 - Output_loss: 0.1380 - Opt_Output_loss: 0.1139 - Output_categorical_accuracy: 0.9439 - Opt_Output_categorical_accuracy: 0.9545 - val_loss: 0.6091 - val_Output_loss: 0.3322 - val_Opt_Output_loss: 0.2769 - val_Output_categorical_accuracy: 0.9002 - val_Opt_Output_categorical_accuracy: 0.8989 - lr: 0.0010\n",
            "Epoch 7/151\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.2404 - Output_loss: 0.1277 - Opt_Output_loss: 0.1127 - Output_categorical_accuracy: 0.9439 - Opt_Output_categorical_accuracy: 0.9501 - val_loss: 0.5691 - val_Output_loss: 0.3111 - val_Opt_Output_loss: 0.2581 - val_Output_categorical_accuracy: 0.9121 - val_Opt_Output_categorical_accuracy: 0.9077 - lr: 0.0010\n",
            "Epoch 8/151\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.2380 - Output_loss: 0.1282 - Opt_Output_loss: 0.1098 - Output_categorical_accuracy: 0.9439 - Opt_Output_categorical_accuracy: 0.9544 - val_loss: 0.7277 - val_Output_loss: 0.4045 - val_Opt_Output_loss: 0.3233 - val_Output_categorical_accuracy: 0.9097 - val_Opt_Output_categorical_accuracy: 0.9172 - lr: 0.0010\n",
            "Epoch 9/151\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.2312 - Output_loss: 0.1251 - Opt_Output_loss: 0.1060 - Output_categorical_accuracy: 0.9482 - Opt_Output_categorical_accuracy: 0.9552 - val_loss: 0.6828 - val_Output_loss: 0.3864 - val_Opt_Output_loss: 0.2964 - val_Output_categorical_accuracy: 0.9104 - val_Opt_Output_categorical_accuracy: 0.9050 - lr: 0.0010\n",
            "Epoch 10/151\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.2312 - Output_loss: 0.1255 - Opt_Output_loss: 0.1057 - Output_categorical_accuracy: 0.9471 - Opt_Output_categorical_accuracy: 0.9529 - val_loss: 0.5593 - val_Output_loss: 0.2888 - val_Opt_Output_loss: 0.2706 - val_Output_categorical_accuracy: 0.9196 - val_Opt_Output_categorical_accuracy: 0.9196 - lr: 0.0010\n",
            "Epoch 11/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.2400 - Output_loss: 0.1313 - Opt_Output_loss: 0.1087 - Output_categorical_accuracy: 0.9479 - Opt_Output_categorical_accuracy: 0.9541 - val_loss: 0.6971 - val_Output_loss: 0.3787 - val_Opt_Output_loss: 0.3184 - val_Output_categorical_accuracy: 0.9057 - val_Opt_Output_categorical_accuracy: 0.9036 - lr: 0.0010\n",
            "Epoch 12/151\n",
            "114/114 [==============================] - 7s 57ms/step - loss: 0.2317 - Output_loss: 0.1284 - Opt_Output_loss: 0.1033 - Output_categorical_accuracy: 0.9420 - Opt_Output_categorical_accuracy: 0.9523 - val_loss: 0.6761 - val_Output_loss: 0.3627 - val_Opt_Output_loss: 0.3133 - val_Output_categorical_accuracy: 0.9009 - val_Opt_Output_categorical_accuracy: 0.9026 - lr: 0.0010\n",
            "Epoch 13/151\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.2317 - Output_loss: 0.1256 - Opt_Output_loss: 0.1061 - Output_categorical_accuracy: 0.9426 - Opt_Output_categorical_accuracy: 0.9515 - val_loss: 1.0029 - val_Output_loss: 0.5055 - val_Opt_Output_loss: 0.4973 - val_Output_categorical_accuracy: 0.8962 - val_Opt_Output_categorical_accuracy: 0.9002 - lr: 0.0010\n",
            "Epoch 14/151\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.2282 - Output_loss: 0.1243 - Opt_Output_loss: 0.1039 - Output_categorical_accuracy: 0.9446 - Opt_Output_categorical_accuracy: 0.9534 - val_loss: 0.6471 - val_Output_loss: 0.3311 - val_Opt_Output_loss: 0.3160 - val_Output_categorical_accuracy: 0.9101 - val_Opt_Output_categorical_accuracy: 0.9118 - lr: 0.0010\n",
            "Epoch 15/151\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.2099 - Output_loss: 0.1123 - Opt_Output_loss: 0.0976 - Output_categorical_accuracy: 0.9519 - Opt_Output_categorical_accuracy: 0.9552 - val_loss: 0.7227 - val_Output_loss: 0.3632 - val_Opt_Output_loss: 0.3595 - val_Output_categorical_accuracy: 0.9053 - val_Opt_Output_categorical_accuracy: 0.8989 - lr: 0.0010\n",
            "Epoch 16/151\n",
            "114/114 [==============================] - 7s 57ms/step - loss: 0.2107 - Output_loss: 0.1113 - Opt_Output_loss: 0.0993 - Output_categorical_accuracy: 0.9526 - Opt_Output_categorical_accuracy: 0.9572 - val_loss: 0.8711 - val_Output_loss: 0.4553 - val_Opt_Output_loss: 0.4158 - val_Output_categorical_accuracy: 0.9145 - val_Opt_Output_categorical_accuracy: 0.9155 - lr: 0.0010\n",
            "Epoch 17/151\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.2138 - Output_loss: 0.1136 - Opt_Output_loss: 0.1001 - Output_categorical_accuracy: 0.9523 - Opt_Output_categorical_accuracy: 0.9553 - val_loss: 0.6767 - val_Output_loss: 0.3504 - val_Opt_Output_loss: 0.3263 - val_Output_categorical_accuracy: 0.9128 - val_Opt_Output_categorical_accuracy: 0.9172 - lr: 0.0010\n",
            "Epoch 18/151\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.2125 - Output_loss: 0.1141 - Opt_Output_loss: 0.0984 - Output_categorical_accuracy: 0.9494 - Opt_Output_categorical_accuracy: 0.9556 - val_loss: 0.6310 - val_Output_loss: 0.3161 - val_Opt_Output_loss: 0.3149 - val_Output_categorical_accuracy: 0.9192 - val_Opt_Output_categorical_accuracy: 0.9175 - lr: 0.0010\n",
            "Epoch 19/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.1957 - Output_loss: 0.1024 - Opt_Output_loss: 0.0934 - Output_categorical_accuracy: 0.9519 - Opt_Output_categorical_accuracy: 0.9567 - val_loss: 0.8250 - val_Output_loss: 0.4411 - val_Opt_Output_loss: 0.3839 - val_Output_categorical_accuracy: 0.9084 - val_Opt_Output_categorical_accuracy: 0.9091 - lr: 0.0010\n",
            "Epoch 20/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.2058 - Output_loss: 0.1100 - Opt_Output_loss: 0.0958 - Output_categorical_accuracy: 0.9538 - Opt_Output_categorical_accuracy: 0.9581 - val_loss: 0.7085 - val_Output_loss: 0.3747 - val_Opt_Output_loss: 0.3338 - val_Output_categorical_accuracy: 0.9135 - val_Opt_Output_categorical_accuracy: 0.9138 - lr: 0.0010\n",
            "Epoch 21/151\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.2096 - Output_loss: 0.1108 - Opt_Output_loss: 0.0988 - Output_categorical_accuracy: 0.9529 - Opt_Output_categorical_accuracy: 0.9571 - val_loss: 0.7581 - val_Output_loss: 0.4442 - val_Opt_Output_loss: 0.3139 - val_Output_categorical_accuracy: 0.9104 - val_Opt_Output_categorical_accuracy: 0.9158 - lr: 0.0010\n",
            "Epoch 22/151\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 0.1998 - Output_loss: 0.1078 - Opt_Output_loss: 0.0920 - Output_categorical_accuracy: 0.9507 - Opt_Output_categorical_accuracy: 0.9574 - val_loss: 0.5575 - val_Output_loss: 0.3033 - val_Opt_Output_loss: 0.2542 - val_Output_categorical_accuracy: 0.9223 - val_Opt_Output_categorical_accuracy: 0.9223 - lr: 0.0010\n",
            "Epoch 23/151\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.2095 - Output_loss: 0.1160 - Opt_Output_loss: 0.0935 - Output_categorical_accuracy: 0.9538 - Opt_Output_categorical_accuracy: 0.9601 - val_loss: 0.5569 - val_Output_loss: 0.2798 - val_Opt_Output_loss: 0.2771 - val_Output_categorical_accuracy: 0.9284 - val_Opt_Output_categorical_accuracy: 0.9270 - lr: 0.0010\n",
            "Epoch 24/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.2028 - Output_loss: 0.1092 - Opt_Output_loss: 0.0936 - Output_categorical_accuracy: 0.9530 - Opt_Output_categorical_accuracy: 0.9578 - val_loss: 0.5643 - val_Output_loss: 0.2961 - val_Opt_Output_loss: 0.2682 - val_Output_categorical_accuracy: 0.9192 - val_Opt_Output_categorical_accuracy: 0.9155 - lr: 0.0010\n",
            "Epoch 25/151\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.2298 - Output_loss: 0.1267 - Opt_Output_loss: 0.1032 - Output_categorical_accuracy: 0.9454 - Opt_Output_categorical_accuracy: 0.9526 - val_loss: 0.7611 - val_Output_loss: 0.3430 - val_Opt_Output_loss: 0.4182 - val_Output_categorical_accuracy: 0.9097 - val_Opt_Output_categorical_accuracy: 0.9080 - lr: 0.0010\n",
            "Epoch 26/151\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.1984 - Output_loss: 0.1042 - Opt_Output_loss: 0.0942 - Output_categorical_accuracy: 0.9539 - Opt_Output_categorical_accuracy: 0.9593 - val_loss: 0.7870 - val_Output_loss: 0.3741 - val_Opt_Output_loss: 0.4130 - val_Output_categorical_accuracy: 0.9108 - val_Opt_Output_categorical_accuracy: 0.9057 - lr: 0.0010\n",
            "Epoch 27/151\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.1921 - Output_loss: 0.1037 - Opt_Output_loss: 0.0884 - Output_categorical_accuracy: 0.9550 - Opt_Output_categorical_accuracy: 0.9613 - val_loss: 0.5668 - val_Output_loss: 0.2658 - val_Opt_Output_loss: 0.3010 - val_Output_categorical_accuracy: 0.9203 - val_Opt_Output_categorical_accuracy: 0.9165 - lr: 0.0010\n",
            "Epoch 28/151\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.1854 - Output_loss: 0.0983 - Opt_Output_loss: 0.0871 - Output_categorical_accuracy: 0.9553 - Opt_Output_categorical_accuracy: 0.9605 - val_loss: 0.7074 - val_Output_loss: 0.3238 - val_Opt_Output_loss: 0.3836 - val_Output_categorical_accuracy: 0.9084 - val_Opt_Output_categorical_accuracy: 0.9077 - lr: 0.0010\n",
            "Epoch 29/151\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.1886 - Output_loss: 0.1007 - Opt_Output_loss: 0.0879 - Output_categorical_accuracy: 0.9592 - Opt_Output_categorical_accuracy: 0.9619 - val_loss: 0.4801 - val_Output_loss: 0.2589 - val_Opt_Output_loss: 0.2213 - val_Output_categorical_accuracy: 0.9213 - val_Opt_Output_categorical_accuracy: 0.9237 - lr: 0.0010\n",
            "Epoch 30/151\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.1960 - Output_loss: 0.1037 - Opt_Output_loss: 0.0923 - Output_categorical_accuracy: 0.9516 - Opt_Output_categorical_accuracy: 0.9581 - val_loss: 0.6110 - val_Output_loss: 0.3258 - val_Opt_Output_loss: 0.2852 - val_Output_categorical_accuracy: 0.9230 - val_Opt_Output_categorical_accuracy: 0.9253 - lr: 0.0010\n",
            "Epoch 31/151\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.1868 - Output_loss: 0.1024 - Opt_Output_loss: 0.0844 - Output_categorical_accuracy: 0.9574 - Opt_Output_categorical_accuracy: 0.9622 - val_loss: 0.7635 - val_Output_loss: 0.3851 - val_Opt_Output_loss: 0.3783 - val_Output_categorical_accuracy: 0.8985 - val_Opt_Output_categorical_accuracy: 0.9036 - lr: 9.9005e-04\n",
            "Epoch 32/151\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.1973 - Output_loss: 0.1066 - Opt_Output_loss: 0.0907 - Output_categorical_accuracy: 0.9548 - Opt_Output_categorical_accuracy: 0.9578 - val_loss: 0.7449 - val_Output_loss: 0.3717 - val_Opt_Output_loss: 0.3733 - val_Output_categorical_accuracy: 0.9118 - val_Opt_Output_categorical_accuracy: 0.9145 - lr: 9.8020e-04\n",
            "Epoch 33/151\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.1853 - Output_loss: 0.0991 - Opt_Output_loss: 0.0862 - Output_categorical_accuracy: 0.9605 - Opt_Output_categorical_accuracy: 0.9631 - val_loss: 0.7778 - val_Output_loss: 0.4005 - val_Opt_Output_loss: 0.3772 - val_Output_categorical_accuracy: 0.9165 - val_Opt_Output_categorical_accuracy: 0.9165 - lr: 9.7045e-04\n",
            "Epoch 34/151\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.1969 - Output_loss: 0.1076 - Opt_Output_loss: 0.0894 - Output_categorical_accuracy: 0.9555 - Opt_Output_categorical_accuracy: 0.9589 - val_loss: 0.7759 - val_Output_loss: 0.3939 - val_Opt_Output_loss: 0.3820 - val_Output_categorical_accuracy: 0.9223 - val_Opt_Output_categorical_accuracy: 0.9257 - lr: 9.6079e-04\n",
            "Epoch 35/151\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.2015 - Output_loss: 0.1075 - Opt_Output_loss: 0.0940 - Output_categorical_accuracy: 0.9526 - Opt_Output_categorical_accuracy: 0.9586 - val_loss: 0.6595 - val_Output_loss: 0.3453 - val_Opt_Output_loss: 0.3143 - val_Output_categorical_accuracy: 0.9226 - val_Opt_Output_categorical_accuracy: 0.9206 - lr: 9.5123e-04\n",
            "Epoch 36/151\n",
            "114/114 [==============================] - 7s 64ms/step - loss: 0.1895 - Output_loss: 0.1018 - Opt_Output_loss: 0.0878 - Output_categorical_accuracy: 0.9574 - Opt_Output_categorical_accuracy: 0.9619 - val_loss: 0.7995 - val_Output_loss: 0.4431 - val_Opt_Output_loss: 0.3564 - val_Output_categorical_accuracy: 0.8996 - val_Opt_Output_categorical_accuracy: 0.9060 - lr: 9.4176e-04\n",
            "Epoch 37/151\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.1862 - Output_loss: 0.0994 - Opt_Output_loss: 0.0868 - Output_categorical_accuracy: 0.9570 - Opt_Output_categorical_accuracy: 0.9605 - val_loss: 0.7734 - val_Output_loss: 0.4218 - val_Opt_Output_loss: 0.3516 - val_Output_categorical_accuracy: 0.9182 - val_Opt_Output_categorical_accuracy: 0.9203 - lr: 9.3239e-04\n",
            "Epoch 38/151\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.2017 - Output_loss: 0.1125 - Opt_Output_loss: 0.0892 - Output_categorical_accuracy: 0.9548 - Opt_Output_categorical_accuracy: 0.9637 - val_loss: 0.6359 - val_Output_loss: 0.3267 - val_Opt_Output_loss: 0.3092 - val_Output_categorical_accuracy: 0.9301 - val_Opt_Output_categorical_accuracy: 0.9182 - lr: 9.2312e-04\n",
            "Epoch 39/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1796 - Output_loss: 0.0975 - Opt_Output_loss: 0.0821 - Output_categorical_accuracy: 0.9585 - Opt_Output_categorical_accuracy: 0.9626 - val_loss: 0.6475 - val_Output_loss: 0.3585 - val_Opt_Output_loss: 0.2890 - val_Output_categorical_accuracy: 0.9002 - val_Opt_Output_categorical_accuracy: 0.9158 - lr: 9.1393e-04\n",
            "Epoch 40/151\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.1987 - Output_loss: 0.1107 - Opt_Output_loss: 0.0881 - Output_categorical_accuracy: 0.9537 - Opt_Output_categorical_accuracy: 0.9598 - val_loss: 0.5394 - val_Output_loss: 0.2839 - val_Opt_Output_loss: 0.2555 - val_Output_categorical_accuracy: 0.9284 - val_Opt_Output_categorical_accuracy: 0.9230 - lr: 9.0484e-04\n",
            "Epoch 41/151\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.1832 - Output_loss: 0.0990 - Opt_Output_loss: 0.0842 - Output_categorical_accuracy: 0.9545 - Opt_Output_categorical_accuracy: 0.9629 - val_loss: 0.8454 - val_Output_loss: 0.4699 - val_Opt_Output_loss: 0.3755 - val_Output_categorical_accuracy: 0.9267 - val_Opt_Output_categorical_accuracy: 0.9226 - lr: 8.9583e-04\n",
            "Epoch 42/151\n",
            "114/114 [==============================] - 7s 57ms/step - loss: 0.1749 - Output_loss: 0.0936 - Opt_Output_loss: 0.0813 - Output_categorical_accuracy: 0.9586 - Opt_Output_categorical_accuracy: 0.9650 - val_loss: 0.6888 - val_Output_loss: 0.3238 - val_Opt_Output_loss: 0.3649 - val_Output_categorical_accuracy: 0.9206 - val_Opt_Output_categorical_accuracy: 0.9240 - lr: 8.8692e-04\n",
            "Epoch 43/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1669 - Output_loss: 0.0896 - Opt_Output_loss: 0.0774 - Output_categorical_accuracy: 0.9596 - Opt_Output_categorical_accuracy: 0.9631 - val_loss: 0.6059 - val_Output_loss: 0.3151 - val_Opt_Output_loss: 0.2908 - val_Output_categorical_accuracy: 0.9206 - val_Opt_Output_categorical_accuracy: 0.9298 - lr: 8.7810e-04\n",
            "Epoch 44/151\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.1751 - Output_loss: 0.0924 - Opt_Output_loss: 0.0827 - Output_categorical_accuracy: 0.9603 - Opt_Output_categorical_accuracy: 0.9649 - val_loss: 0.7199 - val_Output_loss: 0.3887 - val_Opt_Output_loss: 0.3311 - val_Output_categorical_accuracy: 0.9277 - val_Opt_Output_categorical_accuracy: 0.9301 - lr: 8.6936e-04\n",
            "Epoch 45/151\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.1614 - Output_loss: 0.0849 - Opt_Output_loss: 0.0765 - Output_categorical_accuracy: 0.9601 - Opt_Output_categorical_accuracy: 0.9653 - val_loss: 0.6059 - val_Output_loss: 0.3194 - val_Opt_Output_loss: 0.2866 - val_Output_categorical_accuracy: 0.9376 - val_Opt_Output_categorical_accuracy: 0.9369 - lr: 8.6071e-04\n",
            "Epoch 46/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1564 - Output_loss: 0.0838 - Opt_Output_loss: 0.0726 - Output_categorical_accuracy: 0.9626 - Opt_Output_categorical_accuracy: 0.9679 - val_loss: 0.5399 - val_Output_loss: 0.2690 - val_Opt_Output_loss: 0.2709 - val_Output_categorical_accuracy: 0.9284 - val_Opt_Output_categorical_accuracy: 0.9325 - lr: 8.5214e-04\n",
            "Epoch 47/151\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.1518 - Output_loss: 0.0816 - Opt_Output_loss: 0.0702 - Output_categorical_accuracy: 0.9633 - Opt_Output_categorical_accuracy: 0.9701 - val_loss: 0.7066 - val_Output_loss: 0.3564 - val_Opt_Output_loss: 0.3502 - val_Output_categorical_accuracy: 0.9186 - val_Opt_Output_categorical_accuracy: 0.9237 - lr: 8.4366e-04\n",
            "Epoch 48/151\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.1713 - Output_loss: 0.0940 - Opt_Output_loss: 0.0773 - Output_categorical_accuracy: 0.9619 - Opt_Output_categorical_accuracy: 0.9682 - val_loss: 0.7471 - val_Output_loss: 0.3974 - val_Opt_Output_loss: 0.3498 - val_Output_categorical_accuracy: 0.9220 - val_Opt_Output_categorical_accuracy: 0.9247 - lr: 8.3527e-04\n",
            "Epoch 49/151\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.1779 - Output_loss: 0.0974 - Opt_Output_loss: 0.0805 - Output_categorical_accuracy: 0.9594 - Opt_Output_categorical_accuracy: 0.9646 - val_loss: 0.6869 - val_Output_loss: 0.3601 - val_Opt_Output_loss: 0.3268 - val_Output_categorical_accuracy: 0.9298 - val_Opt_Output_categorical_accuracy: 0.9277 - lr: 8.2696e-04\n",
            "Epoch 50/151\n",
            "114/114 [==============================] - 5s 48ms/step - loss: 0.1657 - Output_loss: 0.0900 - Opt_Output_loss: 0.0757 - Output_categorical_accuracy: 0.9630 - Opt_Output_categorical_accuracy: 0.9670 - val_loss: 0.6190 - val_Output_loss: 0.2997 - val_Opt_Output_loss: 0.3194 - val_Output_categorical_accuracy: 0.9332 - val_Opt_Output_categorical_accuracy: 0.9376 - lr: 8.1873e-04\n",
            "Epoch 51/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.1566 - Output_loss: 0.0849 - Opt_Output_loss: 0.0716 - Output_categorical_accuracy: 0.9640 - Opt_Output_categorical_accuracy: 0.9700 - val_loss: 0.5407 - val_Output_loss: 0.2965 - val_Opt_Output_loss: 0.2442 - val_Output_categorical_accuracy: 0.9389 - val_Opt_Output_categorical_accuracy: 0.9403 - lr: 8.1058e-04\n",
            "Epoch 52/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.1628 - Output_loss: 0.0884 - Opt_Output_loss: 0.0744 - Output_categorical_accuracy: 0.9637 - Opt_Output_categorical_accuracy: 0.9678 - val_loss: 0.6102 - val_Output_loss: 0.2968 - val_Opt_Output_loss: 0.3133 - val_Output_categorical_accuracy: 0.9250 - val_Opt_Output_categorical_accuracy: 0.9298 - lr: 8.0252e-04\n",
            "Epoch 53/151\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.1484 - Output_loss: 0.0789 - Opt_Output_loss: 0.0695 - Output_categorical_accuracy: 0.9648 - Opt_Output_categorical_accuracy: 0.9700 - val_loss: 0.5592 - val_Output_loss: 0.2811 - val_Opt_Output_loss: 0.2781 - val_Output_categorical_accuracy: 0.9294 - val_Opt_Output_categorical_accuracy: 0.9315 - lr: 7.9453e-04\n",
            "Epoch 54/151\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.1659 - Output_loss: 0.0918 - Opt_Output_loss: 0.0740 - Output_categorical_accuracy: 0.9634 - Opt_Output_categorical_accuracy: 0.9674 - val_loss: 0.6927 - val_Output_loss: 0.3660 - val_Opt_Output_loss: 0.3266 - val_Output_categorical_accuracy: 0.9335 - val_Opt_Output_categorical_accuracy: 0.9328 - lr: 7.8663e-04\n",
            "Epoch 55/151\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.1935 - Output_loss: 0.1120 - Opt_Output_loss: 0.0815 - Output_categorical_accuracy: 0.9583 - Opt_Output_categorical_accuracy: 0.9674 - val_loss: 0.6398 - val_Output_loss: 0.3484 - val_Opt_Output_loss: 0.2913 - val_Output_categorical_accuracy: 0.9287 - val_Opt_Output_categorical_accuracy: 0.9291 - lr: 7.7880e-04\n",
            "Epoch 56/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1407 - Output_loss: 0.0752 - Opt_Output_loss: 0.0654 - Output_categorical_accuracy: 0.9679 - Opt_Output_categorical_accuracy: 0.9716 - val_loss: 0.5835 - val_Output_loss: 0.3043 - val_Opt_Output_loss: 0.2793 - val_Output_categorical_accuracy: 0.9308 - val_Opt_Output_categorical_accuracy: 0.9287 - lr: 7.7105e-04\n",
            "Epoch 57/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1390 - Output_loss: 0.0766 - Opt_Output_loss: 0.0624 - Output_categorical_accuracy: 0.9671 - Opt_Output_categorical_accuracy: 0.9733 - val_loss: 0.6630 - val_Output_loss: 0.3371 - val_Opt_Output_loss: 0.3259 - val_Output_categorical_accuracy: 0.9332 - val_Opt_Output_categorical_accuracy: 0.9253 - lr: 7.6338e-04\n",
            "Epoch 58/151\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.1356 - Output_loss: 0.0725 - Opt_Output_loss: 0.0632 - Output_categorical_accuracy: 0.9693 - Opt_Output_categorical_accuracy: 0.9719 - val_loss: 0.7630 - val_Output_loss: 0.4192 - val_Opt_Output_loss: 0.3438 - val_Output_categorical_accuracy: 0.9267 - val_Opt_Output_categorical_accuracy: 0.9213 - lr: 7.5578e-04\n",
            "Epoch 59/151\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.1404 - Output_loss: 0.0785 - Opt_Output_loss: 0.0619 - Output_categorical_accuracy: 0.9663 - Opt_Output_categorical_accuracy: 0.9740 - val_loss: 0.8035 - val_Output_loss: 0.3762 - val_Opt_Output_loss: 0.4272 - val_Output_categorical_accuracy: 0.9338 - val_Opt_Output_categorical_accuracy: 0.9301 - lr: 7.4826e-04\n",
            "Epoch 60/151\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.1516 - Output_loss: 0.0829 - Opt_Output_loss: 0.0687 - Output_categorical_accuracy: 0.9671 - Opt_Output_categorical_accuracy: 0.9700 - val_loss: 0.6592 - val_Output_loss: 0.3459 - val_Opt_Output_loss: 0.3133 - val_Output_categorical_accuracy: 0.9311 - val_Opt_Output_categorical_accuracy: 0.9301 - lr: 7.4082e-04\n",
            "Epoch 61/151\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.1325 - Output_loss: 0.0734 - Opt_Output_loss: 0.0591 - Output_categorical_accuracy: 0.9704 - Opt_Output_categorical_accuracy: 0.9757 - val_loss: 0.5321 - val_Output_loss: 0.2930 - val_Opt_Output_loss: 0.2391 - val_Output_categorical_accuracy: 0.9328 - val_Opt_Output_categorical_accuracy: 0.9389 - lr: 7.3345e-04\n",
            "Epoch 62/151\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.1319 - Output_loss: 0.0731 - Opt_Output_loss: 0.0587 - Output_categorical_accuracy: 0.9711 - Opt_Output_categorical_accuracy: 0.9766 - val_loss: 0.4982 - val_Output_loss: 0.2314 - val_Opt_Output_loss: 0.2668 - val_Output_categorical_accuracy: 0.9450 - val_Opt_Output_categorical_accuracy: 0.9393 - lr: 7.2615e-04\n",
            "Epoch 63/151\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.1331 - Output_loss: 0.0718 - Opt_Output_loss: 0.0614 - Output_categorical_accuracy: 0.9712 - Opt_Output_categorical_accuracy: 0.9759 - val_loss: 0.5340 - val_Output_loss: 0.2636 - val_Opt_Output_loss: 0.2704 - val_Output_categorical_accuracy: 0.9427 - val_Opt_Output_categorical_accuracy: 0.9501 - lr: 7.1892e-04\n",
            "Epoch 64/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.1133 - Output_loss: 0.0623 - Opt_Output_loss: 0.0509 - Output_categorical_accuracy: 0.9749 - Opt_Output_categorical_accuracy: 0.9782 - val_loss: 0.5061 - val_Output_loss: 0.2542 - val_Opt_Output_loss: 0.2519 - val_Output_categorical_accuracy: 0.9460 - val_Opt_Output_categorical_accuracy: 0.9467 - lr: 7.1177e-04\n",
            "Epoch 65/151\n",
            "114/114 [==============================] - 7s 57ms/step - loss: 0.1172 - Output_loss: 0.0642 - Opt_Output_loss: 0.0530 - Output_categorical_accuracy: 0.9744 - Opt_Output_categorical_accuracy: 0.9783 - val_loss: 0.5324 - val_Output_loss: 0.2594 - val_Opt_Output_loss: 0.2730 - val_Output_categorical_accuracy: 0.9444 - val_Opt_Output_categorical_accuracy: 0.9437 - lr: 7.0469e-04\n",
            "Epoch 66/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.1129 - Output_loss: 0.0601 - Opt_Output_loss: 0.0528 - Output_categorical_accuracy: 0.9760 - Opt_Output_categorical_accuracy: 0.9782 - val_loss: 0.5748 - val_Output_loss: 0.2948 - val_Opt_Output_loss: 0.2800 - val_Output_categorical_accuracy: 0.9437 - val_Opt_Output_categorical_accuracy: 0.9437 - lr: 6.9768e-04\n",
            "Epoch 67/151\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.1086 - Output_loss: 0.0591 - Opt_Output_loss: 0.0495 - Output_categorical_accuracy: 0.9762 - Opt_Output_categorical_accuracy: 0.9804 - val_loss: 0.7049 - val_Output_loss: 0.3609 - val_Opt_Output_loss: 0.3440 - val_Output_categorical_accuracy: 0.9410 - val_Opt_Output_categorical_accuracy: 0.9467 - lr: 6.9073e-04\n",
            "Epoch 68/151\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.1098 - Output_loss: 0.0604 - Opt_Output_loss: 0.0493 - Output_categorical_accuracy: 0.9752 - Opt_Output_categorical_accuracy: 0.9797 - val_loss: 0.7246 - val_Output_loss: 0.3570 - val_Opt_Output_loss: 0.3676 - val_Output_categorical_accuracy: 0.9352 - val_Opt_Output_categorical_accuracy: 0.9450 - lr: 6.8386e-04\n",
            "Epoch 69/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.1109 - Output_loss: 0.0624 - Opt_Output_loss: 0.0485 - Output_categorical_accuracy: 0.9778 - Opt_Output_categorical_accuracy: 0.9807 - val_loss: 0.6323 - val_Output_loss: 0.3332 - val_Opt_Output_loss: 0.2991 - val_Output_categorical_accuracy: 0.9304 - val_Opt_Output_categorical_accuracy: 0.9348 - lr: 6.7706e-04\n",
            "Epoch 70/151\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.1174 - Output_loss: 0.0676 - Opt_Output_loss: 0.0498 - Output_categorical_accuracy: 0.9768 - Opt_Output_categorical_accuracy: 0.9800 - val_loss: 0.5426 - val_Output_loss: 0.2749 - val_Opt_Output_loss: 0.2677 - val_Output_categorical_accuracy: 0.9369 - val_Opt_Output_categorical_accuracy: 0.9447 - lr: 6.7032e-04\n",
            "Epoch 71/151\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.1250 - Output_loss: 0.0718 - Opt_Output_loss: 0.0532 - Output_categorical_accuracy: 0.9730 - Opt_Output_categorical_accuracy: 0.9793 - val_loss: 0.6034 - val_Output_loss: 0.2885 - val_Opt_Output_loss: 0.3148 - val_Output_categorical_accuracy: 0.9308 - val_Opt_Output_categorical_accuracy: 0.9372 - lr: 6.6365e-04\n",
            "Epoch 72/151\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.0931 - Output_loss: 0.0521 - Opt_Output_loss: 0.0410 - Output_categorical_accuracy: 0.9789 - Opt_Output_categorical_accuracy: 0.9838 - val_loss: 0.6650 - val_Output_loss: 0.3267 - val_Opt_Output_loss: 0.3383 - val_Output_categorical_accuracy: 0.9365 - val_Opt_Output_categorical_accuracy: 0.9433 - lr: 6.5705e-04\n",
            "Epoch 73/151\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.1008 - Output_loss: 0.0581 - Opt_Output_loss: 0.0426 - Output_categorical_accuracy: 0.9788 - Opt_Output_categorical_accuracy: 0.9814 - val_loss: 0.5878 - val_Output_loss: 0.2746 - val_Opt_Output_loss: 0.3132 - val_Output_categorical_accuracy: 0.9433 - val_Opt_Output_categorical_accuracy: 0.9450 - lr: 6.5051e-04\n",
            "Epoch 74/151\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.1029 - Output_loss: 0.0567 - Opt_Output_loss: 0.0462 - Output_categorical_accuracy: 0.9786 - Opt_Output_categorical_accuracy: 0.9830 - val_loss: 0.6072 - val_Output_loss: 0.2638 - val_Opt_Output_loss: 0.3435 - val_Output_categorical_accuracy: 0.9433 - val_Opt_Output_categorical_accuracy: 0.9457 - lr: 6.4404e-04\n",
            "Epoch 75/151\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0894 - Output_loss: 0.0506 - Opt_Output_loss: 0.0388 - Output_categorical_accuracy: 0.9818 - Opt_Output_categorical_accuracy: 0.9852 - val_loss: 0.5696 - val_Output_loss: 0.2745 - val_Opt_Output_loss: 0.2950 - val_Output_categorical_accuracy: 0.9474 - val_Opt_Output_categorical_accuracy: 0.9511 - lr: 6.3763e-04\n",
            "Epoch 76/151\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0975 - Output_loss: 0.0552 - Opt_Output_loss: 0.0423 - Output_categorical_accuracy: 0.9809 - Opt_Output_categorical_accuracy: 0.9841 - val_loss: 0.5250 - val_Output_loss: 0.2698 - val_Opt_Output_loss: 0.2551 - val_Output_categorical_accuracy: 0.9532 - val_Opt_Output_categorical_accuracy: 0.9549 - lr: 6.3128e-04\n",
            "Epoch 77/151\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0899 - Output_loss: 0.0510 - Opt_Output_loss: 0.0389 - Output_categorical_accuracy: 0.9789 - Opt_Output_categorical_accuracy: 0.9833 - val_loss: 0.6048 - val_Output_loss: 0.3014 - val_Opt_Output_loss: 0.3034 - val_Output_categorical_accuracy: 0.9420 - val_Opt_Output_categorical_accuracy: 0.9433 - lr: 6.2500e-04\n",
            "Epoch 78/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0907 - Output_loss: 0.0501 - Opt_Output_loss: 0.0407 - Output_categorical_accuracy: 0.9790 - Opt_Output_categorical_accuracy: 0.9826 - val_loss: 0.4820 - val_Output_loss: 0.2604 - val_Opt_Output_loss: 0.2216 - val_Output_categorical_accuracy: 0.9491 - val_Opt_Output_categorical_accuracy: 0.9474 - lr: 6.1878e-04\n",
            "Epoch 79/151\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0776 - Output_loss: 0.0446 - Opt_Output_loss: 0.0331 - Output_categorical_accuracy: 0.9814 - Opt_Output_categorical_accuracy: 0.9856 - val_loss: 0.7286 - val_Output_loss: 0.4074 - val_Opt_Output_loss: 0.3213 - val_Output_categorical_accuracy: 0.9365 - val_Opt_Output_categorical_accuracy: 0.9433 - lr: 6.1263e-04\n",
            "Epoch 80/151\n",
            "114/114 [==============================] - 6s 48ms/step - loss: 0.0824 - Output_loss: 0.0464 - Opt_Output_loss: 0.0360 - Output_categorical_accuracy: 0.9818 - Opt_Output_categorical_accuracy: 0.9863 - val_loss: 0.7776 - val_Output_loss: 0.4256 - val_Opt_Output_loss: 0.3520 - val_Output_categorical_accuracy: 0.9348 - val_Opt_Output_categorical_accuracy: 0.9420 - lr: 6.0653e-04\n",
            "Epoch 81/151\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.1011 - Output_loss: 0.0576 - Opt_Output_loss: 0.0434 - Output_categorical_accuracy: 0.9797 - Opt_Output_categorical_accuracy: 0.9844 - val_loss: 0.7135 - val_Output_loss: 0.3695 - val_Opt_Output_loss: 0.3440 - val_Output_categorical_accuracy: 0.9440 - val_Opt_Output_categorical_accuracy: 0.9494 - lr: 6.0050e-04\n",
            "Epoch 82/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0668 - Output_loss: 0.0403 - Opt_Output_loss: 0.0265 - Output_categorical_accuracy: 0.9849 - Opt_Output_categorical_accuracy: 0.9900 - val_loss: 0.6008 - val_Output_loss: 0.3306 - val_Opt_Output_loss: 0.2702 - val_Output_categorical_accuracy: 0.9338 - val_Opt_Output_categorical_accuracy: 0.9491 - lr: 5.9452e-04\n",
            "Epoch 83/151\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0746 - Output_loss: 0.0430 - Opt_Output_loss: 0.0316 - Output_categorical_accuracy: 0.9840 - Opt_Output_categorical_accuracy: 0.9881 - val_loss: 0.6466 - val_Output_loss: 0.3668 - val_Opt_Output_loss: 0.2797 - val_Output_categorical_accuracy: 0.9471 - val_Opt_Output_categorical_accuracy: 0.9508 - lr: 5.8861e-04\n",
            "Epoch 84/151\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0754 - Output_loss: 0.0439 - Opt_Output_loss: 0.0315 - Output_categorical_accuracy: 0.9825 - Opt_Output_categorical_accuracy: 0.9863 - val_loss: 0.5957 - val_Output_loss: 0.3096 - val_Opt_Output_loss: 0.2862 - val_Output_categorical_accuracy: 0.9427 - val_Opt_Output_categorical_accuracy: 0.9457 - lr: 5.8275e-04\n",
            "Epoch 85/151\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0640 - Output_loss: 0.0374 - Opt_Output_loss: 0.0266 - Output_categorical_accuracy: 0.9863 - Opt_Output_categorical_accuracy: 0.9900 - val_loss: 0.5036 - val_Output_loss: 0.2658 - val_Opt_Output_loss: 0.2378 - val_Output_categorical_accuracy: 0.9522 - val_Opt_Output_categorical_accuracy: 0.9525 - lr: 5.7695e-04\n",
            "Epoch 86/151\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0576 - Output_loss: 0.0345 - Opt_Output_loss: 0.0231 - Output_categorical_accuracy: 0.9881 - Opt_Output_categorical_accuracy: 0.9918 - val_loss: 0.6582 - val_Output_loss: 0.3396 - val_Opt_Output_loss: 0.3186 - val_Output_categorical_accuracy: 0.9403 - val_Opt_Output_categorical_accuracy: 0.9440 - lr: 5.7121e-04\n",
            "Epoch 87/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0688 - Output_loss: 0.0420 - Opt_Output_loss: 0.0268 - Output_categorical_accuracy: 0.9852 - Opt_Output_categorical_accuracy: 0.9905 - val_loss: 0.5266 - val_Output_loss: 0.2387 - val_Opt_Output_loss: 0.2880 - val_Output_categorical_accuracy: 0.9481 - val_Opt_Output_categorical_accuracy: 0.9508 - lr: 5.6553e-04\n",
            "Epoch 88/151\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0680 - Output_loss: 0.0393 - Opt_Output_loss: 0.0287 - Output_categorical_accuracy: 0.9855 - Opt_Output_categorical_accuracy: 0.9897 - val_loss: 0.5200 - val_Output_loss: 0.2386 - val_Opt_Output_loss: 0.2814 - val_Output_categorical_accuracy: 0.9522 - val_Opt_Output_categorical_accuracy: 0.9525 - lr: 5.5990e-04\n",
            "Epoch 89/151\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0515 - Output_loss: 0.0307 - Opt_Output_loss: 0.0207 - Output_categorical_accuracy: 0.9879 - Opt_Output_categorical_accuracy: 0.9912 - val_loss: 0.3831 - val_Output_loss: 0.1825 - val_Opt_Output_loss: 0.2006 - val_Output_categorical_accuracy: 0.9610 - val_Opt_Output_categorical_accuracy: 0.9596 - lr: 5.5433e-04\n",
            "Epoch 90/151\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0585 - Output_loss: 0.0365 - Opt_Output_loss: 0.0220 - Output_categorical_accuracy: 0.9871 - Opt_Output_categorical_accuracy: 0.9926 - val_loss: 0.3515 - val_Output_loss: 0.1676 - val_Opt_Output_loss: 0.1839 - val_Output_categorical_accuracy: 0.9559 - val_Opt_Output_categorical_accuracy: 0.9559 - lr: 5.4881e-04\n",
            "Epoch 91/151\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.0518 - Output_loss: 0.0304 - Opt_Output_loss: 0.0214 - Output_categorical_accuracy: 0.9877 - Opt_Output_categorical_accuracy: 0.9926 - val_loss: 0.6550 - val_Output_loss: 0.3162 - val_Opt_Output_loss: 0.3388 - val_Output_categorical_accuracy: 0.9488 - val_Opt_Output_categorical_accuracy: 0.9491 - lr: 5.4335e-04\n",
            "Epoch 92/151\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0555 - Output_loss: 0.0340 - Opt_Output_loss: 0.0215 - Output_categorical_accuracy: 0.9859 - Opt_Output_categorical_accuracy: 0.9903 - val_loss: 0.4642 - val_Output_loss: 0.1995 - val_Opt_Output_loss: 0.2647 - val_Output_categorical_accuracy: 0.9535 - val_Opt_Output_categorical_accuracy: 0.9518 - lr: 5.3794e-04\n",
            "Epoch 93/151\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.0606 - Output_loss: 0.0399 - Opt_Output_loss: 0.0206 - Output_categorical_accuracy: 0.9881 - Opt_Output_categorical_accuracy: 0.9936 - val_loss: 0.5700 - val_Output_loss: 0.3020 - val_Opt_Output_loss: 0.2680 - val_Output_categorical_accuracy: 0.9484 - val_Opt_Output_categorical_accuracy: 0.9539 - lr: 5.3259e-04\n",
            "Epoch 94/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0491 - Output_loss: 0.0294 - Opt_Output_loss: 0.0196 - Output_categorical_accuracy: 0.9882 - Opt_Output_categorical_accuracy: 0.9931 - val_loss: 0.4429 - val_Output_loss: 0.2293 - val_Opt_Output_loss: 0.2136 - val_Output_categorical_accuracy: 0.9535 - val_Opt_Output_categorical_accuracy: 0.9549 - lr: 5.2729e-04\n",
            "Epoch 95/151\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0391 - Output_loss: 0.0246 - Opt_Output_loss: 0.0145 - Output_categorical_accuracy: 0.9912 - Opt_Output_categorical_accuracy: 0.9958 - val_loss: 0.4547 - val_Output_loss: 0.2450 - val_Opt_Output_loss: 0.2097 - val_Output_categorical_accuracy: 0.9535 - val_Opt_Output_categorical_accuracy: 0.9559 - lr: 5.2205e-04\n",
            "Epoch 96/151\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.0480 - Output_loss: 0.0305 - Opt_Output_loss: 0.0176 - Output_categorical_accuracy: 0.9894 - Opt_Output_categorical_accuracy: 0.9933 - val_loss: 0.6124 - val_Output_loss: 0.3041 - val_Opt_Output_loss: 0.3083 - val_Output_categorical_accuracy: 0.9423 - val_Opt_Output_categorical_accuracy: 0.9444 - lr: 5.1685e-04\n",
            "Epoch 97/151\n",
            "114/114 [==============================] - 8s 66ms/step - loss: 0.0462 - Output_loss: 0.0289 - Opt_Output_loss: 0.0173 - Output_categorical_accuracy: 0.9910 - Opt_Output_categorical_accuracy: 0.9934 - val_loss: 0.4796 - val_Output_loss: 0.2160 - val_Opt_Output_loss: 0.2636 - val_Output_categorical_accuracy: 0.9583 - val_Opt_Output_categorical_accuracy: 0.9579 - lr: 5.1171e-04\n",
            "Epoch 98/151\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 0.0367 - Output_loss: 0.0227 - Opt_Output_loss: 0.0141 - Output_categorical_accuracy: 0.9912 - Opt_Output_categorical_accuracy: 0.9953 - val_loss: 0.5027 - val_Output_loss: 0.2757 - val_Opt_Output_loss: 0.2270 - val_Output_categorical_accuracy: 0.9532 - val_Opt_Output_categorical_accuracy: 0.9566 - lr: 5.0662e-04\n",
            "Epoch 99/151\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0352 - Output_loss: 0.0208 - Opt_Output_loss: 0.0144 - Output_categorical_accuracy: 0.9922 - Opt_Output_categorical_accuracy: 0.9945 - val_loss: 0.4498 - val_Output_loss: 0.2381 - val_Opt_Output_loss: 0.2117 - val_Output_categorical_accuracy: 0.9511 - val_Opt_Output_categorical_accuracy: 0.9572 - lr: 5.0158e-04\n",
            "Epoch 100/151\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.0696 - Output_loss: 0.0436 - Opt_Output_loss: 0.0259 - Output_categorical_accuracy: 0.9873 - Opt_Output_categorical_accuracy: 0.9908 - val_loss: 0.4793 - val_Output_loss: 0.2468 - val_Opt_Output_loss: 0.2324 - val_Output_categorical_accuracy: 0.9464 - val_Opt_Output_categorical_accuracy: 0.9566 - lr: 4.9659e-04\n",
            "Epoch 101/151\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0363 - Output_loss: 0.0239 - Opt_Output_loss: 0.0125 - Output_categorical_accuracy: 0.9908 - Opt_Output_categorical_accuracy: 0.9953 - val_loss: 0.4745 - val_Output_loss: 0.2267 - val_Opt_Output_loss: 0.2478 - val_Output_categorical_accuracy: 0.9542 - val_Opt_Output_categorical_accuracy: 0.9579 - lr: 4.9164e-04\n",
            "Epoch 102/151\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.0302 - Output_loss: 0.0182 - Opt_Output_loss: 0.0120 - Output_categorical_accuracy: 0.9931 - Opt_Output_categorical_accuracy: 0.9956 - val_loss: 0.4793 - val_Output_loss: 0.2286 - val_Opt_Output_loss: 0.2507 - val_Output_categorical_accuracy: 0.9559 - val_Opt_Output_categorical_accuracy: 0.9610 - lr: 4.8675e-04\n",
            "Epoch 103/151\n",
            "114/114 [==============================] - 7s 66ms/step - loss: 0.0270 - Output_loss: 0.0163 - Opt_Output_loss: 0.0107 - Output_categorical_accuracy: 0.9934 - Opt_Output_categorical_accuracy: 0.9958 - val_loss: 0.4708 - val_Output_loss: 0.2367 - val_Opt_Output_loss: 0.2341 - val_Output_categorical_accuracy: 0.9569 - val_Opt_Output_categorical_accuracy: 0.9617 - lr: 4.8191e-04\n",
            "Epoch 104/151\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0242 - Output_loss: 0.0153 - Opt_Output_loss: 0.0090 - Output_categorical_accuracy: 0.9942 - Opt_Output_categorical_accuracy: 0.9966 - val_loss: 0.4855 - val_Output_loss: 0.2436 - val_Opt_Output_loss: 0.2420 - val_Output_categorical_accuracy: 0.9549 - val_Opt_Output_categorical_accuracy: 0.9593 - lr: 4.7711e-04\n",
            "Epoch 105/151\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.0484 - Output_loss: 0.0300 - Opt_Output_loss: 0.0184 - Output_categorical_accuracy: 0.9901 - Opt_Output_categorical_accuracy: 0.9926 - val_loss: 0.3901 - val_Output_loss: 0.1712 - val_Opt_Output_loss: 0.2189 - val_Output_categorical_accuracy: 0.9617 - val_Opt_Output_categorical_accuracy: 0.9640 - lr: 4.7237e-04\n",
            "Epoch 106/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0305 - Output_loss: 0.0188 - Opt_Output_loss: 0.0117 - Output_categorical_accuracy: 0.9933 - Opt_Output_categorical_accuracy: 0.9952 - val_loss: 0.4650 - val_Output_loss: 0.2115 - val_Opt_Output_loss: 0.2535 - val_Output_categorical_accuracy: 0.9562 - val_Opt_Output_categorical_accuracy: 0.9603 - lr: 4.6767e-04\n",
            "Epoch 107/151\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0252 - Output_loss: 0.0163 - Opt_Output_loss: 0.0089 - Output_categorical_accuracy: 0.9944 - Opt_Output_categorical_accuracy: 0.9967 - val_loss: 0.3777 - val_Output_loss: 0.1768 - val_Opt_Output_loss: 0.2009 - val_Output_categorical_accuracy: 0.9572 - val_Opt_Output_categorical_accuracy: 0.9630 - lr: 4.6301e-04\n",
            "Epoch 108/151\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0290 - Output_loss: 0.0171 - Opt_Output_loss: 0.0118 - Output_categorical_accuracy: 0.9941 - Opt_Output_categorical_accuracy: 0.9959 - val_loss: 0.4661 - val_Output_loss: 0.2128 - val_Opt_Output_loss: 0.2533 - val_Output_categorical_accuracy: 0.9589 - val_Opt_Output_categorical_accuracy: 0.9637 - lr: 4.5841e-04\n",
            "Epoch 109/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0292 - Output_loss: 0.0177 - Opt_Output_loss: 0.0115 - Output_categorical_accuracy: 0.9937 - Opt_Output_categorical_accuracy: 0.9958 - val_loss: 0.5087 - val_Output_loss: 0.2471 - val_Opt_Output_loss: 0.2615 - val_Output_categorical_accuracy: 0.9501 - val_Opt_Output_categorical_accuracy: 0.9586 - lr: 4.5384e-04\n",
            "Epoch 110/151\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.0249 - Output_loss: 0.0157 - Opt_Output_loss: 0.0092 - Output_categorical_accuracy: 0.9942 - Opt_Output_categorical_accuracy: 0.9974 - val_loss: 0.5060 - val_Output_loss: 0.2401 - val_Opt_Output_loss: 0.2659 - val_Output_categorical_accuracy: 0.9522 - val_Opt_Output_categorical_accuracy: 0.9589 - lr: 4.4933e-04\n",
            "Epoch 111/151\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0224 - Output_loss: 0.0157 - Opt_Output_loss: 0.0067 - Output_categorical_accuracy: 0.9949 - Opt_Output_categorical_accuracy: 0.9975 - val_loss: 0.6213 - val_Output_loss: 0.3126 - val_Opt_Output_loss: 0.3087 - val_Output_categorical_accuracy: 0.9494 - val_Opt_Output_categorical_accuracy: 0.9505 - lr: 4.4486e-04\n",
            "Epoch 112/151\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.0274 - Output_loss: 0.0171 - Opt_Output_loss: 0.0102 - Output_categorical_accuracy: 0.9937 - Opt_Output_categorical_accuracy: 0.9959 - val_loss: 0.5711 - val_Output_loss: 0.2598 - val_Opt_Output_loss: 0.3113 - val_Output_categorical_accuracy: 0.9515 - val_Opt_Output_categorical_accuracy: 0.9566 - lr: 4.4043e-04\n",
            "Epoch 113/151\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0258 - Output_loss: 0.0175 - Opt_Output_loss: 0.0082 - Output_categorical_accuracy: 0.9937 - Opt_Output_categorical_accuracy: 0.9971 - val_loss: 0.6274 - val_Output_loss: 0.2924 - val_Opt_Output_loss: 0.3350 - val_Output_categorical_accuracy: 0.9477 - val_Opt_Output_categorical_accuracy: 0.9427 - lr: 4.3605e-04\n",
            "Epoch 114/151\n",
            "114/114 [==============================] - 8s 68ms/step - loss: 0.0195 - Output_loss: 0.0123 - Opt_Output_loss: 0.0071 - Output_categorical_accuracy: 0.9951 - Opt_Output_categorical_accuracy: 0.9973 - val_loss: 0.6018 - val_Output_loss: 0.2716 - val_Opt_Output_loss: 0.3301 - val_Output_categorical_accuracy: 0.9569 - val_Opt_Output_categorical_accuracy: 0.9566 - lr: 4.3171e-04\n",
            "Epoch 115/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0264 - Output_loss: 0.0162 - Opt_Output_loss: 0.0103 - Output_categorical_accuracy: 0.9948 - Opt_Output_categorical_accuracy: 0.9971 - val_loss: 0.6661 - val_Output_loss: 0.3124 - val_Opt_Output_loss: 0.3537 - val_Output_categorical_accuracy: 0.9511 - val_Opt_Output_categorical_accuracy: 0.9555 - lr: 4.2742e-04\n",
            "Epoch 116/151\n",
            "114/114 [==============================] - 8s 66ms/step - loss: 0.0222 - Output_loss: 0.0137 - Opt_Output_loss: 0.0085 - Output_categorical_accuracy: 0.9941 - Opt_Output_categorical_accuracy: 0.9966 - val_loss: 0.5709 - val_Output_loss: 0.2629 - val_Opt_Output_loss: 0.3080 - val_Output_categorical_accuracy: 0.9610 - val_Opt_Output_categorical_accuracy: 0.9627 - lr: 4.2316e-04\n",
            "Epoch 117/151\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 0.0361 - Output_loss: 0.0227 - Opt_Output_loss: 0.0134 - Output_categorical_accuracy: 0.9926 - Opt_Output_categorical_accuracy: 0.9951 - val_loss: 0.6534 - val_Output_loss: 0.2928 - val_Opt_Output_loss: 0.3606 - val_Output_categorical_accuracy: 0.9600 - val_Opt_Output_categorical_accuracy: 0.9627 - lr: 4.1895e-04\n",
            "Epoch 118/151\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0163 - Output_loss: 0.0101 - Opt_Output_loss: 0.0063 - Output_categorical_accuracy: 0.9962 - Opt_Output_categorical_accuracy: 0.9973 - val_loss: 0.6168 - val_Output_loss: 0.2901 - val_Opt_Output_loss: 0.3267 - val_Output_categorical_accuracy: 0.9549 - val_Opt_Output_categorical_accuracy: 0.9549 - lr: 4.1478e-04\n",
            "Epoch 119/151\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0249 - Output_loss: 0.0164 - Opt_Output_loss: 0.0085 - Output_categorical_accuracy: 0.9938 - Opt_Output_categorical_accuracy: 0.9970 - val_loss: 0.6655 - val_Output_loss: 0.3172 - val_Opt_Output_loss: 0.3483 - val_Output_categorical_accuracy: 0.9528 - val_Opt_Output_categorical_accuracy: 0.9552 - lr: 4.1066e-04\n",
            "Epoch 120/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0216 - Output_loss: 0.0128 - Opt_Output_loss: 0.0088 - Output_categorical_accuracy: 0.9956 - Opt_Output_categorical_accuracy: 0.9977 - val_loss: 0.6108 - val_Output_loss: 0.3051 - val_Opt_Output_loss: 0.3057 - val_Output_categorical_accuracy: 0.9593 - val_Opt_Output_categorical_accuracy: 0.9627 - lr: 4.0657e-04\n",
            "Epoch 121/151\n",
            "114/114 [==============================] - 7s 57ms/step - loss: 0.0249 - Output_loss: 0.0167 - Opt_Output_loss: 0.0082 - Output_categorical_accuracy: 0.9948 - Opt_Output_categorical_accuracy: 0.9964 - val_loss: 0.4496 - val_Output_loss: 0.2453 - val_Opt_Output_loss: 0.2042 - val_Output_categorical_accuracy: 0.9644 - val_Opt_Output_categorical_accuracy: 0.9650 - lr: 4.0252e-04\n",
            "Epoch 122/151\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0193 - Output_loss: 0.0134 - Opt_Output_loss: 0.0058 - Output_categorical_accuracy: 0.9963 - Opt_Output_categorical_accuracy: 0.9984 - val_loss: 0.4821 - val_Output_loss: 0.2443 - val_Opt_Output_loss: 0.2378 - val_Output_categorical_accuracy: 0.9579 - val_Opt_Output_categorical_accuracy: 0.9589 - lr: 3.9852e-04\n",
            "Epoch 123/151\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.0287 - Output_loss: 0.0203 - Opt_Output_loss: 0.0084 - Output_categorical_accuracy: 0.9941 - Opt_Output_categorical_accuracy: 0.9973 - val_loss: 0.6685 - val_Output_loss: 0.3342 - val_Opt_Output_loss: 0.3343 - val_Output_categorical_accuracy: 0.9491 - val_Opt_Output_categorical_accuracy: 0.9518 - lr: 3.9455e-04\n",
            "Epoch 124/151\n",
            "114/114 [==============================] - 7s 57ms/step - loss: 0.0205 - Output_loss: 0.0147 - Opt_Output_loss: 0.0058 - Output_categorical_accuracy: 0.9953 - Opt_Output_categorical_accuracy: 0.9981 - val_loss: 0.8409 - val_Output_loss: 0.4232 - val_Opt_Output_loss: 0.4178 - val_Output_categorical_accuracy: 0.9477 - val_Opt_Output_categorical_accuracy: 0.9505 - lr: 3.9063e-04\n",
            "Epoch 125/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0172 - Output_loss: 0.0122 - Opt_Output_loss: 0.0050 - Output_categorical_accuracy: 0.9962 - Opt_Output_categorical_accuracy: 0.9985 - val_loss: 0.4893 - val_Output_loss: 0.2257 - val_Opt_Output_loss: 0.2636 - val_Output_categorical_accuracy: 0.9637 - val_Opt_Output_categorical_accuracy: 0.9644 - lr: 3.8674e-04\n",
            "Epoch 126/151\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.0171 - Output_loss: 0.0112 - Opt_Output_loss: 0.0059 - Output_categorical_accuracy: 0.9960 - Opt_Output_categorical_accuracy: 0.9982 - val_loss: 0.5342 - val_Output_loss: 0.2481 - val_Opt_Output_loss: 0.2861 - val_Output_categorical_accuracy: 0.9613 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 3.8289e-04\n",
            "Epoch 127/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0132 - Output_loss: 0.0093 - Opt_Output_loss: 0.0039 - Output_categorical_accuracy: 0.9973 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 0.5780 - val_Output_loss: 0.2805 - val_Opt_Output_loss: 0.2975 - val_Output_categorical_accuracy: 0.9586 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 3.7908e-04\n",
            "Epoch 128/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0193 - Output_loss: 0.0125 - Opt_Output_loss: 0.0068 - Output_categorical_accuracy: 0.9956 - Opt_Output_categorical_accuracy: 0.9974 - val_loss: 0.5189 - val_Output_loss: 0.2288 - val_Opt_Output_loss: 0.2901 - val_Output_categorical_accuracy: 0.9657 - val_Opt_Output_categorical_accuracy: 0.9688 - lr: 3.7531e-04\n",
            "Epoch 129/151\n",
            "114/114 [==============================] - 8s 67ms/step - loss: 0.0126 - Output_loss: 0.0080 - Opt_Output_loss: 0.0046 - Output_categorical_accuracy: 0.9971 - Opt_Output_categorical_accuracy: 0.9984 - val_loss: 0.5068 - val_Output_loss: 0.2469 - val_Opt_Output_loss: 0.2599 - val_Output_categorical_accuracy: 0.9634 - val_Opt_Output_categorical_accuracy: 0.9640 - lr: 3.7158e-04\n",
            "Epoch 130/151\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 0.0192 - Output_loss: 0.0126 - Opt_Output_loss: 0.0066 - Output_categorical_accuracy: 0.9951 - Opt_Output_categorical_accuracy: 0.9978 - val_loss: 0.4444 - val_Output_loss: 0.2247 - val_Opt_Output_loss: 0.2197 - val_Output_categorical_accuracy: 0.9586 - val_Opt_Output_categorical_accuracy: 0.9654 - lr: 3.6788e-04\n",
            "Epoch 131/151\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0128 - Output_loss: 0.0095 - Opt_Output_loss: 0.0033 - Output_categorical_accuracy: 0.9963 - Opt_Output_categorical_accuracy: 0.9990 - val_loss: 0.5288 - val_Output_loss: 0.2436 - val_Opt_Output_loss: 0.2852 - val_Output_categorical_accuracy: 0.9627 - val_Opt_Output_categorical_accuracy: 0.9650 - lr: 3.6422e-04\n",
            "Epoch 132/151\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0258 - Output_loss: 0.0192 - Opt_Output_loss: 0.0065 - Output_categorical_accuracy: 0.9948 - Opt_Output_categorical_accuracy: 0.9981 - val_loss: 0.7129 - val_Output_loss: 0.3562 - val_Opt_Output_loss: 0.3567 - val_Output_categorical_accuracy: 0.9542 - val_Opt_Output_categorical_accuracy: 0.9579 - lr: 3.6060e-04\n",
            "Epoch 133/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0120 - Output_loss: 0.0081 - Opt_Output_loss: 0.0039 - Output_categorical_accuracy: 0.9978 - Opt_Output_categorical_accuracy: 0.9990 - val_loss: 0.5447 - val_Output_loss: 0.2615 - val_Opt_Output_loss: 0.2832 - val_Output_categorical_accuracy: 0.9657 - val_Opt_Output_categorical_accuracy: 0.9667 - lr: 3.5701e-04\n",
            "Epoch 134/151\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.0114 - Output_loss: 0.0079 - Opt_Output_loss: 0.0035 - Output_categorical_accuracy: 0.9970 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 0.6715 - val_Output_loss: 0.3138 - val_Opt_Output_loss: 0.3576 - val_Output_categorical_accuracy: 0.9596 - val_Opt_Output_categorical_accuracy: 0.9562 - lr: 3.5345e-04\n",
            "Epoch 135/151\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0111 - Output_loss: 0.0071 - Opt_Output_loss: 0.0040 - Output_categorical_accuracy: 0.9981 - Opt_Output_categorical_accuracy: 0.9988 - val_loss: 0.6330 - val_Output_loss: 0.2957 - val_Opt_Output_loss: 0.3372 - val_Output_categorical_accuracy: 0.9637 - val_Opt_Output_categorical_accuracy: 0.9627 - lr: 3.4994e-04\n",
            "Epoch 136/151\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.0147 - Output_loss: 0.0101 - Opt_Output_loss: 0.0046 - Output_categorical_accuracy: 0.9962 - Opt_Output_categorical_accuracy: 0.9986 - val_loss: 0.6280 - val_Output_loss: 0.3014 - val_Opt_Output_loss: 0.3266 - val_Output_categorical_accuracy: 0.9583 - val_Opt_Output_categorical_accuracy: 0.9627 - lr: 3.4646e-04\n",
            "Epoch 137/151\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0150 - Output_loss: 0.0107 - Opt_Output_loss: 0.0043 - Output_categorical_accuracy: 0.9967 - Opt_Output_categorical_accuracy: 0.9985 - val_loss: 0.7680 - val_Output_loss: 0.3826 - val_Opt_Output_loss: 0.3854 - val_Output_categorical_accuracy: 0.9491 - val_Opt_Output_categorical_accuracy: 0.9549 - lr: 3.4301e-04\n",
            "Epoch 138/151\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.0205 - Output_loss: 0.0137 - Opt_Output_loss: 0.0068 - Output_categorical_accuracy: 0.9955 - Opt_Output_categorical_accuracy: 0.9979 - val_loss: 0.6267 - val_Output_loss: 0.2929 - val_Opt_Output_loss: 0.3338 - val_Output_categorical_accuracy: 0.9545 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 3.3960e-04\n",
            "Epoch 139/151\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.0151 - Output_loss: 0.0099 - Opt_Output_loss: 0.0051 - Output_categorical_accuracy: 0.9966 - Opt_Output_categorical_accuracy: 0.9978 - val_loss: 0.6168 - val_Output_loss: 0.2929 - val_Opt_Output_loss: 0.3239 - val_Output_categorical_accuracy: 0.9589 - val_Opt_Output_categorical_accuracy: 0.9593 - lr: 3.3622e-04\n",
            "Epoch 140/151\n",
            "114/114 [==============================] - 7s 64ms/step - loss: 0.0080 - Output_loss: 0.0055 - Opt_Output_loss: 0.0025 - Output_categorical_accuracy: 0.9981 - Opt_Output_categorical_accuracy: 0.9995 - val_loss: 0.6076 - val_Output_loss: 0.3192 - val_Opt_Output_loss: 0.2885 - val_Output_categorical_accuracy: 0.9603 - val_Opt_Output_categorical_accuracy: 0.9620 - lr: 3.3287e-04\n",
            "Epoch 141/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0187 - Output_loss: 0.0120 - Opt_Output_loss: 0.0067 - Output_categorical_accuracy: 0.9960 - Opt_Output_categorical_accuracy: 0.9977 - val_loss: 0.6747 - val_Output_loss: 0.3593 - val_Opt_Output_loss: 0.3154 - val_Output_categorical_accuracy: 0.9555 - val_Opt_Output_categorical_accuracy: 0.9583 - lr: 3.2956e-04\n",
            "Epoch 142/151\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.0070 - Output_loss: 0.0049 - Opt_Output_loss: 0.0021 - Output_categorical_accuracy: 0.9984 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.7191 - val_Output_loss: 0.3867 - val_Opt_Output_loss: 0.3324 - val_Output_categorical_accuracy: 0.9525 - val_Opt_Output_categorical_accuracy: 0.9559 - lr: 3.2628e-04\n",
            "Epoch 143/151\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0086 - Output_loss: 0.0066 - Opt_Output_loss: 0.0020 - Output_categorical_accuracy: 0.9979 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 0.7280 - val_Output_loss: 0.3707 - val_Opt_Output_loss: 0.3572 - val_Output_categorical_accuracy: 0.9528 - val_Opt_Output_categorical_accuracy: 0.9545 - lr: 3.2303e-04\n",
            "Epoch 144/151\n",
            "114/114 [==============================] - 8s 67ms/step - loss: 0.0106 - Output_loss: 0.0068 - Opt_Output_loss: 0.0038 - Output_categorical_accuracy: 0.9977 - Opt_Output_categorical_accuracy: 0.9988 - val_loss: 0.6478 - val_Output_loss: 0.3004 - val_Opt_Output_loss: 0.3475 - val_Output_categorical_accuracy: 0.9606 - val_Opt_Output_categorical_accuracy: 0.9627 - lr: 3.1982e-04\n",
            "Epoch 145/151\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0161 - Output_loss: 0.0111 - Opt_Output_loss: 0.0050 - Output_categorical_accuracy: 0.9968 - Opt_Output_categorical_accuracy: 0.9984 - val_loss: 0.6326 - val_Output_loss: 0.3289 - val_Opt_Output_loss: 0.3037 - val_Output_categorical_accuracy: 0.9613 - val_Opt_Output_categorical_accuracy: 0.9634 - lr: 3.1664e-04\n",
            "Epoch 146/151\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0120 - Output_loss: 0.0079 - Opt_Output_loss: 0.0041 - Output_categorical_accuracy: 0.9974 - Opt_Output_categorical_accuracy: 0.9984 - val_loss: 0.6295 - val_Output_loss: 0.3427 - val_Opt_Output_loss: 0.2868 - val_Output_categorical_accuracy: 0.9606 - val_Opt_Output_categorical_accuracy: 0.9650 - lr: 3.1349e-04\n",
            "Epoch 147/151\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.0115 - Output_loss: 0.0079 - Opt_Output_loss: 0.0037 - Output_categorical_accuracy: 0.9977 - Opt_Output_categorical_accuracy: 0.9988 - val_loss: 0.6285 - val_Output_loss: 0.3520 - val_Opt_Output_loss: 0.2764 - val_Output_categorical_accuracy: 0.9566 - val_Opt_Output_categorical_accuracy: 0.9623 - lr: 3.1037e-04\n",
            "Epoch 148/151\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0082 - Output_loss: 0.0058 - Opt_Output_loss: 0.0023 - Output_categorical_accuracy: 0.9979 - Opt_Output_categorical_accuracy: 0.9995 - val_loss: 0.5754 - val_Output_loss: 0.3267 - val_Opt_Output_loss: 0.2487 - val_Output_categorical_accuracy: 0.9593 - val_Opt_Output_categorical_accuracy: 0.9640 - lr: 3.0728e-04\n",
            "Epoch 149/151\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.0040 - Output_loss: 0.0028 - Opt_Output_loss: 0.0012 - Output_categorical_accuracy: 0.9988 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.7517 - val_Output_loss: 0.4290 - val_Opt_Output_loss: 0.3226 - val_Output_categorical_accuracy: 0.9532 - val_Opt_Output_categorical_accuracy: 0.9562 - lr: 3.0422e-04\n",
            "Epoch 150/151\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0124 - Output_loss: 0.0090 - Opt_Output_loss: 0.0035 - Output_categorical_accuracy: 0.9973 - Opt_Output_categorical_accuracy: 0.9988 - val_loss: 0.9237 - val_Output_loss: 0.4701 - val_Opt_Output_loss: 0.4536 - val_Output_categorical_accuracy: 0.9440 - val_Opt_Output_categorical_accuracy: 0.9484 - lr: 3.0119e-04\n",
            "Epoch 151/151\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.0127 - Output_loss: 0.0093 - Opt_Output_loss: 0.0034 - Output_categorical_accuracy: 0.9964 - Opt_Output_categorical_accuracy: 0.9989 - val_loss: 0.8675 - val_Output_loss: 0.4579 - val_Opt_Output_loss: 0.4096 - val_Output_categorical_accuracy: 0.9460 - val_Opt_Output_categorical_accuracy: 0.9518 - lr: 2.9820e-04\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def res_identity(x, filters): \n",
        "  #renet block where dimension doesnot change.\n",
        "  #The skip connection is just simple identity conncection\n",
        "  #we will have 3 blocks and then input will be added\n",
        "\n",
        "  x_skip = x # this will be used for addition with the residual block \n",
        "  f1, f2 = filters\n",
        "\n",
        "  #first block \n",
        "  #kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  \n",
        "\n",
        "  #second block # bottleneck (but size kept same with padding)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same', )(x)\n",
        "  \n",
        "\n",
        "  # third block activation used after adding the input\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.Conv1D(f2, kernel_size=3, strides=1, padding='same')(x)\n",
        "\n",
        "  # add the input \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def conv_skip(x, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=2, padding='valid')(x)\n",
        "  # when s = 2 then it is like downsizing the feature map\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  # x = tf.keras.layers.ZeroPadding1D(padding=(1,1))(x)\n",
        "  \n",
        "\n",
        "  # second block\n",
        "  x = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  #third block\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "  x = tf.keras.layers.Conv1D(f2, kernel_size=3, strides=1, padding='same')(x)\n",
        "  # x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "\n",
        "  # shortcut \n",
        "  x_skip = tf.keras.layers.Conv1D(f2, kernel_size=3, strides=2, padding='valid')(x_skip)\n",
        "  # x_skip = tf.keras.layers.MaxPool1D(pool_size=3,strides=2)(x_skip)\n",
        "  # x_skip = tf.keras.layers.BatchNormalization()(x_skip)\n",
        "\n",
        "  # add \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def multi_fiter_conv(x, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x1 = tf.keras.layers.Conv1D(f1, kernel_size=3, strides=1, padding='same')(x)\n",
        "  x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "  x1 = tf.keras.layers.Activation(tf.keras.activations.relu)(x1)\n",
        "  \n",
        "\n",
        "  # second block\n",
        "  x2 = tf.keras.layers.Conv1D(f1, kernel_size=5, strides=1, padding='same')(x)\n",
        "  x2 = tf.keras.layers.BatchNormalization()(x2)\n",
        "  x2 = tf.keras.layers.Activation(tf.keras.activations.relu)(x2)\n",
        "\n",
        "  #third block\n",
        "  x3 = tf.keras.layers.Conv1D(f1, kernel_size=7, strides=1, padding='same')(x)\n",
        "  x3 = tf.keras.layers.BatchNormalization()(x3)\n",
        "  x3 = tf.keras.layers.Activation(tf.keras.activations.relu)(x3)\n",
        "\n",
        "\n",
        "  # # forth block\n",
        "  # x4 = tf.keras.layers.Conv1D(f1, kernel_size=9, strides=1, padding='same')(x)\n",
        "  # x4 = tf.keras.layers.BatchNormalization()(x4)\n",
        "  # x4 = tf.keras.layers.Activation(tf.keras.activations.relu)(x4)\n",
        "\n",
        "  # concatenate \n",
        "  x = tf.keras.layers.Concatenate()([x1,x2,x3])\n",
        "  # x = tf.keras.layers.Conv1D(f2,kernel_size=3,strides=1,padding='same')(x)\n",
        "  # x = tf.keras.layers.BatchNormalization()(x)\n",
        "  # x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  #add\n",
        "  # x = tf.keras.layers.Add()([x,x_skip])\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "\n",
        "class SelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_units):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_units = num_units\n",
        "        self.WQ = tf.keras.layers.Dense(units=num_units)\n",
        "        self.WK = tf.keras.layers.Dense(units=num_units)\n",
        "        self.WV = tf.keras.layers.Dense(units=num_units)\n",
        "        self.softmax = tf.keras.layers.Softmax()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Q = self.WQ(inputs)\n",
        "        K = self.WK(inputs)\n",
        "        V = self.WV(inputs)\n",
        "        attention_logits = tf.matmul(Q, K, transpose_b=True)\n",
        "        attention_weights = self.softmax(attention_logits)\n",
        "        attention_output = tf.matmul(attention_weights, V)\n",
        "        return attention_output\n",
        "\n",
        "def buildAE(input_shape,classes,learning_rate):\n",
        "\n",
        "    input = tf.keras.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.ZeroPadding1D(padding=(1,1))(input)\n",
        "    x = tf.keras.layers.Conv1D(16, kernel_size=3, strides=1,padding='valid',)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)(x)\n",
        "\n",
        "    x = multi_fiter_conv(x,filters=(16,16))\n",
        "\n",
        "    x = conv_skip(x ,filters=(16,32))\n",
        "    # print(x.shape)\n",
        "    x = res_identity(x,filters=(16,32))\n",
        "    x = multi_fiter_conv(x,filters=(32,32))\n",
        "    x = conv_skip(x ,filters=(32,64))\n",
        "    x = res_identity(x,filters=(32,64))\n",
        "    x = res_identity(x,filters=(32,64))\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "    # x = maxpool_skip(x,filters=(16,16))\n",
        "    \n",
        "    # output_opt = SelfAttention(32)(x)\n",
        "    output_opt = tf.keras.layers.Flatten()(x)\n",
        "    # output_opt = tf.keras.layers.Dense(units=64)(output_opt)\n",
        "    output_opt = tf.keras.layers.Dense(units=classes)(output_opt)\n",
        "    output_opt = tf.keras.layers.Activation('softmax',name='Opt_Output')(output_opt)\n",
        "\n",
        "\n",
        "\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=16,activation='tanh',return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = SelfAttention(num_units=32)(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=32,activation='tanh',return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = SelfAttention(num_units=64)(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    \n",
        "\n",
        "    # x = tf.keras.layers.Dense(units=64)(x)\n",
        "    x = tf.keras.layers.Dense(units=classes)(x)\n",
        "    output = tf.keras.layers.Activation('softmax',name='Output')(x)\n",
        "\n",
        "\n",
        "    # model = tf.keras.Model(input,output)\n",
        "\n",
        "    pred_model = tf.keras.Model(input,output)\n",
        "    model = tf.keras.Model(input,[output,output_opt])\n",
        "\n",
        "\n",
        "    \n",
        "    print('Params', model.count_params())\n",
        "    model.compile(loss = [tf.keras.losses.CategoricalCrossentropy(),\n",
        "                          tf.keras.losses.CategoricalCrossentropy()],\n",
        "                  metrics=[tf.keras.metrics.CategoricalAccuracy(),],\n",
        "                   optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
        "\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    return model, pred_model\n",
        "# model = buildAE((128,9),6,0.0005)\n",
        "# tf.keras.utils.plot_model(model,show_shapes=True)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "num_classes = y_train.shape[-1]\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "print(X_train.shape,y_train.shape)\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 30:\n",
        "    return lr\n",
        "  else:\n",
        "     return lr * tf.math.exp(-0.01)\n",
        "\n",
        "dg = DataGenerator(X_train,y_train,batch_size=batch_size,input_shape=X_train.shape[1:])\n",
        "model,pred_model = buildAE(X_train.shape[1:],y_train.shape[-1],learning_rate)\n",
        "# log = MyLogger(n=1, validation_data=(x_test,y_test), AE=model)\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "history = model.fit(dg, epochs=151, verbose=1,callbacks = [lr_scheduler], validation_data=(x_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q3xFGC9nbAG",
        "outputId": "b89cd1fb-7a94-45a4-ead9-38fe8b18d364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 152/251\n",
            "114/114 [==============================] - 7s 57ms/step - loss: 0.0161 - Output_loss: 0.0102 - Opt_Output_loss: 0.0058 - Output_categorical_accuracy: 0.9967 - Opt_Output_categorical_accuracy: 0.9981 - val_loss: 0.9150 - val_Output_loss: 0.4893 - val_Opt_Output_loss: 0.4257 - val_Output_categorical_accuracy: 0.9518 - val_Opt_Output_categorical_accuracy: 0.9532 - lr: 2.9523e-04\n",
            "Epoch 153/251\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0114 - Output_loss: 0.0081 - Opt_Output_loss: 0.0033 - Output_categorical_accuracy: 0.9974 - Opt_Output_categorical_accuracy: 0.9989 - val_loss: 0.8651 - val_Output_loss: 0.4448 - val_Opt_Output_loss: 0.4202 - val_Output_categorical_accuracy: 0.9511 - val_Opt_Output_categorical_accuracy: 0.9532 - lr: 2.9229e-04\n",
            "Epoch 154/251\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0079 - Output_loss: 0.0049 - Opt_Output_loss: 0.0030 - Output_categorical_accuracy: 0.9986 - Opt_Output_categorical_accuracy: 0.9988 - val_loss: 0.8465 - val_Output_loss: 0.4512 - val_Opt_Output_loss: 0.3954 - val_Output_categorical_accuracy: 0.9545 - val_Opt_Output_categorical_accuracy: 0.9535 - lr: 2.8938e-04\n",
            "Epoch 155/251\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0111 - Output_loss: 0.0073 - Opt_Output_loss: 0.0038 - Output_categorical_accuracy: 0.9974 - Opt_Output_categorical_accuracy: 0.9986 - val_loss: 0.8381 - val_Output_loss: 0.4165 - val_Opt_Output_loss: 0.4216 - val_Output_categorical_accuracy: 0.9511 - val_Opt_Output_categorical_accuracy: 0.9522 - lr: 2.8651e-04\n",
            "Epoch 156/251\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0085 - Output_loss: 0.0058 - Opt_Output_loss: 0.0026 - Output_categorical_accuracy: 0.9984 - Opt_Output_categorical_accuracy: 0.9995 - val_loss: 0.8810 - val_Output_loss: 0.4484 - val_Opt_Output_loss: 0.4325 - val_Output_categorical_accuracy: 0.9522 - val_Opt_Output_categorical_accuracy: 0.9545 - lr: 2.8365e-04\n",
            "Epoch 157/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0098 - Output_loss: 0.0067 - Opt_Output_loss: 0.0031 - Output_categorical_accuracy: 0.9981 - Opt_Output_categorical_accuracy: 0.9988 - val_loss: 0.5826 - val_Output_loss: 0.3425 - val_Opt_Output_loss: 0.2401 - val_Output_categorical_accuracy: 0.9583 - val_Opt_Output_categorical_accuracy: 0.9596 - lr: 2.8083e-04\n",
            "Epoch 158/251\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0125 - Output_loss: 0.0080 - Opt_Output_loss: 0.0045 - Output_categorical_accuracy: 0.9973 - Opt_Output_categorical_accuracy: 0.9982 - val_loss: 0.6184 - val_Output_loss: 0.3573 - val_Opt_Output_loss: 0.2611 - val_Output_categorical_accuracy: 0.9569 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 2.7804e-04\n",
            "Epoch 159/251\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0074 - Output_loss: 0.0057 - Opt_Output_loss: 0.0017 - Output_categorical_accuracy: 0.9973 - Opt_Output_categorical_accuracy: 0.9995 - val_loss: 0.7499 - val_Output_loss: 0.4261 - val_Opt_Output_loss: 0.3238 - val_Output_categorical_accuracy: 0.9518 - val_Opt_Output_categorical_accuracy: 0.9549 - lr: 2.7527e-04\n",
            "Epoch 160/251\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0081 - Output_loss: 0.0061 - Opt_Output_loss: 0.0020 - Output_categorical_accuracy: 0.9981 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.7795 - val_Output_loss: 0.4289 - val_Opt_Output_loss: 0.3506 - val_Output_categorical_accuracy: 0.9539 - val_Opt_Output_categorical_accuracy: 0.9535 - lr: 2.7253e-04\n",
            "Epoch 161/251\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0061 - Output_loss: 0.0042 - Opt_Output_loss: 0.0019 - Output_categorical_accuracy: 0.9989 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.7169 - val_Output_loss: 0.4000 - val_Opt_Output_loss: 0.3169 - val_Output_categorical_accuracy: 0.9562 - val_Opt_Output_categorical_accuracy: 0.9576 - lr: 2.6982e-04\n",
            "Epoch 162/251\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0053 - Output_loss: 0.0035 - Opt_Output_loss: 0.0019 - Output_categorical_accuracy: 0.9986 - Opt_Output_categorical_accuracy: 0.9995 - val_loss: 0.8310 - val_Output_loss: 0.4009 - val_Opt_Output_loss: 0.4300 - val_Output_categorical_accuracy: 0.9555 - val_Opt_Output_categorical_accuracy: 0.9545 - lr: 2.6714e-04\n",
            "Epoch 163/251\n",
            "114/114 [==============================] - 7s 57ms/step - loss: 0.0110 - Output_loss: 0.0080 - Opt_Output_loss: 0.0029 - Output_categorical_accuracy: 0.9966 - Opt_Output_categorical_accuracy: 0.9992 - val_loss: 0.7334 - val_Output_loss: 0.3596 - val_Opt_Output_loss: 0.3739 - val_Output_categorical_accuracy: 0.9528 - val_Opt_Output_categorical_accuracy: 0.9559 - lr: 2.6448e-04\n",
            "Epoch 164/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0047 - Output_loss: 0.0037 - Opt_Output_loss: 9.5646e-04 - Output_categorical_accuracy: 0.9988 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.6955 - val_Output_loss: 0.3691 - val_Opt_Output_loss: 0.3264 - val_Output_categorical_accuracy: 0.9549 - val_Opt_Output_categorical_accuracy: 0.9583 - lr: 2.6185e-04\n",
            "Epoch 165/251\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0101 - Output_loss: 0.0080 - Opt_Output_loss: 0.0021 - Output_categorical_accuracy: 0.9968 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 0.6737 - val_Output_loss: 0.3762 - val_Opt_Output_loss: 0.2975 - val_Output_categorical_accuracy: 0.9518 - val_Opt_Output_categorical_accuracy: 0.9562 - lr: 2.5924e-04\n",
            "Epoch 166/251\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.0024 - Output_loss: 0.0014 - Opt_Output_loss: 0.0010 - Output_categorical_accuracy: 0.9999 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.6217 - val_Output_loss: 0.3388 - val_Opt_Output_loss: 0.2829 - val_Output_categorical_accuracy: 0.9576 - val_Opt_Output_categorical_accuracy: 0.9617 - lr: 2.5666e-04\n",
            "Epoch 167/251\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0069 - Output_loss: 0.0043 - Opt_Output_loss: 0.0025 - Output_categorical_accuracy: 0.9992 - Opt_Output_categorical_accuracy: 0.9989 - val_loss: 0.7387 - val_Output_loss: 0.3668 - val_Opt_Output_loss: 0.3719 - val_Output_categorical_accuracy: 0.9532 - val_Opt_Output_categorical_accuracy: 0.9572 - lr: 2.5411e-04\n",
            "Epoch 168/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0051 - Output_loss: 0.0032 - Opt_Output_loss: 0.0019 - Output_categorical_accuracy: 0.9988 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 0.7035 - val_Output_loss: 0.3707 - val_Opt_Output_loss: 0.3327 - val_Output_categorical_accuracy: 0.9586 - val_Opt_Output_categorical_accuracy: 0.9596 - lr: 2.5158e-04\n",
            "Epoch 169/251\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.0082 - Output_loss: 0.0054 - Opt_Output_loss: 0.0028 - Output_categorical_accuracy: 0.9982 - Opt_Output_categorical_accuracy: 0.9992 - val_loss: 0.7613 - val_Output_loss: 0.3879 - val_Opt_Output_loss: 0.3734 - val_Output_categorical_accuracy: 0.9416 - val_Opt_Output_categorical_accuracy: 0.9488 - lr: 2.4908e-04\n",
            "Epoch 170/251\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0043 - Output_loss: 0.0030 - Opt_Output_loss: 0.0013 - Output_categorical_accuracy: 0.9988 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.6698 - val_Output_loss: 0.3540 - val_Opt_Output_loss: 0.3158 - val_Output_categorical_accuracy: 0.9481 - val_Opt_Output_categorical_accuracy: 0.9518 - lr: 2.4660e-04\n",
            "Epoch 171/251\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.0064 - Output_loss: 0.0050 - Opt_Output_loss: 0.0014 - Output_categorical_accuracy: 0.9985 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.8030 - val_Output_loss: 0.4329 - val_Opt_Output_loss: 0.3701 - val_Output_categorical_accuracy: 0.9420 - val_Opt_Output_categorical_accuracy: 0.9471 - lr: 2.4414e-04\n",
            "Epoch 172/251\n",
            "114/114 [==============================] - 6s 48ms/step - loss: 0.0020 - Output_loss: 0.0012 - Opt_Output_loss: 7.6797e-04 - Output_categorical_accuracy: 0.9996 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 1.0006 - val_Output_loss: 0.5030 - val_Opt_Output_loss: 0.4976 - val_Output_categorical_accuracy: 0.9444 - val_Opt_Output_categorical_accuracy: 0.9477 - lr: 2.4171e-04\n",
            "Epoch 173/251\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.0169 - Output_loss: 0.0110 - Opt_Output_loss: 0.0059 - Output_categorical_accuracy: 0.9960 - Opt_Output_categorical_accuracy: 0.9978 - val_loss: 1.2749 - val_Output_loss: 0.5754 - val_Opt_Output_loss: 0.6994 - val_Output_categorical_accuracy: 0.9427 - val_Opt_Output_categorical_accuracy: 0.9444 - lr: 2.3931e-04\n",
            "Epoch 174/251\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0057 - Output_loss: 0.0043 - Opt_Output_loss: 0.0014 - Output_categorical_accuracy: 0.9982 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.9895 - val_Output_loss: 0.4532 - val_Opt_Output_loss: 0.5363 - val_Output_categorical_accuracy: 0.9460 - val_Opt_Output_categorical_accuracy: 0.9501 - lr: 2.3693e-04\n",
            "Epoch 175/251\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 0.0097 - Output_loss: 0.0070 - Opt_Output_loss: 0.0027 - Output_categorical_accuracy: 0.9982 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 0.9840 - val_Output_loss: 0.4326 - val_Opt_Output_loss: 0.5514 - val_Output_categorical_accuracy: 0.9545 - val_Opt_Output_categorical_accuracy: 0.9559 - lr: 2.3457e-04\n",
            "Epoch 176/251\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0051 - Output_loss: 0.0035 - Opt_Output_loss: 0.0016 - Output_categorical_accuracy: 0.9986 - Opt_Output_categorical_accuracy: 0.9992 - val_loss: 0.9044 - val_Output_loss: 0.4061 - val_Opt_Output_loss: 0.4983 - val_Output_categorical_accuracy: 0.9552 - val_Opt_Output_categorical_accuracy: 0.9576 - lr: 2.3224e-04\n",
            "Epoch 177/251\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0077 - Output_loss: 0.0062 - Opt_Output_loss: 0.0015 - Output_categorical_accuracy: 0.9984 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 0.8472 - val_Output_loss: 0.3143 - val_Opt_Output_loss: 0.5329 - val_Output_categorical_accuracy: 0.9572 - val_Opt_Output_categorical_accuracy: 0.9579 - lr: 2.2993e-04\n",
            "Epoch 178/251\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0080 - Output_loss: 0.0056 - Opt_Output_loss: 0.0024 - Output_categorical_accuracy: 0.9985 - Opt_Output_categorical_accuracy: 0.9992 - val_loss: 0.5644 - val_Output_loss: 0.2553 - val_Opt_Output_loss: 0.3091 - val_Output_categorical_accuracy: 0.9610 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 2.2764e-04\n",
            "Epoch 179/251\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0127 - Output_loss: 0.0077 - Opt_Output_loss: 0.0051 - Output_categorical_accuracy: 0.9982 - Opt_Output_categorical_accuracy: 0.9989 - val_loss: 0.5490 - val_Output_loss: 0.2206 - val_Opt_Output_loss: 0.3284 - val_Output_categorical_accuracy: 0.9637 - val_Opt_Output_categorical_accuracy: 0.9647 - lr: 2.2537e-04\n",
            "Epoch 180/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0040 - Output_loss: 0.0032 - Opt_Output_loss: 7.0504e-04 - Output_categorical_accuracy: 0.9993 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.5412 - val_Output_loss: 0.2342 - val_Opt_Output_loss: 0.3070 - val_Output_categorical_accuracy: 0.9583 - val_Opt_Output_categorical_accuracy: 0.9623 - lr: 2.2313e-04\n",
            "Epoch 181/251\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.0030 - Output_loss: 0.0023 - Opt_Output_loss: 7.7528e-04 - Output_categorical_accuracy: 0.9992 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.6654 - val_Output_loss: 0.2557 - val_Opt_Output_loss: 0.4098 - val_Output_categorical_accuracy: 0.9600 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 2.2091e-04\n",
            "Epoch 182/251\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0026 - Output_loss: 0.0019 - Opt_Output_loss: 6.8721e-04 - Output_categorical_accuracy: 0.9996 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.5574 - val_Output_loss: 0.2345 - val_Opt_Output_loss: 0.3230 - val_Output_categorical_accuracy: 0.9610 - val_Opt_Output_categorical_accuracy: 0.9667 - lr: 2.1871e-04\n",
            "Epoch 183/251\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0039 - Output_loss: 0.0025 - Opt_Output_loss: 0.0014 - Output_categorical_accuracy: 0.9990 - Opt_Output_categorical_accuracy: 0.9995 - val_loss: 0.7166 - val_Output_loss: 0.2747 - val_Opt_Output_loss: 0.4419 - val_Output_categorical_accuracy: 0.9589 - val_Opt_Output_categorical_accuracy: 0.9640 - lr: 2.1654e-04\n",
            "Epoch 184/251\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0060 - Output_loss: 0.0038 - Opt_Output_loss: 0.0022 - Output_categorical_accuracy: 0.9984 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 0.4659 - val_Output_loss: 0.2136 - val_Opt_Output_loss: 0.2523 - val_Output_categorical_accuracy: 0.9589 - val_Opt_Output_categorical_accuracy: 0.9603 - lr: 2.1438e-04\n",
            "Epoch 185/251\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0019 - Output_loss: 0.0014 - Opt_Output_loss: 5.7209e-04 - Output_categorical_accuracy: 0.9997 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.4857 - val_Output_loss: 0.2359 - val_Opt_Output_loss: 0.2498 - val_Output_categorical_accuracy: 0.9617 - val_Opt_Output_categorical_accuracy: 0.9640 - lr: 2.1225e-04\n",
            "Epoch 186/251\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0090 - Output_loss: 0.0064 - Opt_Output_loss: 0.0026 - Output_categorical_accuracy: 0.9982 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 0.6102 - val_Output_loss: 0.2582 - val_Opt_Output_loss: 0.3521 - val_Output_categorical_accuracy: 0.9555 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 2.1014e-04\n",
            "Epoch 187/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0063 - Output_loss: 0.0044 - Opt_Output_loss: 0.0019 - Output_categorical_accuracy: 0.9985 - Opt_Output_categorical_accuracy: 0.9995 - val_loss: 0.5222 - val_Output_loss: 0.2113 - val_Opt_Output_loss: 0.3109 - val_Output_categorical_accuracy: 0.9606 - val_Opt_Output_categorical_accuracy: 0.9650 - lr: 2.0805e-04\n",
            "Epoch 188/251\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0035 - Output_loss: 0.0030 - Opt_Output_loss: 5.4172e-04 - Output_categorical_accuracy: 0.9989 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.6258 - val_Output_loss: 0.2315 - val_Opt_Output_loss: 0.3943 - val_Output_categorical_accuracy: 0.9620 - val_Opt_Output_categorical_accuracy: 0.9637 - lr: 2.0598e-04\n",
            "Epoch 189/251\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0022 - Output_loss: 0.0015 - Opt_Output_loss: 6.3006e-04 - Output_categorical_accuracy: 0.9995 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.6619 - val_Output_loss: 0.2698 - val_Opt_Output_loss: 0.3921 - val_Output_categorical_accuracy: 0.9562 - val_Opt_Output_categorical_accuracy: 0.9606 - lr: 2.0393e-04\n",
            "Epoch 190/251\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0055 - Output_loss: 0.0043 - Opt_Output_loss: 0.0011 - Output_categorical_accuracy: 0.9986 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.6998 - val_Output_loss: 0.2674 - val_Opt_Output_loss: 0.4324 - val_Output_categorical_accuracy: 0.9586 - val_Opt_Output_categorical_accuracy: 0.9684 - lr: 2.0190e-04\n",
            "Epoch 191/251\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0071 - Output_loss: 0.0041 - Opt_Output_loss: 0.0030 - Output_categorical_accuracy: 0.9988 - Opt_Output_categorical_accuracy: 0.9992 - val_loss: 0.7899 - val_Output_loss: 0.2969 - val_Opt_Output_loss: 0.4930 - val_Output_categorical_accuracy: 0.9555 - val_Opt_Output_categorical_accuracy: 0.9606 - lr: 1.9989e-04\n",
            "Epoch 192/251\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0032 - Output_loss: 0.0019 - Opt_Output_loss: 0.0012 - Output_categorical_accuracy: 0.9996 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 0.8524 - val_Output_loss: 0.3242 - val_Opt_Output_loss: 0.5282 - val_Output_categorical_accuracy: 0.9562 - val_Opt_Output_categorical_accuracy: 0.9620 - lr: 1.9790e-04\n",
            "Epoch 193/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0032 - Output_loss: 0.0023 - Opt_Output_loss: 8.7152e-04 - Output_categorical_accuracy: 0.9993 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.8010 - val_Output_loss: 0.3186 - val_Opt_Output_loss: 0.4823 - val_Output_categorical_accuracy: 0.9545 - val_Opt_Output_categorical_accuracy: 0.9610 - lr: 1.9593e-04\n",
            "Epoch 194/251\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.0027 - Output_loss: 0.0020 - Opt_Output_loss: 6.5619e-04 - Output_categorical_accuracy: 0.9992 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.8036 - val_Output_loss: 0.3101 - val_Opt_Output_loss: 0.4935 - val_Output_categorical_accuracy: 0.9562 - val_Opt_Output_categorical_accuracy: 0.9617 - lr: 1.9398e-04\n",
            "Epoch 195/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0054 - Output_loss: 0.0036 - Opt_Output_loss: 0.0019 - Output_categorical_accuracy: 0.9985 - Opt_Output_categorical_accuracy: 0.9995 - val_loss: 0.9164 - val_Output_loss: 0.3313 - val_Opt_Output_loss: 0.5850 - val_Output_categorical_accuracy: 0.9491 - val_Opt_Output_categorical_accuracy: 0.9579 - lr: 1.9205e-04\n",
            "Epoch 196/251\n",
            "114/114 [==============================] - 7s 64ms/step - loss: 0.0039 - Output_loss: 0.0030 - Opt_Output_loss: 9.2918e-04 - Output_categorical_accuracy: 0.9992 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.7536 - val_Output_loss: 0.3095 - val_Opt_Output_loss: 0.4441 - val_Output_categorical_accuracy: 0.9545 - val_Opt_Output_categorical_accuracy: 0.9620 - lr: 1.9014e-04\n",
            "Epoch 197/251\n",
            "114/114 [==============================] - 5s 45ms/step - loss: 0.0088 - Output_loss: 0.0068 - Opt_Output_loss: 0.0020 - Output_categorical_accuracy: 0.9986 - Opt_Output_categorical_accuracy: 0.9989 - val_loss: 0.8716 - val_Output_loss: 0.3542 - val_Opt_Output_loss: 0.5175 - val_Output_categorical_accuracy: 0.9508 - val_Opt_Output_categorical_accuracy: 0.9562 - lr: 1.8825e-04\n",
            "Epoch 198/251\n",
            "114/114 [==============================] - 7s 58ms/step - loss: 0.0060 - Output_loss: 0.0037 - Opt_Output_loss: 0.0023 - Output_categorical_accuracy: 0.9988 - Opt_Output_categorical_accuracy: 0.9995 - val_loss: 0.8219 - val_Output_loss: 0.3098 - val_Opt_Output_loss: 0.5121 - val_Output_categorical_accuracy: 0.9583 - val_Opt_Output_categorical_accuracy: 0.9637 - lr: 1.8637e-04\n",
            "Epoch 199/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0023 - Output_loss: 0.0015 - Opt_Output_loss: 8.4263e-04 - Output_categorical_accuracy: 0.9996 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.7993 - val_Output_loss: 0.3430 - val_Opt_Output_loss: 0.4563 - val_Output_categorical_accuracy: 0.9539 - val_Opt_Output_categorical_accuracy: 0.9600 - lr: 1.8452e-04\n",
            "Epoch 200/251\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0014 - Output_loss: 0.0012 - Opt_Output_loss: 2.2274e-04 - Output_categorical_accuracy: 0.9995 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.8120 - val_Output_loss: 0.3448 - val_Opt_Output_loss: 0.4673 - val_Output_categorical_accuracy: 0.9559 - val_Opt_Output_categorical_accuracy: 0.9606 - lr: 1.8268e-04\n",
            "Epoch 201/251\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0037 - Output_loss: 0.0031 - Opt_Output_loss: 5.8624e-04 - Output_categorical_accuracy: 0.9990 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.8083 - val_Output_loss: 0.3569 - val_Opt_Output_loss: 0.4514 - val_Output_categorical_accuracy: 0.9566 - val_Opt_Output_categorical_accuracy: 0.9623 - lr: 1.8087e-04\n",
            "Epoch 202/251\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 0.0046 - Output_loss: 0.0037 - Opt_Output_loss: 9.6341e-04 - Output_categorical_accuracy: 0.9992 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.7929 - val_Output_loss: 0.3710 - val_Opt_Output_loss: 0.4219 - val_Output_categorical_accuracy: 0.9555 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 1.7907e-04\n",
            "Epoch 203/251\n",
            "114/114 [==============================] - 7s 61ms/step - loss: 0.0045 - Output_loss: 0.0033 - Opt_Output_loss: 0.0012 - Output_categorical_accuracy: 0.9985 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.5813 - val_Output_loss: 0.3316 - val_Opt_Output_loss: 0.2497 - val_Output_categorical_accuracy: 0.9600 - val_Opt_Output_categorical_accuracy: 0.9644 - lr: 1.7728e-04\n",
            "Epoch 204/251\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 0.0078 - Output_loss: 0.0047 - Opt_Output_loss: 0.0032 - Output_categorical_accuracy: 0.9992 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 0.6101 - val_Output_loss: 0.3315 - val_Opt_Output_loss: 0.2786 - val_Output_categorical_accuracy: 0.9532 - val_Opt_Output_categorical_accuracy: 0.9566 - lr: 1.7552e-04\n",
            "Epoch 205/251\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0067 - Output_loss: 0.0042 - Opt_Output_loss: 0.0025 - Output_categorical_accuracy: 0.9986 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.6688 - val_Output_loss: 0.3836 - val_Opt_Output_loss: 0.2852 - val_Output_categorical_accuracy: 0.9522 - val_Opt_Output_categorical_accuracy: 0.9579 - lr: 1.7377e-04\n",
            "Epoch 206/251\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0019 - Output_loss: 0.0011 - Opt_Output_loss: 7.1189e-04 - Output_categorical_accuracy: 0.9996 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.6964 - val_Output_loss: 0.4094 - val_Opt_Output_loss: 0.2870 - val_Output_categorical_accuracy: 0.9535 - val_Opt_Output_categorical_accuracy: 0.9579 - lr: 1.7205e-04\n",
            "Epoch 207/251\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0012 - Output_loss: 0.0010 - Opt_Output_loss: 2.3305e-04 - Output_categorical_accuracy: 0.9996 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.6883 - val_Output_loss: 0.3981 - val_Opt_Output_loss: 0.2901 - val_Output_categorical_accuracy: 0.9525 - val_Opt_Output_categorical_accuracy: 0.9583 - lr: 1.7033e-04\n",
            "Epoch 208/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0022 - Output_loss: 0.0014 - Opt_Output_loss: 7.2141e-04 - Output_categorical_accuracy: 0.9996 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.8647 - val_Output_loss: 0.4828 - val_Opt_Output_loss: 0.3819 - val_Output_categorical_accuracy: 0.9508 - val_Opt_Output_categorical_accuracy: 0.9545 - lr: 1.6864e-04\n",
            "Epoch 209/251\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0028 - Output_loss: 0.0021 - Opt_Output_loss: 7.1571e-04 - Output_categorical_accuracy: 0.9995 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.7762 - val_Output_loss: 0.4409 - val_Opt_Output_loss: 0.3353 - val_Output_categorical_accuracy: 0.9552 - val_Opt_Output_categorical_accuracy: 0.9600 - lr: 1.6696e-04\n",
            "Epoch 210/251\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 0.0026 - Output_loss: 0.0017 - Opt_Output_loss: 9.5646e-04 - Output_categorical_accuracy: 0.9993 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.8397 - val_Output_loss: 0.4546 - val_Opt_Output_loss: 0.3851 - val_Output_categorical_accuracy: 0.9508 - val_Opt_Output_categorical_accuracy: 0.9572 - lr: 1.6530e-04\n",
            "Epoch 211/251\n",
            "114/114 [==============================] - 5s 48ms/step - loss: 0.0061 - Output_loss: 0.0039 - Opt_Output_loss: 0.0022 - Output_categorical_accuracy: 0.9990 - Opt_Output_categorical_accuracy: 0.9992 - val_loss: 0.8305 - val_Output_loss: 0.4149 - val_Opt_Output_loss: 0.4156 - val_Output_categorical_accuracy: 0.9539 - val_Opt_Output_categorical_accuracy: 0.9596 - lr: 1.6365e-04\n",
            "Epoch 212/251\n",
            "114/114 [==============================] - 7s 60ms/step - loss: 0.0039 - Output_loss: 0.0029 - Opt_Output_loss: 9.6401e-04 - Output_categorical_accuracy: 0.9989 - Opt_Output_categorical_accuracy: 0.9995 - val_loss: 0.9171 - val_Output_loss: 0.4439 - val_Opt_Output_loss: 0.4731 - val_Output_categorical_accuracy: 0.9522 - val_Opt_Output_categorical_accuracy: 0.9583 - lr: 1.6203e-04\n",
            "Epoch 213/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0011 - Output_loss: 8.8795e-04 - Opt_Output_loss: 1.8470e-04 - Output_categorical_accuracy: 0.9997 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.8560 - val_Output_loss: 0.4296 - val_Opt_Output_loss: 0.4264 - val_Output_categorical_accuracy: 0.9539 - val_Opt_Output_categorical_accuracy: 0.9600 - lr: 1.6041e-04\n",
            "Epoch 214/251\n",
            "114/114 [==============================] - 7s 63ms/step - loss: 0.0034 - Output_loss: 0.0024 - Opt_Output_loss: 0.0010 - Output_categorical_accuracy: 0.9993 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.7884 - val_Output_loss: 0.4118 - val_Opt_Output_loss: 0.3766 - val_Output_categorical_accuracy: 0.9549 - val_Opt_Output_categorical_accuracy: 0.9596 - lr: 1.5882e-04\n",
            "Epoch 215/251\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0029 - Output_loss: 0.0021 - Opt_Output_loss: 7.7684e-04 - Output_categorical_accuracy: 0.9989 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.8201 - val_Output_loss: 0.4269 - val_Opt_Output_loss: 0.3932 - val_Output_categorical_accuracy: 0.9542 - val_Opt_Output_categorical_accuracy: 0.9586 - lr: 1.5724e-04\n",
            "Epoch 216/251\n",
            "114/114 [==============================] - 7s 64ms/step - loss: 0.0040 - Output_loss: 0.0030 - Opt_Output_loss: 0.0010 - Output_categorical_accuracy: 0.9985 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.6929 - val_Output_loss: 0.3728 - val_Opt_Output_loss: 0.3202 - val_Output_categorical_accuracy: 0.9535 - val_Opt_Output_categorical_accuracy: 0.9603 - lr: 1.5567e-04\n",
            "Epoch 217/251\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0046 - Output_loss: 0.0029 - Opt_Output_loss: 0.0017 - Output_categorical_accuracy: 0.9992 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 0.6987 - val_Output_loss: 0.3569 - val_Opt_Output_loss: 0.3418 - val_Output_categorical_accuracy: 0.9569 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 1.5412e-04\n",
            "Epoch 218/251\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0036 - Output_loss: 0.0022 - Opt_Output_loss: 0.0013 - Output_categorical_accuracy: 0.9992 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.6242 - val_Output_loss: 0.3326 - val_Opt_Output_loss: 0.2916 - val_Output_categorical_accuracy: 0.9589 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 1.5259e-04\n",
            "Epoch 219/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 7.4290e-04 - Output_loss: 5.4713e-04 - Opt_Output_loss: 1.9577e-04 - Output_categorical_accuracy: 0.9999 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.6924 - val_Output_loss: 0.3806 - val_Opt_Output_loss: 0.3117 - val_Output_categorical_accuracy: 0.9569 - val_Opt_Output_categorical_accuracy: 0.9583 - lr: 1.5107e-04\n",
            "Epoch 220/251\n",
            "114/114 [==============================] - 7s 57ms/step - loss: 0.0010 - Output_loss: 6.6335e-04 - Opt_Output_loss: 3.6335e-04 - Output_categorical_accuracy: 0.9997 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.6672 - val_Output_loss: 0.3827 - val_Opt_Output_loss: 0.2845 - val_Output_categorical_accuracy: 0.9589 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 1.4957e-04\n",
            "Epoch 221/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 4.2330e-04 - Output_loss: 2.5092e-04 - Opt_Output_loss: 1.7237e-04 - Output_categorical_accuracy: 1.0000 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.8043 - val_Output_loss: 0.4220 - val_Opt_Output_loss: 0.3823 - val_Output_categorical_accuracy: 0.9576 - val_Opt_Output_categorical_accuracy: 0.9603 - lr: 1.4808e-04\n",
            "Epoch 222/251\n",
            "114/114 [==============================] - 6s 50ms/step - loss: 8.6341e-04 - Output_loss: 6.0043e-04 - Opt_Output_loss: 2.6298e-04 - Output_categorical_accuracy: 0.9997 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.8731 - val_Output_loss: 0.4631 - val_Opt_Output_loss: 0.4100 - val_Output_categorical_accuracy: 0.9539 - val_Opt_Output_categorical_accuracy: 0.9576 - lr: 1.4661e-04\n",
            "Epoch 223/251\n",
            "114/114 [==============================] - 6s 55ms/step - loss: 0.0018 - Output_loss: 0.0016 - Opt_Output_loss: 1.3669e-04 - Output_categorical_accuracy: 0.9999 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.8154 - val_Output_loss: 0.4161 - val_Opt_Output_loss: 0.3994 - val_Output_categorical_accuracy: 0.9579 - val_Opt_Output_categorical_accuracy: 0.9606 - lr: 1.4515e-04\n",
            "Epoch 224/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0057 - Output_loss: 0.0042 - Opt_Output_loss: 0.0014 - Output_categorical_accuracy: 0.9984 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.7658 - val_Output_loss: 0.3969 - val_Opt_Output_loss: 0.3689 - val_Output_categorical_accuracy: 0.9572 - val_Opt_Output_categorical_accuracy: 0.9617 - lr: 1.4370e-04\n",
            "Epoch 225/251\n",
            "114/114 [==============================] - 7s 64ms/step - loss: 0.0039 - Output_loss: 0.0025 - Opt_Output_loss: 0.0014 - Output_categorical_accuracy: 0.9990 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.8154 - val_Output_loss: 0.3818 - val_Opt_Output_loss: 0.4336 - val_Output_categorical_accuracy: 0.9566 - val_Opt_Output_categorical_accuracy: 0.9586 - lr: 1.4227e-04\n",
            "Epoch 226/251\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0021 - Output_loss: 0.0017 - Opt_Output_loss: 4.3002e-04 - Output_categorical_accuracy: 0.9993 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.8425 - val_Output_loss: 0.3905 - val_Opt_Output_loss: 0.4520 - val_Output_categorical_accuracy: 0.9562 - val_Opt_Output_categorical_accuracy: 0.9576 - lr: 1.4086e-04\n",
            "Epoch 227/251\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0044 - Output_loss: 0.0026 - Opt_Output_loss: 0.0019 - Output_categorical_accuracy: 0.9995 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.7116 - val_Output_loss: 0.3567 - val_Opt_Output_loss: 0.3549 - val_Output_categorical_accuracy: 0.9589 - val_Opt_Output_categorical_accuracy: 0.9603 - lr: 1.3946e-04\n",
            "Epoch 228/251\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0015 - Output_loss: 9.5810e-04 - Opt_Output_loss: 5.0301e-04 - Output_categorical_accuracy: 0.9997 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.7507 - val_Output_loss: 0.3700 - val_Opt_Output_loss: 0.3807 - val_Output_categorical_accuracy: 0.9593 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 1.3807e-04\n",
            "Epoch 229/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0012 - Output_loss: 9.5399e-04 - Opt_Output_loss: 2.7685e-04 - Output_categorical_accuracy: 0.9993 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.6958 - val_Output_loss: 0.3634 - val_Opt_Output_loss: 0.3324 - val_Output_categorical_accuracy: 0.9600 - val_Opt_Output_categorical_accuracy: 0.9610 - lr: 1.3670e-04\n",
            "Epoch 230/251\n",
            "114/114 [==============================] - 7s 64ms/step - loss: 0.0013 - Output_loss: 0.0012 - Opt_Output_loss: 1.4138e-04 - Output_categorical_accuracy: 0.9997 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.7312 - val_Output_loss: 0.3709 - val_Opt_Output_loss: 0.3604 - val_Output_categorical_accuracy: 0.9566 - val_Opt_Output_categorical_accuracy: 0.9606 - lr: 1.3534e-04\n",
            "Epoch 231/251\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0020 - Output_loss: 0.0015 - Opt_Output_loss: 4.8099e-04 - Output_categorical_accuracy: 0.9995 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.7655 - val_Output_loss: 0.3686 - val_Opt_Output_loss: 0.3969 - val_Output_categorical_accuracy: 0.9559 - val_Opt_Output_categorical_accuracy: 0.9610 - lr: 1.3399e-04\n",
            "Epoch 232/251\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0015 - Output_loss: 0.0014 - Opt_Output_loss: 1.2100e-04 - Output_categorical_accuracy: 0.9997 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.7243 - val_Output_loss: 0.3586 - val_Opt_Output_loss: 0.3657 - val_Output_categorical_accuracy: 0.9586 - val_Opt_Output_categorical_accuracy: 0.9627 - lr: 1.3266e-04\n",
            "Epoch 233/251\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 4.2842e-04 - Output_loss: 3.6169e-04 - Opt_Output_loss: 6.6724e-05 - Output_categorical_accuracy: 1.0000 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.7912 - val_Output_loss: 0.3823 - val_Opt_Output_loss: 0.4090 - val_Output_categorical_accuracy: 0.9569 - val_Opt_Output_categorical_accuracy: 0.9613 - lr: 1.3134e-04\n",
            "Epoch 234/251\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0046 - Output_loss: 0.0034 - Opt_Output_loss: 0.0012 - Output_categorical_accuracy: 0.9992 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.8238 - val_Output_loss: 0.3482 - val_Opt_Output_loss: 0.4755 - val_Output_categorical_accuracy: 0.9583 - val_Opt_Output_categorical_accuracy: 0.9586 - lr: 1.3003e-04\n",
            "Epoch 235/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 5.1421e-04 - Output_loss: 3.9142e-04 - Opt_Output_loss: 1.2278e-04 - Output_categorical_accuracy: 0.9999 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.8336 - val_Output_loss: 0.3564 - val_Opt_Output_loss: 0.4772 - val_Output_categorical_accuracy: 0.9569 - val_Opt_Output_categorical_accuracy: 0.9603 - lr: 1.2874e-04\n",
            "Epoch 236/251\n",
            "114/114 [==============================] - 7s 62ms/step - loss: 0.0020 - Output_loss: 0.0016 - Opt_Output_loss: 3.6786e-04 - Output_categorical_accuracy: 0.9999 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.8469 - val_Output_loss: 0.3592 - val_Opt_Output_loss: 0.4877 - val_Output_categorical_accuracy: 0.9576 - val_Opt_Output_categorical_accuracy: 0.9617 - lr: 1.2745e-04\n",
            "Epoch 237/251\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 5.0726e-04 - Output_loss: 2.1127e-04 - Opt_Output_loss: 2.9599e-04 - Output_categorical_accuracy: 0.9999 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.8606 - val_Output_loss: 0.3632 - val_Opt_Output_loss: 0.4973 - val_Output_categorical_accuracy: 0.9576 - val_Opt_Output_categorical_accuracy: 0.9617 - lr: 1.2619e-04\n",
            "Epoch 238/251\n",
            "114/114 [==============================] - 6s 57ms/step - loss: 3.6052e-04 - Output_loss: 2.5496e-04 - Opt_Output_loss: 1.0556e-04 - Output_categorical_accuracy: 0.9999 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.8165 - val_Output_loss: 0.3607 - val_Opt_Output_loss: 0.4559 - val_Output_categorical_accuracy: 0.9576 - val_Opt_Output_categorical_accuracy: 0.9627 - lr: 1.2493e-04\n",
            "Epoch 239/251\n",
            "114/114 [==============================] - 5s 47ms/step - loss: 0.0032 - Output_loss: 0.0022 - Opt_Output_loss: 9.7078e-04 - Output_categorical_accuracy: 0.9993 - Opt_Output_categorical_accuracy: 0.9995 - val_loss: 0.7876 - val_Output_loss: 0.3505 - val_Opt_Output_loss: 0.4371 - val_Output_categorical_accuracy: 0.9620 - val_Opt_Output_categorical_accuracy: 0.9644 - lr: 1.2369e-04\n",
            "Epoch 240/251\n",
            "114/114 [==============================] - 7s 64ms/step - loss: 0.0049 - Output_loss: 0.0037 - Opt_Output_loss: 0.0011 - Output_categorical_accuracy: 0.9986 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 0.8373 - val_Output_loss: 0.3432 - val_Opt_Output_loss: 0.4941 - val_Output_categorical_accuracy: 0.9613 - val_Opt_Output_categorical_accuracy: 0.9667 - lr: 1.2246e-04\n",
            "Epoch 241/251\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0045 - Output_loss: 0.0031 - Opt_Output_loss: 0.0014 - Output_categorical_accuracy: 0.9989 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.8349 - val_Output_loss: 0.3544 - val_Opt_Output_loss: 0.4805 - val_Output_categorical_accuracy: 0.9586 - val_Opt_Output_categorical_accuracy: 0.9644 - lr: 1.2124e-04\n",
            "Epoch 242/251\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0012 - Output_loss: 7.0766e-04 - Opt_Output_loss: 4.9308e-04 - Output_categorical_accuracy: 0.9999 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.8886 - val_Output_loss: 0.3782 - val_Opt_Output_loss: 0.5104 - val_Output_categorical_accuracy: 0.9606 - val_Opt_Output_categorical_accuracy: 0.9654 - lr: 1.2003e-04\n",
            "Epoch 243/251\n",
            "114/114 [==============================] - 6s 49ms/step - loss: 0.0013 - Output_loss: 0.0011 - Opt_Output_loss: 2.1595e-04 - Output_categorical_accuracy: 0.9997 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.8579 - val_Output_loss: 0.3237 - val_Opt_Output_loss: 0.5342 - val_Output_categorical_accuracy: 0.9603 - val_Opt_Output_categorical_accuracy: 0.9644 - lr: 1.1884e-04\n",
            "Epoch 244/251\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0022 - Output_loss: 0.0019 - Opt_Output_loss: 2.4025e-04 - Output_categorical_accuracy: 0.9993 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.8540 - val_Output_loss: 0.3217 - val_Opt_Output_loss: 0.5323 - val_Output_categorical_accuracy: 0.9593 - val_Opt_Output_categorical_accuracy: 0.9630 - lr: 1.1765e-04\n",
            "Epoch 245/251\n",
            "114/114 [==============================] - 5s 46ms/step - loss: 0.0012 - Output_loss: 9.3314e-04 - Opt_Output_loss: 2.2089e-04 - Output_categorical_accuracy: 0.9996 - Opt_Output_categorical_accuracy: 1.0000 - val_loss: 0.8527 - val_Output_loss: 0.3318 - val_Opt_Output_loss: 0.5209 - val_Output_categorical_accuracy: 0.9593 - val_Opt_Output_categorical_accuracy: 0.9637 - lr: 1.1648e-04\n",
            "Epoch 246/251\n",
            "114/114 [==============================] - 7s 59ms/step - loss: 0.0011 - Output_loss: 8.8310e-04 - Opt_Output_loss: 2.4172e-04 - Output_categorical_accuracy: 0.9997 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 0.9233 - val_Output_loss: 0.3407 - val_Opt_Output_loss: 0.5826 - val_Output_categorical_accuracy: 0.9566 - val_Opt_Output_categorical_accuracy: 0.9603 - lr: 1.1533e-04\n",
            "Epoch 247/251\n",
            "114/114 [==============================] - 6s 51ms/step - loss: 0.0052 - Output_loss: 0.0036 - Opt_Output_loss: 0.0015 - Output_categorical_accuracy: 0.9986 - Opt_Output_categorical_accuracy: 0.9993 - val_loss: 1.1069 - val_Output_loss: 0.4086 - val_Opt_Output_loss: 0.6983 - val_Output_categorical_accuracy: 0.9576 - val_Opt_Output_categorical_accuracy: 0.9610 - lr: 1.1418e-04\n",
            "Epoch 248/251\n",
            "114/114 [==============================] - 6s 56ms/step - loss: 0.0032 - Output_loss: 0.0027 - Opt_Output_loss: 5.2312e-04 - Output_categorical_accuracy: 0.9993 - Opt_Output_categorical_accuracy: 0.9999 - val_loss: 1.0098 - val_Output_loss: 0.3915 - val_Opt_Output_loss: 0.6183 - val_Output_categorical_accuracy: 0.9566 - val_Opt_Output_categorical_accuracy: 0.9589 - lr: 1.1304e-04\n",
            "Epoch 249/251\n",
            "114/114 [==============================] - 6s 53ms/step - loss: 0.0027 - Output_loss: 0.0018 - Opt_Output_loss: 9.0092e-04 - Output_categorical_accuracy: 0.9990 - Opt_Output_categorical_accuracy: 0.9996 - val_loss: 1.0441 - val_Output_loss: 0.4048 - val_Opt_Output_loss: 0.6393 - val_Output_categorical_accuracy: 0.9559 - val_Opt_Output_categorical_accuracy: 0.9579 - lr: 1.1192e-04\n",
            "Epoch 250/251\n",
            "114/114 [==============================] - 6s 52ms/step - loss: 0.0014 - Output_loss: 7.5818e-04 - Opt_Output_loss: 5.9923e-04 - Output_categorical_accuracy: 0.9999 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.9967 - val_Output_loss: 0.3975 - val_Opt_Output_loss: 0.5993 - val_Output_categorical_accuracy: 0.9552 - val_Opt_Output_categorical_accuracy: 0.9566 - lr: 1.1080e-04\n",
            "Epoch 251/251\n",
            "114/114 [==============================] - 6s 54ms/step - loss: 0.0026 - Output_loss: 0.0019 - Opt_Output_loss: 6.4351e-04 - Output_categorical_accuracy: 0.9993 - Opt_Output_categorical_accuracy: 0.9997 - val_loss: 0.9788 - val_Output_loss: 0.3861 - val_Opt_Output_loss: 0.5927 - val_Output_categorical_accuracy: 0.9576 - val_Opt_Output_categorical_accuracy: 0.9579 - lr: 1.0970e-04\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dg, epochs=251,  verbose=1,callbacks = [lr_scheduler], validation_data=(x_test,y_test),initial_epoch=151)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "IgoV0Vi4bYDF",
        "outputId": "2e1f5dd9-36f6-43a6-c57d-c4b05ece8ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "93/93 [==============================] - 2s 25ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00       496\n",
            "           1       1.00      0.96      0.98       471\n",
            "           2       0.99      1.00      0.99       420\n",
            "           3       0.94      0.89      0.92       491\n",
            "           4       0.91      0.98      0.94       532\n",
            "           5       0.99      1.00      1.00       537\n",
            "\n",
            "    accuracy                           0.97      2947\n",
            "   macro avg       0.97      0.97      0.97      2947\n",
            "weighted avg       0.97      0.97      0.97      2947\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.save('Adaptive_Multikernel_ResNetV2_BI-LSTM_Attention_extra.h5')\n",
        "\n",
        "# model = load_model('/content/Adaptive_Multikernel_ResNetV2_BI-LSTM_Attention.h5', custom_objects={'AttentionLayer': AttentionLayer})\n",
        "\n",
        "# model = tf.keras.models.load_model('/content/Adaptive_Multikernel_ResNetV2_BI-LSTM_Attention_extra.h5',\n",
        "                                  #  custom_objects= {'SelfAttention':SelfAttention})\n",
        "outs = model.predict(x_test)\n",
        "\n",
        "\n",
        "class_names= np.unique(np.argmax(y_test,axis=1))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(np.argmax(y_test,axis=1), np.argmax(outs[0],axis=1))\n",
        "\n",
        "print(classification_report(np.argmax(y_test,axis=1), np.argmax(outs[0],axis=1)))\n",
        "\n",
        "\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "# set the axis labels and title of the plot\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "plt.savefig('confusion_matrix.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoWdBCy9j_gx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hpflMwa3moG7",
        "6vSPhbEvrIAa",
        "lPD-L19PXiur"
      ],
      "provenance": [],
      "mount_file_id": "1onV_ZFJ_Fs26iyMZL3fSnY5BFbfn-87i",
      "authorship_tag": "ABX9TyPfZGy5cX3Nuc7m2wb/qzX0",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}